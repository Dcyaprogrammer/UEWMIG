{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJjXifWFe9K"
      },
      "outputs": [],
      "source": [
        "%pip install jax\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install xminigrid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as jtu\n",
        "import numpy as np\n",
        "\n",
        "import timeit\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "import xminigrid"
      ],
      "metadata": {
        "id": "0gdi3TeiF481"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeStep(struct.PyTreeNode):\n",
        "    # hidden environment state, such as grid, agent, goal, etc\n",
        "    state: State\n",
        "\n",
        "    # similar to the dm_env enterface\n",
        "    step_type: StepType\n",
        "    reward: jax.Array\n",
        "    discount: jax.Array\n",
        "    observation: jax.Array"
      ],
      "metadata": {
        "id": "4L7duDLiJ9Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect Rollouts"
      ],
      "metadata": {
        "id": "hNLDJZfm4Hx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xminigrid.wrappers import GymAutoResetWrapper\n",
        "\n",
        "def build_rollout(env, env_params, num_steps):\n",
        "  def rollout(rng):\n",
        "    def _step_fn(carry, _):\n",
        "      rng, timestep = carry\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      action = jax.random.randint(_rng, shape=(), minval=0, maxval=env.num_actions(env_params))\n",
        "\n",
        "      timestep = env.step(env_params, timestep, action)\n",
        "\n",
        "      return (rng, timestep), (timestep,action)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    timestep = env.reset(env_params, _rng)\n",
        "    rng, (transitions, actions) = jax.lax.scan(_step_fn, (rng, timestep), None, length=num_steps)\n",
        "\n",
        "    return transitions, actions\n",
        "  return rollout"
      ],
      "metadata": {
        "id": "SOVYwUxm70Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-8x8\")\n",
        "env = GymAutoResetWrapper(env)\n",
        "\n",
        "rollout_fn = jax.jit(build_rollout(env, env_params, num_steps=1000))\n",
        "\n",
        "transitions, actions = rollout_fn(jax.random.key(0))"
      ],
      "metadata": {
        "id": "bAmVT6PtTAnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transitions shapes: \\n\", jtu.tree_map(jnp.shape, transitions))\n",
        "print(\"Actions shape:\", actions.shape)\n",
        "print(type(actions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp44_CUcTpXV",
        "outputId": "5e4d114f-d520-4272-8620-a9451aebf73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transitions shapes: \n",
            " TimeStep(state=State(key=(1000,), step_num=(1000,), grid=(1000, 8, 8, 2), agent=AgentState(position=(1000, 2), direction=(1000,), pocket=(1000, 2)), goal_encoding=(1000, 5), rule_encoding=(1000, 1, 7), carry=EnvCarry()), step_type=(1000,), reward=(1000,), discount=(1000,), observation=(1000, 7, 7, 2))\n",
            "Actions shape: (1000,)\n",
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_replay_buffer(transitions, actions):\n",
        "\n",
        "  observations = transitions.observation # (T, 7, 7, 2)\n",
        "  rewards = transitions.reward # (T,)\n",
        "  dones = transitions.step_type == 2 # (T,)\n",
        "  next_observations = jnp.concatenate([observations[1:], observations[-1:]], axis=0) #(T, 7, 7, 2)\n",
        "  actions = jnp.array(actions, dtype=jnp.int32) #(T,)\n",
        "\n",
        "  replay_buffer = {'observations': observations,\n",
        "                   'actions': actions,\n",
        "                   'rewards': rewards,\n",
        "                   'next_observations': next_observations,\n",
        "                   'dones': dones}\n",
        "\n",
        "  # print(\"=== Replay Buffer 构建完成 ===\")\n",
        "  # print(f\"数据点数量: {len(observations)}\")\n",
        "  # print(f\"平均奖励: {jnp.mean(rewards):.4f}\")\n",
        "  # print(f\"Episode结束次数: {jnp.sum(dones)}\")\n",
        "  # print(f\"动作分布: {jnp.bincount(actions)}\")\n",
        "  return replay_buffer"
      ],
      "metadata": {
        "id": "pcUGKy_aSeK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potential issue with sparse reward"
      ],
      "metadata": {
        "id": "0JzlW7Ht0M9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = create_replay_buffer(transitions, actions)"
      ],
      "metadata": {
        "id": "QygjqK1SzZr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(replay_buffer, batch_size=32, num_batches=None):\n",
        "  data_size = len(replay_buffer['observations'])\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = max(1, data_size // batch_size)\n",
        "\n",
        "  batches = []\n",
        "\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "-1lsLd27zekf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD3BC"
      ],
      "metadata": {
        "id": "UPNRnO8D4DqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "XvngrFj44C2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions"
      ],
      "metadata": {
        "id": "qqcNMUJUuGu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "hU9mkQlN3qNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BCConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"TD3-BC\"\n",
        "    project: str = \"train-TD3-BC\"\n",
        "    env_name: str = \"MiniGrid-Empty-8x8\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100000\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Sequence[int] = (256, 256)\n",
        "    critic_lr: float = 1e-3\n",
        "    actor_lr: float = 1e-3\n",
        "    # TD3-BC SPECIFIC\n",
        "    policy_freq: int = 2  # update actor every policy_freq updates\n",
        "    alpha: float = 2.5  # BC loss weight\n",
        "    policy_noise_std: float = 0.2  # std of policy noise\n",
        "    policy_noise_clip: float = 0.5  # clip policy noise\n",
        "    tau: float = 0.005  # target network update rate\n",
        "    discount: float = 0.99  # discount factor\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "conf_dict = OmegaConf.from_cli() # CLI Input\n",
        "config = TD3BCConfig(**conf_dict)\n",
        "\n",
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    if i + 1 < len(self.hidden_dims):\n",
        "                        x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "class DoubleCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observation: jnp.ndarray, action: jnp.ndarray\n",
        "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "        x = jnp.concatenate([observation, action], axis=-1)\n",
        "        q1 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        q2 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "class TD3Actor(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    max_action: float = 1.0  # In D4RL, action is scaled to [-1, 1]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observation: jnp.ndarray) -> jnp.ndarray:\n",
        "        action = MLP((*self.hidden_dims, self.action_dim))(observation)\n",
        "        action = self.max_action * jnp.tanh(\n",
        "            action\n",
        "        )  # scale to [-max_action, max_action]\n",
        "        return action\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "\n",
        "class TD3BCTrainState(NamedTuple):\n",
        "    actor: TrainState\n",
        "    critic: TrainState\n",
        "    target_actor: TrainState\n",
        "    target_critic: TrainState\n",
        "    max_action: float = 1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "Wyj1sj8G1O0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3BC Object"
      ],
      "metadata": {
        "id": "eJ50iJGFuprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BC(object):\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            predicted_action = train_state.actor.apply_fn(\n",
        "                actor_params, batch.observations\n",
        "            )\n",
        "            critic_params = jax.lax.stop_gradient(train_state.critic.params)\n",
        "            q_value, _ = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, predicted_action\n",
        "            )\n",
        "\n",
        "            mean_abs_q = jax.lax.stop_gradient(jnp.abs(q_value).mean())\n",
        "            loss_lambda = config.alpha / mean_abs_q\n",
        "\n",
        "            bc_loss = jnp.square(predicted_action - batch.actions).mean()\n",
        "            loss_actor = -1.0 * q_value.mean() * loss_lambda + bc_loss\n",
        "            return loss_actor\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q_pred_1, q_pred_2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            target_next_action = train_state.target_actor.apply_fn(\n",
        "                train_state.target_actor.params, batch.next_observations\n",
        "            )\n",
        "            policy_noise = (\n",
        "                config.policy_noise_std\n",
        "                * train_state.max_action\n",
        "                * jax.random.normal(rng, batch.actions.shape)\n",
        "            )\n",
        "            target_next_action = target_next_action + policy_noise.clip(\n",
        "                -config.policy_noise_clip, config.policy_noise_clip\n",
        "            )\n",
        "            target_next_action = target_next_action.clip(\n",
        "                -train_state.max_action, train_state.max_action\n",
        "            )\n",
        "            q_next_1, q_next_2 = train_state.target_critic.apply_fn(\n",
        "                train_state.target_critic.params,\n",
        "                batch.next_observations,\n",
        "                target_next_action,\n",
        "            )\n",
        "            target = batch.rewards[..., None] + config.discount * jnp.minimum(\n",
        "                q_next_1, q_next_2\n",
        "            ) * (1 - batch.dones[..., None])\n",
        "            target = jax.lax.stop_gradient(target)  # stop gradient for target\n",
        "            value_loss_1 = jnp.square(q_pred_1 - target)\n",
        "            value_loss_2 = jnp.square(q_pred_2 - target)\n",
        "            value_loss = (value_loss_1 + value_loss_2).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        data: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", Dict]:\n",
        "        for _ in range(\n",
        "            config.n_jitted_updates\n",
        "        ):  # we can jit for roop for static unroll\n",
        "            rng, batch_rng = jax.random.split(rng, 2)\n",
        "            batch_idx = jax.random.randint(\n",
        "                batch_rng, (config.batch_size,), 0, len(data.observations)\n",
        "            )\n",
        "            batch: Transition = jax.tree_util.tree_map(lambda x: x[batch_idx], data)\n",
        "            rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "            train_state, critic_loss = self.update_critic(\n",
        "                train_state, batch, critic_rng, config\n",
        "            )\n",
        "            if _ % config.policy_freq == 0:\n",
        "                train_state, actor_loss = self.update_actor(\n",
        "                    train_state, batch, actor_rng, config\n",
        "                )\n",
        "                new_target_critic = target_update(\n",
        "                    train_state.critic, train_state.target_critic, config.tau\n",
        "                )\n",
        "                new_target_actor = target_update(\n",
        "                    train_state.actor, train_state.target_actor, config.tau\n",
        "                )\n",
        "                train_state = train_state._replace(\n",
        "                    target_critic=new_target_critic,\n",
        "                    target_actor=new_target_actor,\n",
        "                )\n",
        "        return train_state, {\n",
        "            \"critic_loss\": critic_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        obs: jnp.ndarray,\n",
        "        max_action: float = 1.0,  # In D4RL, action is scaled to [-1, 1]\n",
        "    ) -> jnp.ndarray:\n",
        "        action = train_state.actor.apply_fn(train_state.actor.params, obs)\n",
        "        action = action.clip(-max_action, max_action)\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "aAkZZy3ouKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TrainState"
      ],
      "metadata": {
        "id": "MBarr3TLvEuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_td3bc_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: TD3BCConfig,\n",
        ") -> TD3BCTrainState:\n",
        "    critic_model = DoubleCritic(\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    action_dim = actions.shape[-1]\n",
        "    actor_model = TD3Actor(\n",
        "        action_dim=action_dim,\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "    # initialize critic\n",
        "    critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    target_critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    # initialize actor\n",
        "    actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    target_actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    return TD3BCTrainState(\n",
        "        actor=actor_train_state,\n",
        "        critic=critic_train_state,\n",
        "        target_actor=target_actor_train_state,\n",
        "        target_critic=target_critic_train_state,\n",
        "    )"
      ],
      "metadata": {
        "id": "VllbSO2Bu7oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "b64_CT78vHk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn: Callable[[jnp.ndarray], jnp.ndarray],\n",
        "    env_name: str,\n",
        "    num_episodes: int,\n",
        "    obs_mean,\n",
        "    obs_std,\n",
        "    max_steps_per_episode: int = 100,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    评估策略\n",
        "\n",
        "    Args:\n",
        "        policy_fn: 策略函数\n",
        "        env_name: 环境名称\n",
        "        num_episodes: episode数量\n",
        "        obs_mean: observation均值\n",
        "        obs_std: observation标准差\n",
        "        max_steps_per_episode: 每个episode的最大步数\n",
        "\n",
        "    Returns:\n",
        "        平均episode回报\n",
        "    \"\"\"\n",
        "    # 创建环境\n",
        "    env, env_params = xminigrid.make(env_name)\n",
        "\n",
        "    # 创建数据处理器\n",
        "    processor = MinariDataProcessor(\n",
        "        image_size=(22, 22),\n",
        "        use_mission=False,\n",
        "        normalize_image=True,\n",
        "        normalize_direction=True\n",
        "    )\n",
        "\n",
        "    episode_returns = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        episode_return = 0\n",
        "        timestep = env.reset(env_params, jax.random.PRNGKey(episode))\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            # 处理observation - xminigrid的observation是直接的JAX数组\n",
        "            obs_array = timestep.observation\n",
        "            obs_numpy = np.array(obs_array)\n",
        "\n",
        "            # xminigrid的observation形状是(7, 7, 2)\n",
        "            if obs_numpy.shape == (7, 7, 2):\n",
        "                # 将(7, 7, 2)转换为(7, 7, 3)的RGB图像\n",
        "                object_types = obs_numpy[:, :, 0]\n",
        "                colors = obs_numpy[:, :, 1]\n",
        "\n",
        "                rgb_image = np.zeros((7, 7, 3), dtype=np.uint8)\n",
        "                rgb_image[:, :, 0] = colors\n",
        "                rgb_image[:, :, 1] = object_types\n",
        "                rgb_image[:, :, 2] = 0\n",
        "\n",
        "                # 上采样到22x22\n",
        "                from scipy.ndimage import zoom\n",
        "                try:\n",
        "                    rgb_image = zoom(rgb_image, (22/7, 22/7, 1), order=0)\n",
        "                except ImportError:\n",
        "                    rgb_image = np.repeat(np.repeat(rgb_image, 3, axis=0), 3, axis=1)\n",
        "                    rgb_image = rgb_image[:22, :22, :]\n",
        "\n",
        "                direction = np.array([0.0])\n",
        "            else:\n",
        "                rgb_image = obs_numpy\n",
        "                direction = np.array([0.0])\n",
        "\n",
        "            obs_dict = {\n",
        "                'image': rgb_image,\n",
        "                'direction': direction\n",
        "            }\n",
        "            processed_obs = processor.process_observation(obs_dict)\n",
        "\n",
        "            # 归一化observation\n",
        "            if obs_mean is not None and obs_std is not None:\n",
        "                processed_obs = (processed_obs - obs_mean) / obs_std\n",
        "\n",
        "            # 获取动作\n",
        "            action = policy_fn(obs=processed_obs)\n",
        "\n",
        "            # 执行动作\n",
        "            timestep = env.step(env_params, timestep, action)\n",
        "            episode_return += timestep.reward\n",
        "\n",
        "            if timestep.is_done():\n",
        "                break\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "    return np.mean(episode_returns)\n"
      ],
      "metadata": {
        "id": "ISV0wZG6vBMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(project=config.project, config=config)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    # dataset, obs_mean, obs_std = get_dataset(config)\n",
        "\n",
        "    # create train_state\n",
        "    rng, subkey = jax.random.split(rng)\n",
        "    # example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state = create_td3bc_train_state(\n",
        "        subkey, example_batch.observations, example_batch.actions, config\n",
        "    )\n",
        "    algo = TD3BC()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, update_rng = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(\n",
        "            train_state,\n",
        "            dataset,\n",
        "            update_rng,\n",
        "            config,\n",
        "        )  # update parameters\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        if i % eval_interval == 0:\n",
        "            policy_fn = partial(act_fn, train_state=train_state)\n",
        "            normalized_score = evaluate(\n",
        "                policy_fn,\n",
        "                config.env_name,\n",
        "                num_episodes=config.eval_episodes,\n",
        "                obs_mean=obs_mean,\n",
        "                obs_std=obs_std,\n",
        "            )\n",
        "            print(i, normalized_score)\n",
        "            eval_metrics = {f\"{config.env_name}/episode_return\": normalized_score}\n",
        "            wandb.log(eval_metrics, step=i)\n",
        "\n",
        "    # final evaluation\n",
        "    policy_fn = partial(act_fn, train_state=train_state)\n",
        "    normalized_score = evaluate(\n",
        "        policy_fn,\n",
        "        config.env_name,\n",
        "        num_episodes=config.eval_episodes,\n",
        "        obs_mean=obs_mean,\n",
        "        obs_std=obs_std,\n",
        "    )\n",
        "    print(\"Final Evaluation Score:\", normalized_score)\n",
        "    wandb.log({f\"{config.env_name}/final_episode_return\": normalized_score})\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "5eHE93XFvDk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yy-bA2CviLu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}