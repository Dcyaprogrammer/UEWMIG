{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0NstW8rlmhcB",
        "QvkJdJ9EnpNT",
        "UPNRnO8D4DqD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJjXifWFe9K"
      },
      "outputs": [],
      "source": [
        "%pip install jax\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install xminigrid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as jtu\n",
        "import numpy as np\n",
        "\n",
        "import timeit\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "import xminigrid"
      ],
      "metadata": {
        "id": "0gdi3TeiF481"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeStep(struct.PyTreeNode):\n",
        "    # hidden environment state, such as grid, agent, goal, etc\n",
        "    state: State\n",
        "\n",
        "    # similar to the dm_env enterface\n",
        "    step_type: StepType\n",
        "    reward: jax.Array\n",
        "    discount: jax.Array\n",
        "    observation: jax.Array"
      ],
      "metadata": {
        "id": "4L7duDLiJ9Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect Rollouts"
      ],
      "metadata": {
        "id": "hNLDJZfm4Hx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xminigrid.wrappers import GymAutoResetWrapper\n",
        "\n",
        "def build_rollout(env, env_params, num_steps):\n",
        "  def rollout(rng):\n",
        "    def _step_fn(carry, _):\n",
        "      rng, timestep = carry\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      action = jax.random.randint(_rng, shape=(), minval=0, maxval=env.num_actions(env_params))\n",
        "\n",
        "      timestep = env.step(env_params, timestep, action)\n",
        "\n",
        "      return (rng, timestep), (timestep,action)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    timestep = env.reset(env_params, _rng)\n",
        "    rng, (transitions, actions) = jax.lax.scan(_step_fn, (rng, timestep), None, length=num_steps)\n",
        "\n",
        "    return transitions, actions\n",
        "  return rollout"
      ],
      "metadata": {
        "id": "SOVYwUxm70Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-6x6\")\n",
        "env = GymAutoResetWrapper(env)\n",
        "\n",
        "rollout_fn = jax.jit(build_rollout(env, env_params, num_steps=1000))\n",
        "\n",
        "transitions, actions = rollout_fn(jax.random.key(0))"
      ],
      "metadata": {
        "id": "bAmVT6PtTAnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transitions shapes: \\n\", jtu.tree_map(jnp.shape, transitions))\n",
        "print(\"Actions shape:\", actions.shape)\n",
        "print(type(actions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp44_CUcTpXV",
        "outputId": "0b1f1d9d-4366-4217-c84c-c6f44a990c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transitions shapes: \n",
            " TimeStep(state=State(key=(1000,), step_num=(1000,), grid=(1000, 6, 6, 2), agent=AgentState(position=(1000, 2), direction=(1000,), pocket=(1000, 2)), goal_encoding=(1000, 5), rule_encoding=(1000, 1, 7), carry=EnvCarry()), step_type=(1000,), reward=(1000,), discount=(1000,), observation=(1000, 7, 7, 2))\n",
            "Actions shape: (1000,)\n",
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_replay_buffer(transitions, actions):\n",
        "\n",
        "  observations = transitions.observation # (T, 7, 7, 2)\n",
        "  rewards = transitions.reward # (T,)\n",
        "  dones = transitions.step_type == 2 # (T,)\n",
        "  next_observations = jnp.concatenate([observations[1:], observations[-1:]], axis=0) #(T, 7, 7, 2)\n",
        "  actions = jnp.array(actions, dtype=jnp.int32) #(T,)\n",
        "\n",
        "  replay_buffer = {'observations': observations,\n",
        "                   'actions': actions,\n",
        "                   'rewards': rewards,\n",
        "                   'next_observations': next_observations,\n",
        "                   'dones': dones}\n",
        "\n",
        "  # print(\"=== Replay Buffer 构建完成 ===\")\n",
        "  # print(f\"数据点数量: {len(observations)}\")\n",
        "  # print(f\"平均奖励: {jnp.mean(rewards):.4f}\")\n",
        "  # print(f\"Episode结束次数: {jnp.sum(dones)}\")\n",
        "  # print(f\"动作分布: {jnp.bincount(actions)}\")\n",
        "  return replay_buffer"
      ],
      "metadata": {
        "id": "pcUGKy_aSeK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potential issue with sparse reward"
      ],
      "metadata": {
        "id": "0JzlW7Ht0M9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = create_replay_buffer(transitions, actions)"
      ],
      "metadata": {
        "id": "QygjqK1SzZr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(replay_buffer, batch_size=32, num_batches=None):\n",
        "  data_size = len(replay_buffer['observations'])\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = max(1, data_size // batch_size)\n",
        "\n",
        "  batches = []\n",
        "\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "-1lsLd27zekf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IQL"
      ],
      "metadata": {
        "id": "yQSQh8M2lsdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install distrax"
      ],
      "metadata": {
        "id": "xXYr_IUtmVWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import distrax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n"
      ],
      "metadata": {
        "id": "WNXxwUJSmMkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "0NstW8rlmhcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"IQL\"\n",
        "    project: str = \"train-IQL\"\n",
        "    env_name: str = \"MiniGrid-EmptyRandom-6x6\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100000\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = False\n",
        "    normalize_reward: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Tuple[int, int] = (256, 256)\n",
        "    actor_lr: float = 3e-4\n",
        "    value_lr: float = 3e-4\n",
        "    critic_lr: float = 3e-4\n",
        "    layer_norm: bool = True\n",
        "    opt_decay_schedule: bool = True\n",
        "    # IQL SPECIFIC\n",
        "    expectile: float = (\n",
        "        0.7  # FYI: for Hopper-me, 0.5 produce better result. (antmaze: expectile=0.9)\n",
        "    )\n",
        "    beta: float = (\n",
        "        3.0  # FYI: for Hopper-me, 6.0 produce better result. (antmaze: beta=10.0)\n",
        "    )\n",
        "    tau: float = 0.005\n",
        "    discount: float = 0.99\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "\n",
        "conf_dict = OmegaConf.from_cli()\n",
        "config = IQLConfig(**conf_dict)"
      ],
      "metadata": {
        "id": "WcEIlxtMmkI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Networks"
      ],
      "metadata": {
        "id": "FW1HYQS-m0BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray, actions: jnp.ndarray) -> jnp.ndarray:\n",
        "        r_observations = observations.reshape(-1)\n",
        "        r_actions = actions.reshape(-1)\n",
        "        inputs = jnp.concatenate([r_observations, r_actions], -1)\n",
        "        critic = MLP((*self.hidden_dims, 1), activations=self.activations)(inputs)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    split_rngs = kwargs.pop(\"split_rngs\", {})\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={\"params\": 0},\n",
        "        split_rngs={**split_rngs, \"params\": True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "class ValueCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        critic = MLP((*self.hidden_dims, 1), layer_norm=self.layer_norm)(observations)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    log_std_min: Optional[float] = -5.0\n",
        "    log_std_max: Optional[float] = 2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observations: jnp.ndarray, temperature: float = 1.0\n",
        "    ) -> distrax.Distribution:\n",
        "        outputs = MLP(\n",
        "            self.hidden_dims,\n",
        "            activate_final=True,\n",
        "        )(observations)\n",
        "\n",
        "        means = nn.Dense(\n",
        "            self.action_dim, kernel_init=default_init()\n",
        "        )(outputs)\n",
        "        log_stds = self.param(\"log_stds\", nn.initializers.zeros, (self.action_dim,))\n",
        "        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        distribution = distrax.MultivariateNormalDiag(\n",
        "            loc=means, scale_diag=jnp.exp(log_stds) * temperature\n",
        "        )\n",
        "        return distribution"
      ],
      "metadata": {
        "id": "9nqPOs5Umxhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "QvkJdJ9EnpNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jtu.tree_map(jnp.shape, replay_buffer))\n",
        "print(type(replay_buffer))\n",
        "print(replay_buffer[\"dones\"])"
      ],
      "metadata": {
        "id": "dtarzRCfprRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "    dones_float: jnp.ndarray"
      ],
      "metadata": {
        "id": "tJxWv8DinrFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalization(dataset: Transition) -> float:\n",
        "    # into numpy.ndarray\n",
        "    dataset = jax.tree_util.tree_map(lambda x: np.array(x), dataset)\n",
        "    returns = []\n",
        "    ret = 0\n",
        "    for r, term in zip(dataset.rewards, dataset.dones_float):\n",
        "        ret += r\n",
        "        if term:\n",
        "            returns.append(ret)\n",
        "            ret = 0\n",
        "    return (max(returns) - min(returns)) / 1000"
      ],
      "metadata": {
        "id": "ga3QaSnJntei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(\n",
        "     dataset: dict, config: IQLConfig, clip_to_eps: bool = True, eps: float = 1e-5\n",
        ") -> Transition:\n",
        "\n",
        "    if clip_to_eps:\n",
        "        lim = 1 - eps\n",
        "        dataset[\"actions\"] = jnp.clip(dataset[\"actions\"], -lim, lim)\n",
        "\n",
        "    dones_float = np.zeros_like(dataset['dones'])\n",
        "\n",
        "    for i in range(len(dones_float) - 1):\n",
        "        if np.linalg.norm(dataset['observations'][i + 1] -\n",
        "                            dataset['next_observations'][i]\n",
        "                            ) > 1e-6 or dataset['dones'][i] == True:\n",
        "            dones_float[i] = 1\n",
        "        else:\n",
        "            dones_float[i] = 0\n",
        "    dones_float[-1] = 1\n",
        "\n",
        "    dataset = Transition(\n",
        "        observations=jnp.array(dataset[\"observations\"], dtype=jnp.float32),\n",
        "        actions=jnp.array(dataset[\"actions\"], dtype=jnp.float32),\n",
        "        rewards=jnp.array(dataset[\"rewards\"], dtype=jnp.float32),\n",
        "        next_observations=jnp.array(dataset[\"next_observations\"], dtype=jnp.float32),\n",
        "        dones=jnp.array(dataset[\"dones\"], dtype=jnp.float32),\n",
        "        dones_float=jnp.array(dones_float, dtype=jnp.float32),\n",
        "    )\n",
        "\n",
        "    # normalize states\n",
        "    obs_mean, obs_std = 0, 1\n",
        "    if config.normalize_state:\n",
        "        obs_mean = dataset.observations.mean(0)\n",
        "        obs_std = dataset.observations.std(0)\n",
        "        dataset = dataset._replace(\n",
        "            observations=(dataset.observations - obs_mean) / (obs_std + 1e-5),\n",
        "            next_observations=(dataset.next_observations - obs_mean) / (obs_std + 1e-5),\n",
        "        )\n",
        "    # normalize rewards\n",
        "    if config.normalize_reward:\n",
        "        normalizing_factor = get_normalization(dataset)\n",
        "        dataset = dataset._replace(rewards=dataset.rewards / normalizing_factor)\n",
        "\n",
        "    # shuffle data and select the first data_size samples\n",
        "    data_size = min(config.data_size, len(dataset.observations))\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    rng, rng_permute, rng_select = jax.random.split(rng, 3)\n",
        "    perm = jax.random.permutation(rng_permute, len(dataset.observations))\n",
        "    dataset = jax.tree_util.tree_map(lambda x: x[perm], dataset)\n",
        "    assert len(dataset.observations) >= data_size\n",
        "    dataset = jax.tree_util.tree_map(lambda x: x[:data_size], dataset)\n",
        "    return dataset, obs_mean, obs_std"
      ],
      "metadata": {
        "id": "Q4-51Dpin7az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectile_loss(diff, expectile=0.8) -> jnp.ndarray:\n",
        "    weight = jnp.where(diff > 0, expectile, (1 - expectile))\n",
        "    return weight * (diff**2)\n",
        "\n",
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "qlDMKO3toAvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "3jfs8WxhoP3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLTrainState(NamedTuple):\n",
        "    rng: jax.random.PRNGKey\n",
        "    critic: TrainState\n",
        "    target_critic: TrainState\n",
        "    value: TrainState\n",
        "    actor: TrainState\n",
        "\n",
        "class IQL(object):\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        next_v = train_state.value.apply_fn(\n",
        "            train_state.value.params, batch.next_observations\n",
        "        )\n",
        "        target_q = batch.rewards + config.discount * (1 - batch.dones) * next_v\n",
        "\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q1, q2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            critic_loss = ((q1 - target_q) ** 2 + (q2 - target_q) ** 2).mean()\n",
        "            return critic_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_value(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        q1, q2 = train_state.target_critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jax.lax.stop_gradient(jnp.minimum(q1, q2))\n",
        "        def value_loss_fn(value_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            v = train_state.value.apply_fn(value_params, batch.observations)\n",
        "            value_loss = expectile_loss(q - v, config.expectile).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_value, value_loss = update_by_loss_grad(train_state.value, value_loss_fn)\n",
        "        return train_state._replace(value=new_value), value_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        v = train_state.value.apply_fn(train_state.value.params, batch.observations)\n",
        "        q1, q2 = train_state.critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jnp.minimum(q1, q2)\n",
        "        exp_a = jnp.exp((q - v) * config.beta)\n",
        "        exp_a = jnp.minimum(exp_a, 100.0)\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            dist = train_state.actor.apply_fn(actor_params, batch.observations)\n",
        "            log_probs = dist.log_prob(batch.actions)\n",
        "            actor_loss = -(exp_a * log_probs).mean()\n",
        "            return actor_loss\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        dataset: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: IQLConfig,\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        for _ in range(config.n_jitted_updates):\n",
        "            rng, subkey = jax.random.split(rng)\n",
        "            batch_indices = jax.random.randint(\n",
        "                subkey, (config.batch_size,), 0, len(dataset.observations)\n",
        "            )\n",
        "            batch = jax.tree_util.tree_map(lambda x: x[batch_indices], dataset)\n",
        "\n",
        "            train_state, value_loss = self.update_value(train_state, batch, config)\n",
        "            train_state, actor_loss = self.update_actor(train_state, batch, config)\n",
        "            train_state, critic_loss = self.update_critic(train_state, batch, config)\n",
        "            new_target_critic = target_update(\n",
        "                train_state.critic, train_state.target_critic, config.tau\n",
        "            )\n",
        "            train_state = train_state._replace(target_critic=new_target_critic)\n",
        "        return train_state, {\n",
        "            \"value_loss\": value_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "            \"critic_loss\": critic_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        observations: np.ndarray,\n",
        "        seed: jax.random.PRNGKey,\n",
        "        temperature: float = 1.0,\n",
        "        max_action: float = 1.0,  # In D4RL, the action space is [-1, 1]\n",
        "    ) -> jnp.ndarray:\n",
        "\n",
        "        # modified for discrete actions\n",
        "        logits = train_state.actor.apply_fn(\n",
        "            train_state.actor.params, observations, temperature=temperature\n",
        "        ).sample(seed=seed)\n",
        "        actions = logits.mean()\n",
        "        # print(\"actions:\", actions, \"shape:\", actions.shape, \"dtype:\", actions.dtype)\n",
        "        return actions"
      ],
      "metadata": {
        "id": "S3ZiQLAwoV_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Evaluate"
      ],
      "metadata": {
        "id": "pJXpO90HoiJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_iql_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: IQLConfig,\n",
        ") -> IQLTrainState:\n",
        "    rng, actor_rng, critic_rng, value_rng = jax.random.split(rng, 4)\n",
        "    # initialize actor\n",
        "    action_dim = 1 # change for minigrid\n",
        "    actor_model = GaussianPolicy(\n",
        "        config.hidden_dims,\n",
        "        action_dim=action_dim,\n",
        "        log_std_min=-5.0,\n",
        "    )\n",
        "    if config.opt_decay_schedule:\n",
        "        schedule_fn = optax.cosine_decay_schedule(-config.actor_lr, config.max_steps)\n",
        "        actor_tx = optax.chain(optax.scale_by_adam(), optax.scale_by_schedule(schedule_fn))\n",
        "    else:\n",
        "        actor_tx = optax.adam(learning_rate=config.actor_lr)\n",
        "    actor = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=actor_tx,\n",
        "    )\n",
        "    # initialize critic\n",
        "    critic_model = ensemblize(Critic, num_qs=2)(config.hidden_dims)\n",
        "    critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    target_critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    # initialize value\n",
        "    value_model = ValueCritic(config.hidden_dims, layer_norm=config.layer_norm)\n",
        "    value = TrainState.create(\n",
        "        apply_fn=value_model.apply,\n",
        "        params=value_model.init(value_rng, observations),\n",
        "        tx=optax.adam(learning_rate=config.value_lr),\n",
        "    )\n",
        "    return IQLTrainState(\n",
        "        rng,\n",
        "        critic=critic,\n",
        "        target_critic=target_critic,\n",
        "        value=value,\n",
        "        actor=actor,\n",
        "    )"
      ],
      "metadata": {
        "id": "yZvRadfAokla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn, env, env_params, num_episodes: int, obs_mean: float, obs_std: float, rng\n",
        ") -> float:\n",
        "    episode_returns = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      episode_return = 0\n",
        "\n",
        "      timestep = env.reset(env_params, _rng)\n",
        "      done = timestep.step_type == 2\n",
        "      observation = timestep.observation\n",
        "\n",
        "      while not done:\n",
        "          norm_obs = (observation - obs_mean) / (obs_std + 1e-5)\n",
        "          action = policy_fn(observations=norm_obs)\n",
        "\n",
        "          timestep = env.step(env_params, timestep, action)\n",
        "          reward = timestep.reward\n",
        "          done = timestep.step_type == 2\n",
        "          observation = timestep.observation\n",
        "\n",
        "          episode_return += reward\n",
        "      episode_returns.append(episode_return)\n",
        "    return float(jnp.mean(jnp.array(episode_returns)))"
      ],
      "metadata": {
        "id": "9B6VH9JgoptW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Evaluate"
      ],
      "metadata": {
        "id": "EV4q58W408x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = jax.random.PRNGKey(config.seed)\n",
        "rng, _rng = jax.random.split(rng)\n",
        "\n",
        "dataset, obs_mean, obs_std = preprocess_dataset(replay_buffer, config)\n",
        "# print(jtu.tree_map(jnp.shape, dataset))\n",
        "unit_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "# print(jtu.tree_map(jnp.shape, unit_batch))\n",
        "\n",
        "train_state: IQLTrainState = create_iql_train_state(\n",
        "    _rng,\n",
        "    unit_batch.observations,\n",
        "    unit_batch.actions,\n",
        "    config\n",
        ")\n",
        "\n",
        "algo = IQL()\n",
        "update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "act_fn = jax.jit(algo.get_action)\n",
        "num_steps = config.max_steps // config.n_jitted_updates\n",
        "eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "\n",
        "policy_fn = partial(\n",
        "    act_fn,\n",
        "    temperature=0.0,\n",
        "    seed=jax.random.PRNGKey(0),\n",
        "    train_state=train_state,\n",
        ")\n",
        "\n",
        "normalized_score = evaluate(\n",
        "    policy_fn,\n",
        "    env,\n",
        "    env_params,\n",
        "    num_episodes=config.eval_episodes,\n",
        "    obs_mean=obs_mean,\n",
        "    obs_std=obs_std,\n",
        "    rng = _rng\n",
        ")\n",
        "print(\"Final Evaluation\", normalized_score)"
      ],
      "metadata": {
        "id": "H-Jjuyog069Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(config=config, project=config.project)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "\n",
        "    env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-6x6\")\n",
        "    env = GymAutoResetWrapper(env)\n",
        "\n",
        "    dataset, obs_mean, obs_std = preprocess_dataset(replay_buffer, config)\n",
        "\n",
        "    # create train_state\n",
        "    example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state: IQLTrainState = create_iql_train_state(\n",
        "        _rng,\n",
        "        example_batch.observations,\n",
        "        example_batch.actions,\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    algo = IQL()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(train_state, dataset, subkey, config)\n",
        "\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        # if i % eval_interval == 0:\n",
        "        #     policy_fn = partial(\n",
        "        #         act_fn,\n",
        "        #         temperature=0.0,\n",
        "        #         seed=jax.random.PRNGKey(0),\n",
        "        #         train_state=train_state,\n",
        "        #     )\n",
        "        #     normalized_score = evaluate(\n",
        "        #         policy_fn,\n",
        "        #         env,\n",
        "        #         num_episodes=config.eval_episodes,\n",
        "        #         obs_mean=obs_mean,\n",
        "        #         obs_std=obs_std,\n",
        "        #     )\n",
        "        #     print(i, normalized_score)\n",
        "        #     eval_metrics = {f\"{config.env_name}/normalized_score\": normalized_score}\n",
        "        #     wandb.log(eval_metrics, step=i)\n",
        "    # final evaluation\n",
        "    # policy_fn = partial(\n",
        "    #     act_fn,\n",
        "    #     temperature=0.0,\n",
        "    #     seed=jax.random.PRNGKey(0),\n",
        "    #     train_state=train_state,\n",
        "    # )\n",
        "    # normalized_score = evaluate(\n",
        "    #     policy_fn,\n",
        "    #     env,\n",
        "    #     num_episodes=config.eval_episodes,\n",
        "    #     obs_mean=obs_mean,\n",
        "    #     obs_std=obs_std,\n",
        "    # )\n",
        "    # print(\"Final Evaluation\", normalized_score)\n",
        "    # wandb.log({f\"{config.env_name}/final_normalized_score\": normalized_score})\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mQkNH7rvo2mS",
        "outputId": "08e2cadc-1397-4356-f42d-b997bc3d1639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdcyjustin\u001b[0m (\u001b[33mdcyjustin-xi-an-jiaotong-liverpool-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250623_132815-zq92o9um</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dcyjustin-xi-an-jiaotong-liverpool-university/train-IQL/runs/zq92o9um' target=\"_blank\">usual-wind-1</a></strong> to <a href='https://wandb.ai/dcyjustin-xi-an-jiaotong-liverpool-university/train-IQL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dcyjustin-xi-an-jiaotong-liverpool-university/train-IQL' target=\"_blank\">https://wandb.ai/dcyjustin-xi-an-jiaotong-liverpool-university/train-IQL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dcyjustin-xi-an-jiaotong-liverpool-university/train-IQL/runs/zq92o9um' target=\"_blank\">https://wandb.ai/dcyjustin-xi-an-jiaotong-liverpool-university/train-IQL/runs/zq92o9um</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/125000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ScopeParamShapeError",
          "evalue": "Initializer expected to generate shape (99, 256) but got shape (25344, 256) instead for parameter \"kernel\" in \"/vmap()/MLP_0/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-78-2113022861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-1947713527.py\u001b[0m in \u001b[0;36mupdate_n_times\u001b[0;34m(self, train_state, dataset, rng, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-1947713527.py\u001b[0m in \u001b[0;36mupdate_value\u001b[0;34m(self, train_state, batch, config)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIQLTrainState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTransition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIQLConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     ) -> Tuple[\"IQLTrainState\", Dict]:\n\u001b[0;32m---> 37\u001b[0;31m         q1, q2 = train_state.target_critic.apply_fn(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         )\n",
            "    \u001b[0;31m[... skipping hidden 18 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-73-1935468795.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, observations, actions)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mr_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-73-1935468795.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dims\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate_final\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Add layer norm after activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \"\"\"\n\u001b[0;32m--> 263\u001b[0;31m     kernel = self.param(\n\u001b[0m\u001b[1;32m    264\u001b[0m       \u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mparam\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# for inference to a half float type for example.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m           raise errors.ScopeParamShapeError(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m           )\n",
            "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (99, 256) but got shape (25344, 256) instead for parameter \"kernel\" in \"/vmap()/MLP_0/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD3BC"
      ],
      "metadata": {
        "id": "UPNRnO8D4DqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "XvngrFj44C2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions"
      ],
      "metadata": {
        "id": "qqcNMUJUuGu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "hU9mkQlN3qNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BCConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"TD3-BC\"\n",
        "    project: str = \"train-TD3-BC\"\n",
        "    env_name: str = \"MiniGrid-Empty-8x8\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100000\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Sequence[int] = (256, 256)\n",
        "    critic_lr: float = 1e-3\n",
        "    actor_lr: float = 1e-3\n",
        "    # TD3-BC SPECIFIC\n",
        "    policy_freq: int = 2  # update actor every policy_freq updates\n",
        "    alpha: float = 2.5  # BC loss weight\n",
        "    policy_noise_std: float = 0.2  # std of policy noise\n",
        "    policy_noise_clip: float = 0.5  # clip policy noise\n",
        "    tau: float = 0.005  # target network update rate\n",
        "    discount: float = 0.99  # discount factor\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "conf_dict = OmegaConf.from_cli() # CLI Input\n",
        "config = TD3BCConfig(**conf_dict)\n",
        "\n",
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    if i + 1 < len(self.hidden_dims):\n",
        "                        x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "class DoubleCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observation: jnp.ndarray, action: jnp.ndarray\n",
        "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "        x = jnp.concatenate([observation, action], axis=-1)\n",
        "        q1 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        q2 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "class TD3Actor(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    max_action: float = 1.0  # In D4RL, action is scaled to [-1, 1]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observation: jnp.ndarray) -> jnp.ndarray:\n",
        "        action = MLP((*self.hidden_dims, self.action_dim))(observation)\n",
        "        action = self.max_action * jnp.tanh(\n",
        "            action\n",
        "        )  # scale to [-max_action, max_action]\n",
        "        return action\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "\n",
        "class TD3BCTrainState(NamedTuple):\n",
        "    actor: TrainState\n",
        "    critic: TrainState\n",
        "    target_actor: TrainState\n",
        "    target_critic: TrainState\n",
        "    max_action: float = 1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "Wyj1sj8G1O0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3BC Object"
      ],
      "metadata": {
        "id": "eJ50iJGFuprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BC(object):\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            predicted_action = train_state.actor.apply_fn(\n",
        "                actor_params, batch.observations\n",
        "            )\n",
        "            critic_params = jax.lax.stop_gradient(train_state.critic.params)\n",
        "            q_value, _ = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, predicted_action\n",
        "            )\n",
        "\n",
        "            mean_abs_q = jax.lax.stop_gradient(jnp.abs(q_value).mean())\n",
        "            loss_lambda = config.alpha / mean_abs_q\n",
        "\n",
        "            bc_loss = jnp.square(predicted_action - batch.actions).mean()\n",
        "            loss_actor = -1.0 * q_value.mean() * loss_lambda + bc_loss\n",
        "            return loss_actor\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q_pred_1, q_pred_2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            target_next_action = train_state.target_actor.apply_fn(\n",
        "                train_state.target_actor.params, batch.next_observations\n",
        "            )\n",
        "            policy_noise = (\n",
        "                config.policy_noise_std\n",
        "                * train_state.max_action\n",
        "                * jax.random.normal(rng, batch.actions.shape)\n",
        "            )\n",
        "            target_next_action = target_next_action + policy_noise.clip(\n",
        "                -config.policy_noise_clip, config.policy_noise_clip\n",
        "            )\n",
        "            target_next_action = target_next_action.clip(\n",
        "                -train_state.max_action, train_state.max_action\n",
        "            )\n",
        "            q_next_1, q_next_2 = train_state.target_critic.apply_fn(\n",
        "                train_state.target_critic.params,\n",
        "                batch.next_observations,\n",
        "                target_next_action,\n",
        "            )\n",
        "            target = batch.rewards[..., None] + config.discount * jnp.minimum(\n",
        "                q_next_1, q_next_2\n",
        "            ) * (1 - batch.dones[..., None])\n",
        "            target = jax.lax.stop_gradient(target)  # stop gradient for target\n",
        "            value_loss_1 = jnp.square(q_pred_1 - target)\n",
        "            value_loss_2 = jnp.square(q_pred_2 - target)\n",
        "            value_loss = (value_loss_1 + value_loss_2).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        data: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", Dict]:\n",
        "        for _ in range(\n",
        "            config.n_jitted_updates\n",
        "        ):  # we can jit for roop for static unroll\n",
        "            rng, batch_rng = jax.random.split(rng, 2)\n",
        "            batch_idx = jax.random.randint(\n",
        "                batch_rng, (config.batch_size,), 0, len(data.observations)\n",
        "            )\n",
        "            batch: Transition = jax.tree_util.tree_map(lambda x: x[batch_idx], data)\n",
        "            rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "            train_state, critic_loss = self.update_critic(\n",
        "                train_state, batch, critic_rng, config\n",
        "            )\n",
        "            if _ % config.policy_freq == 0:\n",
        "                train_state, actor_loss = self.update_actor(\n",
        "                    train_state, batch, actor_rng, config\n",
        "                )\n",
        "                new_target_critic = target_update(\n",
        "                    train_state.critic, train_state.target_critic, config.tau\n",
        "                )\n",
        "                new_target_actor = target_update(\n",
        "                    train_state.actor, train_state.target_actor, config.tau\n",
        "                )\n",
        "                train_state = train_state._replace(\n",
        "                    target_critic=new_target_critic,\n",
        "                    target_actor=new_target_actor,\n",
        "                )\n",
        "        return train_state, {\n",
        "            \"critic_loss\": critic_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        obs: jnp.ndarray,\n",
        "        max_action: float = 1.0,  # In D4RL, action is scaled to [-1, 1]\n",
        "    ) -> jnp.ndarray:\n",
        "        action = train_state.actor.apply_fn(train_state.actor.params, obs)\n",
        "        action = action.clip(-max_action, max_action)\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "aAkZZy3ouKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TrainState"
      ],
      "metadata": {
        "id": "MBarr3TLvEuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_td3bc_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: TD3BCConfig,\n",
        ") -> TD3BCTrainState:\n",
        "    critic_model = DoubleCritic(\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    action_dim = actions.shape[-1]\n",
        "    actor_model = TD3Actor(\n",
        "        action_dim=action_dim,\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "    # initialize critic\n",
        "    critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    target_critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    # initialize actor\n",
        "    actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    target_actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    return TD3BCTrainState(\n",
        "        actor=actor_train_state,\n",
        "        critic=critic_train_state,\n",
        "        target_actor=target_actor_train_state,\n",
        "        target_critic=target_critic_train_state,\n",
        "    )"
      ],
      "metadata": {
        "id": "VllbSO2Bu7oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "b64_CT78vHk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn: Callable[[jnp.ndarray], jnp.ndarray],\n",
        "    env_name: str,\n",
        "    num_episodes: int,\n",
        "    obs_mean,\n",
        "    obs_std,\n",
        "    max_steps_per_episode: int = 100,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    评估策略\n",
        "\n",
        "    Args:\n",
        "        policy_fn: 策略函数\n",
        "        env_name: 环境名称\n",
        "        num_episodes: episode数量\n",
        "        obs_mean: observation均值\n",
        "        obs_std: observation标准差\n",
        "        max_steps_per_episode: 每个episode的最大步数\n",
        "\n",
        "    Returns:\n",
        "        平均episode回报\n",
        "    \"\"\"\n",
        "    # 创建环境\n",
        "    env, env_params = xminigrid.make(env_name)\n",
        "\n",
        "    episode_returns = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        episode_return = 0\n",
        "        timestep = env.reset(env_params, jax.random.PRNGKey(episode))\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            # 处理observation - xminigrid的observation是直接的JAX数组\n",
        "            obs_array = timestep.observation\n",
        "            obs_numpy = np.array(obs_array)\n",
        "\n",
        "            # xminigrid的observation形状是(7, 7, 2)\n",
        "            if obs_numpy.shape == (7, 7, 2):\n",
        "                # 将(7, 7, 2)转换为(7, 7, 3)的RGB图像\n",
        "                object_types = obs_numpy[:, :, 0]\n",
        "                colors = obs_numpy[:, :, 1]\n",
        "\n",
        "                rgb_image = np.zeros((7, 7, 3), dtype=np.uint8)\n",
        "                rgb_image[:, :, 0] = colors\n",
        "                rgb_image[:, :, 1] = object_types\n",
        "                rgb_image[:, :, 2] = 0\n",
        "\n",
        "                # 上采样到22x22\n",
        "                from scipy.ndimage import zoom\n",
        "                try:\n",
        "                    rgb_image = zoom(rgb_image, (22/7, 22/7, 1), order=0)\n",
        "                except ImportError:\n",
        "                    rgb_image = np.repeat(np.repeat(rgb_image, 3, axis=0), 3, axis=1)\n",
        "                    rgb_image = rgb_image[:22, :22, :]\n",
        "\n",
        "                direction = np.array([0.0])\n",
        "            else:\n",
        "                rgb_image = obs_numpy\n",
        "                direction = np.array([0.0])\n",
        "\n",
        "            obs_dict = {\n",
        "                'image': rgb_image,\n",
        "                'direction': direction\n",
        "            }\n",
        "            processed_obs = processor.process_observation(obs_dict)\n",
        "\n",
        "            # 归一化observation\n",
        "            if obs_mean is not None and obs_std is not None:\n",
        "                processed_obs = (processed_obs - obs_mean) / obs_std\n",
        "\n",
        "            # 获取动作\n",
        "            action = policy_fn(obs=processed_obs)\n",
        "\n",
        "            # 执行动作\n",
        "            timestep = env.step(env_params, timestep, action)\n",
        "            episode_return += timestep.reward\n",
        "\n",
        "            if timestep.is_done():\n",
        "                break\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "    return np.mean(episode_returns)\n"
      ],
      "metadata": {
        "id": "ISV0wZG6vBMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(project=config.project, config=config)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    # dataset, obs_mean, obs_std = get_dataset(config)\n",
        "\n",
        "    # create train_state\n",
        "    rng, subkey = jax.random.split(rng)\n",
        "    # example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state = create_td3bc_train_state(\n",
        "        subkey, example_batch.observations, example_batch.actions, config\n",
        "    )\n",
        "    algo = TD3BC()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, update_rng = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(\n",
        "            train_state,\n",
        "            dataset,\n",
        "            update_rng,\n",
        "            config,\n",
        "        )  # update parameters\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        if i % eval_interval == 0:\n",
        "            policy_fn = partial(act_fn, train_state=train_state)\n",
        "            normalized_score = evaluate(\n",
        "                policy_fn,\n",
        "                config.env_name,\n",
        "                num_episodes=config.eval_episodes,\n",
        "                obs_mean=obs_mean,\n",
        "                obs_std=obs_std,\n",
        "            )\n",
        "            print(i, normalized_score)\n",
        "            eval_metrics = {f\"{config.env_name}/episode_return\": normalized_score}\n",
        "            wandb.log(eval_metrics, step=i)\n",
        "\n",
        "    # final evaluation\n",
        "    policy_fn = partial(act_fn, train_state=train_state)\n",
        "    normalized_score = evaluate(\n",
        "        policy_fn,\n",
        "        config.env_name,\n",
        "        num_episodes=config.eval_episodes,\n",
        "        obs_mean=obs_mean,\n",
        "        obs_std=obs_std,\n",
        "    )\n",
        "    print(\"Final Evaluation Score:\", normalized_score)\n",
        "    wandb.log({f\"{config.env_name}/final_episode_return\": normalized_score})\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "5eHE93XFvDk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j06BRkx7luFa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}