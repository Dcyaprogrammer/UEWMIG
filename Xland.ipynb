{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oSjqwPwiXxCf",
        "fYSuZTE1Y4NA",
        "UFZ3e361Y6hb",
        "MBDZtOMH5FSQ",
        "JsW95dg75Jij",
        "8bYxWU5s5PCm",
        "Z6TMiJUFZ_uY",
        "yQSQh8M2lsdJ",
        "hNLDJZfm4Hx7",
        "UPNRnO8D4DqD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zvJjXifWFe9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3a4665-6c97-4742-c072-2e234b9c0742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting xminigrid\n",
            "  Downloading xminigrid-0.9.1-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from xminigrid) (0.5.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from xminigrid) (0.5.1)\n",
            "Requirement already satisfied: flax>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from xminigrid) (0.10.6)\n",
            "Requirement already satisfied: rich>=13.4.2 in /usr/local/lib/python3.11/dist-packages (from xminigrid) (13.9.4)\n",
            "Requirement already satisfied: chex>=0.1.85 in /usr/local/lib/python3.11/dist-packages (from xminigrid) (0.1.89)\n",
            "Requirement already satisfied: imageio>=2.31.2 in /usr/local/lib/python3.11/dist-packages (from xminigrid) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.4.9 in /usr/local/lib/python3.11/dist-packages (from xminigrid) (0.6.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.85->xminigrid) (1.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.85->xminigrid) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.85->xminigrid) (2.0.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.85->xminigrid) (0.12.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.8.0->xminigrid) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.8.0->xminigrid) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.8.0->xminigrid) (0.11.16)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.8.0->xminigrid) (0.1.74)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.8.0->xminigrid) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.8.0->xminigrid) (0.1.9)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.31.2->xminigrid) (11.2.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->xminigrid) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->xminigrid) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->xminigrid) (1.15.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.4.2->xminigrid) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.4.2->xminigrid) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.4.2->xminigrid) (0.1.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.8.0->xminigrid) (1.12.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.8.0->xminigrid) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.8.0->xminigrid) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.8.0->xminigrid) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.8.0->xminigrid) (3.20.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8.0->xminigrid) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8.0->xminigrid) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8.0->xminigrid) (3.23.0)\n",
            "Downloading xminigrid-0.9.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xminigrid\n",
            "Successfully installed xminigrid-0.9.1\n",
            "Collecting gymnax\n",
            "  Downloading gymnax-0.0.9-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from gymnax) (0.5.2)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (from gymnax) (0.10.6)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from gymnax) (1.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from gymnax) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from gymnax) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.11.16)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.1.74)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (4.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.1.9)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (1.15.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->gymnax) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->gymnax) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn->gymnax) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->gymnax) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax->gymnax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax->gymnax) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax->flax->gymnax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax->gymnax) (0.1.89)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (1.12.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (3.20.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax->gymnax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->gymnax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (3.23.0)\n",
            "Downloading gymnax-0.0.9-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnax\n",
            "Successfully installed gymnax-0.0.9\n",
            "Collecting distrax\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.1.89)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.5.2)\n",
            "Requirement already satisfied: jaxlib>=0.1.67 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (2.0.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.8->distrax) (4.14.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.8->distrax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.1.55->distrax) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.1.55->distrax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.1.55->distrax) (1.15.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability>=0.15.0->distrax) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability>=0.15.0->distrax) (1.17.2)\n",
            "Downloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distrax\n",
            "Successfully installed distrax-0.1.5\n"
          ]
        }
      ],
      "source": [
        "%pip install jax\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install xminigrid\n",
        "%pip install gymnax\n",
        "%pip install distrax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as jtu\n",
        "import numpy as np\n",
        "import distrax\n",
        "\n",
        "import timeit\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "from flax import nnx\n",
        "import xminigrid"
      ],
      "metadata": {
        "id": "0gdi3TeiF481"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TimeStep(struct.PyTreeNode):\n",
        "#     # hidden environment state, such as grid, agent, goal, etc\n",
        "#     state: State\n",
        "\n",
        "#     # similar to the dm_env enterface\n",
        "#     step_type: StepType\n",
        "#     reward: jax.Array\n",
        "#     discount: jax.Array\n",
        "#     observation: jax.Array"
      ],
      "metadata": {
        "id": "4L7duDLiJ9Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "oSjqwPwiXxCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoders"
      ],
      "metadata": {
        "id": "fYSuZTE1Y4NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.linear(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class ActionEncoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.embed = nnx.Embed(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.embed(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class JointEncoder(nnx.Module):\n",
        "  def __init__(self, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear1 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm1 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm2 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm3 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array, rng):\n",
        "    dist_distrax = distrax.MultivariateNormalDiag(loc=x, scale_diag=1e-1*jnp.ones_like(x))\n",
        "    # potential shape issue\n",
        "    x = dist_distrax.sample(seed=rng, sample_shape=(1,))\n",
        "    x = self.layer_norm0(x)\n",
        "    h0 = self.linear1(x)\n",
        "    h = nn.relu(h0)\n",
        "    h = self.layer_norm1(h) + h0\n",
        "    h0 = self.linear2(h)\n",
        "    h = self.layer_norm2(h) + h0\n",
        "    return self.layer_norm3(h)"
      ],
      "metadata": {
        "id": "kZlsjAvVXzLc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.nn import one_hot\n",
        "\n",
        "class ImageOneHotEncoder(nnx.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 num_title_types: int,  # New: Number of categories for the first channel\n",
        "                 num_colors: int,\n",
        "                 rngs: nnx.Rngs,\n",
        "                 hidden_dim=32,):\n",
        "\n",
        "        self.num_title_types = num_title_types\n",
        "        self.num_colors = num_colors\n",
        "\n",
        "        self.input_channels_after_one_hot = self.num_title_types + self.num_colors\n",
        "\n",
        "        self.conv1 = nnx.Conv(\n",
        "            self.input_channels_after_one_hot, hidden_dim, kernel_size=(3, 3), strides=(1, 1), rngs=rngs)\n",
        "        self.conv2 = nnx.Conv(hidden_dim, hidden_dim*2, kernel_size=(3, 3), strides=(1, 1), rngs=rngs)\n",
        "        self.conv3 = nnx.Conv(hidden_dim*2, hidden_dim*4, kernel_size=(3, 3), strides=(1, 1), rngs=rngs)\n",
        "\n",
        "        self.linear = nnx.Linear(1, output_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, obs_raw: jnp.ndarray) -> jnp.ndarray:\n",
        "\n",
        "        original_ndim = obs_raw.ndim\n",
        "        # ensure batch dimension\n",
        "        if original_ndim == 3:\n",
        "            obs_raw = jnp.expand_dims(obs_raw, axis=0)\n",
        "\n",
        "        title_ids = obs_raw[..., 0]\n",
        "        color_ids = obs_raw[..., 1]\n",
        "\n",
        "        titles_onehot = one_hot(title_ids, num_classes=self.num_title_types)\n",
        "        colors_onehot = one_hot(color_ids, num_classes=self.num_colors)\n",
        "\n",
        "        obs_processed = jnp.concatenate(\n",
        "            [titles_onehot, colors_onehot], axis=-1\n",
        "        )\n",
        "\n",
        "        x = nnx.relu(self.conv1(obs_processed))\n",
        "        x = nnx.relu(self.conv2(x))\n",
        "        x = nnx.relu(self.conv3(x))\n",
        "\n",
        "        flattened_features = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        output_features = self.linear(flattened_features)\n",
        "\n",
        "        # Remove the batch dimension if the original input didn't have one\n",
        "        if original_ndim == 3:\n",
        "            return jnp.squeeze(output_features, axis=0)\n",
        "        else:\n",
        "            return output_features"
      ],
      "metadata": {
        "id": "Mo4iz0KsXn4W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.nn import one_hot\n",
        "\n",
        "\n",
        "NUM_TITLE_TYPES = 13\n",
        "NUM_COLORS = 12\n",
        "\n",
        "\n",
        "class ObservationActionEncoder(nnx.Module):\n",
        "    def __init__(self,\n",
        "                 num_actions: int,\n",
        "                 hidden_dim: int,\n",
        "                 action_embedding_dim: int,\n",
        "                 output_channels: int,\n",
        "                 num_title_types: int,\n",
        "                 num_colors: int,\n",
        "                 rngs: nnx.Rngs):\n",
        "\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.action_embedding_dim = action_embedding_dim\n",
        "        self.num_title_types = num_title_types\n",
        "        self.num_colors = num_colors\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        self.action_linear_embed = nnx.Linear(num_actions, action_embedding_dim, rngs=rngs)\n",
        "        self.action_layer_norm = nnx.LayerNorm(action_embedding_dim, rngs=rngs)\n",
        "\n",
        "        self.obs_input_channels_after_one_hot = self.num_title_types + self.num_colors\n",
        "\n",
        "        self.cnn_input_channels = self.obs_input_channels_after_one_hot + self.action_embedding_dim\n",
        "\n",
        "        self.conv1 = nnx.Conv(self.cnn_input_channels, 64, kernel_size=(3, 3), strides=(1, 1), padding='SAME', rngs=rngs)\n",
        "        self.conv2 = nnx.Conv(hidden_dim, hidden_dim*2, kernel_size=(3, 3), strides=(1, 1), padding='SAME', rngs=rngs)\n",
        "        self.conv_final = nnx.Conv(hidden_dim*2, self.output_channels, kernel_size=(1, 1), strides=(1, 1), padding='SAME', rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, obs_raw: jnp.ndarray, action: jnp.ndarray) -> jnp.ndarray:\n",
        "\n",
        "        #output: (Batch, H_out, W_out, output_channels)\n",
        "\n",
        "        original_obs_ndim = obs_raw.ndim\n",
        "        if original_obs_ndim == 3:\n",
        "            obs_raw = jnp.expand_dims(obs_raw, axis=0)\n",
        "\n",
        "        title_ids = obs_raw[..., 0]\n",
        "        color_ids = obs_raw[..., 1]\n",
        "\n",
        "        titles_onehot = one_hot(title_ids, num_classes=self.num_title_types)\n",
        "        colors_onehot = one_hot(color_ids, num_classes=self.num_colors)\n",
        "\n",
        "        # obs_processed : (Batch, H, W, self.obs_input_channels_after_one_hot)\n",
        "        obs_processed = jnp.concatenate(\n",
        "            [titles_onehot, colors_onehot], axis=-1\n",
        "        )\n",
        "\n",
        "        action = action.astype(jnp.int32)\n",
        "\n",
        "        action_one_hot = one_hot(action, num_classes=self.num_actions)\n",
        "\n",
        "        action_embedding = self.action_linear_embed(action_one_hot)\n",
        "        action_embedding = self.action_layer_norm(action_embedding) # (Batch, action_embedding_dim)|(action_embedding_dim,)\n",
        "\n",
        "\n",
        "        H, W = obs_processed.shape[1], obs_processed.shape[2]\n",
        "\n",
        "        if action_embedding.ndim == 1:\n",
        "            action_embedding = jnp.expand_dims(action_embedding, axis=0) # (1, action_embedding_dim)\n",
        "\n",
        "        #  action_embedding  -> (Batch, 1, 1, action_embedding_dim)\n",
        "        #  (Batch, H, W, action_embedding_dim)\n",
        "        action_spatial = jnp.expand_dims(jnp.expand_dims(action_embedding, axis=1), axis=1) # (Batch, 1, 1, D)\n",
        "        action_spatial = jnp.tile(action_spatial, (1, H, W, 1)) # (Batch, H, W, D)\n",
        "\n",
        "        #  (Batch, H, W, self.cnn_input_channels)\n",
        "        cnn_input = jnp.concatenate([obs_processed, action_spatial], axis=-1)\n",
        "\n",
        "        x = nnx.relu(self.conv1(cnn_input))\n",
        "        x = nnx.relu(self.conv2(x))\n",
        "        output_feature_map = self.conv_final(x)\n",
        "\n",
        "        if original_obs_ndim == 3:\n",
        "            return jnp.squeeze(output_feature_map, axis=0)\n",
        "        else:\n",
        "            return output_feature_map"
      ],
      "metadata": {
        "id": "0vUL9WewrbG3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor"
      ],
      "metadata": {
        "id": "UFZ3e361Y6hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import lax\n",
        "import distrax\n",
        "\n",
        "class Actor(nnx.Module):\n",
        "  # environment related ???\n",
        "  log_std_min: float = -4\n",
        "  log_std_max: float = 2\n",
        "\n",
        "  def __init__(self, obs_dim, action_dim, hidden_dim, rngs: nnx.Rngs):\n",
        "    self.mean = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "    self.log_std = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    mean = self.mean(x)\n",
        "    log_std = jnp.clip(self.log_std(x), self.log_std_min, self.log_std_max)\n",
        "    return mean, log_std"
      ],
      "metadata": {
        "id": "jODz_HXsY_MY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "-qfKrd1lkNHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### computaion"
      ],
      "metadata": {
        "id": "MBDZtOMH5FSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_info_gain_normal(mean, prec, l_prec, next_obs):\n",
        "  prec = jnp.maximum(prec, 1e-6)\n",
        "  posterior_prec = prec + l_prec\n",
        "  prec_ratio = prec / posterior_prec\n",
        "\n",
        "  posterior_mean = (prec * mean + l_prec * next_obs) / posterior_prec\n",
        "\n",
        "  delta_mean = next_obs - posterior_mean\n",
        "  kl = delta_mean * delta_mean * prec\n",
        "  kl = kl + prec_ratio - jnp.log(prec_ratio) - 1\n",
        "  kl = 0.5 * jnp.sum(kl, axis=-1)\n",
        "  return kl, delta_mean\n",
        "\n",
        "@jax.jit\n",
        "def compute_expected_info_gain_normal(prec, l_prec):\n",
        "  prec = jnp.maximum(prec, 1e-6)\n",
        "  prec_ratio = l_prec / prec\n",
        "  mi_matrix = 0.5 * jnp.sum(jnp.log(1+prec_ratio), axis=-1)\n",
        "  return mi_matrix\n",
        "\n",
        "jnp.set_printoptions(precision=3,suppress=True)\n",
        "from flax.training import train_state\n",
        "from jax.scipy.special import gamma,digamma, gammaln, kl_div\n",
        "\n",
        "def batch_random_split(batch_key,num=2):\n",
        "    split_keys = jax.vmap(jax.random.split,in_axes=(0,None))(batch_key,num)\n",
        "    return [split_keys[:, i]  for i in range(num) ]\n"
      ],
      "metadata": {
        "id": "5HFYlDrokQXw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### shape manipulation"
      ],
      "metadata": {
        "id": "JsW95dg75Jij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reshape(arr):\n",
        "  if arr.dim < 3:\n",
        "    raise ValueError(\"Input array must have at least 3 dimensions (n, b, c, ...).\")\n",
        "\n",
        "  n, b, c, *x_dims = arr.shapes\n",
        "  # Transpose the first two axes (n, b) to (b, n)\n",
        "  # We construct the axes tuple dynamically for flexibility\n",
        "  transpose_axes = (1, 0) + tuple(range(2, arr.ndim))\n",
        "  transposed_arr = jnp.transpose(arr, axes=transpose_axes)\n",
        "  # Reshape into (b, n*c, x0, x1, ...)\n",
        "  new_shape = (b, n * c, *x_dims)\n",
        "  reshaped_arr = jnp.reshape(transposed_arr, new_shape)\n",
        "\n",
        "  return reshaped_arr\n",
        "\n",
        "from typing import List, Any\n",
        "\n",
        "# Define a type alias for PyTree for better readability\n",
        "PyTree = Any\n",
        "from typing import List, Any\n",
        "\n",
        "# Define a type alias for PyTree for better readability\n",
        "PyTree = Any\n",
        "def unpack_pytree_by_first_index(pytree: PyTree) -> List[PyTree]:\n",
        "    \"\"\"\n",
        "    Unpacks a PyTree of JAX arrays along their first dimension (id).\n",
        "\n",
        "    This function assumes that all JAX arrays within the PyTree\n",
        "    have a consistent first dimension (the 'id' dimension) and that\n",
        "    you want to create a separate PyTree for each 'id'.\n",
        "\n",
        "    Args:\n",
        "        pytree: A JAX PyTree where the leaves are JAX arrays\n",
        "                with a leading 'id' dimension.\n",
        "\n",
        "    Returns:\n",
        "        A list of PyTrees, where each PyTree corresponds to a single\n",
        "        'id' from the original PyTree.\n",
        "    \"\"\"\n",
        "    # Get the size of the first dimension from any leaf array\n",
        "    # We assume all arrays have the same first dimension size.\n",
        "    first_leaf = jax.tree_util.tree_leaves(pytree)[0]\n",
        "    num_ids = first_leaf.shape[0]\n",
        "\n",
        "    # Create a list to store the unpacked PyTrees\n",
        "    unpacked_pytrees = []\n",
        "\n",
        "    # Iterate through each ID\n",
        "    for i in range(num_ids):\n",
        "        # Use tree_map to slice each array in the PyTree at the current ID\n",
        "        sliced_pytree = jax.tree_util.tree_map(lambda x: x[i], pytree)\n",
        "        unpacked_pytrees.append(sliced_pytree)\n",
        "\n",
        "    return unpacked_pytrees\n",
        "\n",
        "def unpack_states(pytree):\n",
        "    return unpack_pytree_by_first_index(jax.tree.map(reshape, pytree))"
      ],
      "metadata": {
        "id": "D9NZ-QNk5AKs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### drawing"
      ],
      "metadata": {
        "id": "8bYxWU5s5PCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_mountain_car_heatmap(state,config = {}):\n",
        "    \"\"\"\n",
        "    Draws a heatmap representing the trajectory of the MountainCar environment.\n",
        "\n",
        "    Args:\n",
        "        state_sequence: A sequence of JAX arrays representing the states\n",
        "                        of the MountainCar environment. Each state is expected\n",
        "                        to be a 2-element array [position, velocity].\n",
        "                        ['CartPole-v1',\"MountainCar-v0\",\"Acrobot-v1\"]\n",
        "    \"\"\"\n",
        "    title = config[\"ENV_NAME\"] +' MountainCar Heatmap ' +config[\"MODEL_NAME\"]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if config[\"ENV_NAME\"] == \"MountainCar-v0\":\n",
        "\n",
        "        positions = state.position\n",
        "        velocities = state.velocity\n",
        "\n",
        "        plt.scatter(positions, velocities, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('Position')\n",
        "        plt.ylabel('Velocity')\n",
        "        plt.grid(True)\n",
        "    elif config[\"ENV_NAME\"] == \"CartPole-v1\":\n",
        "        x = state.x\n",
        "        theta = state.theta\n",
        "        plt.scatter(x, theta, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('theta')\n",
        "        plt.grid(True)\n",
        "    elif config[\"ENV_NAME\"] == \"Acrobot-v1\":\n",
        "        joint_angle1 = state.joint_angle1\n",
        "        joint_angle2 = state.joint_angle2\n",
        "        plt.scatter(joint_angle1, joint_angle2, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('Angle1')\n",
        "        plt.ylabel('Angle2')\n",
        "        plt.grid(True)\n",
        "    if \"TOTAL_TIMESTEPS\" in config:\n",
        "        title += \"_TOTAL_TIMESTEPS_\"+str(config[\"TOTAL_TIMESTEPS\"])\n",
        "    if \"DEPTH\" in config:\n",
        "        title += \"_DEPTH_\"+str(config[\"DEPTH\"])\n",
        "    if \"NUM_HIDDEN\" in config:\n",
        "        title += \"_NUM_HIDDEN_\"+str(config[\"NUM_HIDDEN\"])\n",
        "    plt.title(title)\n",
        "    plt.savefig(title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return plt"
      ],
      "metadata": {
        "id": "ZWqlXrDe5SG4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Others"
      ],
      "metadata": {
        "id": "Z6TMiJUFZ_uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Likelihood_Prec(nnx.Module):\n",
        "  log_std_min: float = -2\n",
        "  log_std_max: float = 2\n",
        "\n",
        "  def __init__(self, obs_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear = nnx.Linear(hidden_dim, obs_dim, rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    log_std = jnp.clip(self.linear(x), self.log_std_min, self.log_std_max)\n",
        "    return jnp.exp(-log_std)"
      ],
      "metadata": {
        "id": "Tqfsq_SyaBKk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Explorer"
      ],
      "metadata": {
        "id": "ck6iG5qLTJTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnax.experimental import RolloutWrapper\n",
        "# action = self.model_forward(policy_params, obs, rng_net)\n",
        "import functools\n",
        "import gymnax\n",
        "from typing import Union,Optional,Any\n",
        "import abc"
      ],
      "metadata": {
        "id": "XYxtYAbZTLyM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnsupervisedExplorer(nnx.Module):\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def update(self, obs, actions, next_obs, dones, info):\n",
        "    # update variable parameters\n",
        "    return #{'kl':KL} MI= E[KL]\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self, observations, rng):\n",
        "    return #actions, {\"mi\":mi_matrix}\n",
        "\n",
        "class RandomExplorer(UnsupervisedExplorer):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "  def update(self, rng, obs, actions, next_obs, dones, info):\n",
        "    return {}\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "    if observations.ndim == 1:\n",
        "      # possible shape issue here\n",
        "      actions = jax.random.randint(rng, shape=(1,), minval=0, maxval=self.num_actions)\n",
        "      return actions, {}\n",
        "    actions = jax.random.randint(rng, shape=(observations.shape[0],), minval=0, maxval=self.num_actions)\n",
        "    return actions, {}\n",
        "\n",
        "\n",
        "class DeepSACBayesianExplorer(UnsupervisedExplorer):\n",
        "  # ent?\n",
        "  def __init__(self, obs_dim, num_actions, hidden_dim, rngs: nnx.Rngs,\n",
        "               l_prec=1.0, wd=1e-2, ent_lambda=1e-3, depth=2):\n",
        "    self.obs_dim = obs_dim\n",
        "    self.num_actions = num_actions\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.prec_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)), name='prec_w')\n",
        "    self.mean_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)), name='mean_w')\n",
        "    # what is trainable here\n",
        "    self.trainable_likelihood_prec = Likelihood_Prec(obs_dim, hidden_dim, rngs)\n",
        "    self.weight_decay = wd\n",
        "    self.obs_embeds = Encoder(obs_dim, hidden_dim, rngs)\n",
        "    self.action_embeds = ActionEncoder(num_actions, hidden_dim, rngs)\n",
        "    self.joint_embeds = JointEncoder(hidden_dim, rngs)\n",
        "    self.depth = depth\n",
        "    self.ent_lambda = ent_lambda\n",
        "\n",
        "  def __call__(self,observations,rng):\n",
        "      return self.recursive_mi(observations,rng,self.depth)\n",
        "\n",
        "  def update(self, rng, obs, action, next_obs, done, info):\n",
        "    mean = info[\"mean\"]\n",
        "    prec = info[\"prec\"]\n",
        "\n",
        "    def _likelihood_loss(rng, T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "      mu = mean\n",
        "      # model var + inherent var\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "      log_prob = dist_distrax.log_prob(next_obs)\n",
        "      return -log_prob, l_prec\n",
        "\n",
        "    # jit here\n",
        "    predictive_loss, l_prec = _likelihood_loss(rng, info[\"T\"], mean, prec, next_obs)\n",
        "    # originally jnp.sum\n",
        "    mean_error = jnp.mean((mean - next_obs)**2)\n",
        "    deepkl, delta_mean = compute_info_gain_normal(mean, prec, l_prec, next_obs)\n",
        "    # batch x num_hidden\n",
        "    T = info[\"T\"].reshape(-1, self.hidden_dim)\n",
        "\n",
        "    # batch x obs_dim\n",
        "    l_prec = l_prec.reshape(-1, self.obs_dim)\n",
        "    delta_mean = delta_mean.reshape(-1, self.obs_dim)\n",
        "\n",
        "    T_T = jnp.transpose(T)\n",
        "    covariance = T @ T_T\n",
        "    inv_covariance = jnp.linalg.pinv(covariance)\n",
        "\n",
        "    T_Map = T_T @ inv_covariance\n",
        "\n",
        "    delta_precW = T_Map @ l_prec\n",
        "    self.prec_w.value = (self.prec_w.value + delta_precW) * (1-self.weight_decay)\n",
        "    delta_meanW = T_Map @ delta_mean\n",
        "    self.mean_w.value = (self.mean_w.value + delta_meanW) * (1-self.weight_decay)\n",
        "\n",
        "    return {\"kl\":deepkl,  \"predictive_loss\": predictive_loss, \"mean_error\":mean_error}\n",
        "\n",
        "  # jitable\n",
        "  def loss(self, rng, obs, action, next_obs, done, info):\n",
        "    def _likelihood_loss(T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "      mu = mean\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "\n",
        "      log_prob = dist_distrax.log_prob(next_obs)\n",
        "      return -log_prob\n",
        "\n",
        "    T, mean, prec = info[\"T\"], info[\"mean\"], info[\"prec\"]\n",
        "    likelihood_loss = _likelihood_loss(T, mean, prec, next_obs)\n",
        "    return likelihood_loss\n",
        "\n",
        "  def batch_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "    vmapped = jax.vmap(self.loss)\n",
        "    return vmapped(rng, obs, actions, next_obs, dones, info)\n",
        "\n",
        "  def recursive_mi(self, observations, rng, depth):\n",
        "    obs_embed = self.obs_embed(observations)\n",
        "    action_embed = self.action_embed(jnp.arange(self.num_actions))\n",
        "    # possible shape issue\n",
        "    embed = action_embed + jnp.expand_dims(obs_embed, axis=0)\n",
        "\n",
        "    # num_actions x embed_size\n",
        "    T = self.joint_embeds(embed, rng)\n",
        "    prec = jnp.maximum(T @ self.prec_w, 1e-3)\n",
        "    # num_actions x obs_dim\n",
        "    mean = T @ self.mean_w\n",
        "    l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "    MI = compute_expected_info_gain_normal(prec, l_prec)\n",
        "\n",
        "    if depth > 0:\n",
        "      vmapped = jax.vmap(self.recursive_mi, in_axes=(0,None,None))\n",
        "      # num_actions x 1\n",
        "      actions, info = vmapped(mean, rng, depth-1)\n",
        "      MI = MI + info[\"mi\"]\n",
        "\n",
        "    actions = jnp.argmax(MI, axis=0)\n",
        "    T = T[actions]\n",
        "    MI = MI[actions]\n",
        "    l_prec = l_prec[actions]\n",
        "    prec = prec[actions]\n",
        "    mean = mean[actions]\n",
        "    return actions, {\"mi\":MI,\"T\":T,\"obs_embed\":obs_embed,\"l_prec\":l_prec,\n",
        "                        \"prec\":prec,\"mean\":mean}\n",
        "\n",
        "def show_variable(model, text):\n",
        "\n",
        "    graphdef, params, vars,others = nnx.split(model, nnx.Param, nnx.Variable,...)\n",
        "\n",
        "    print(text,vars)\n"
      ],
      "metadata": {
        "id": "fqBtGnP6TewM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporarily requires oof_dim = hidden_dim"
      ],
      "metadata": {
        "id": "AxTPofOYcouf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XlandRandomExplorer(UnsupervisedExplorer):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "  def update(self, rng, obs, actions, next_obs, dones, info):\n",
        "    return {}\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "      batch_size = 1\n",
        "      if observations.ndim == 4:\n",
        "          batch_size = observations.shape[0]\n",
        "      elif observations.ndim == 3:\n",
        "          batch_size = 1\n",
        "      else:\n",
        "          if observations.ndim == 1: # Single 1D feature vector\n",
        "                batch_size = 1\n",
        "          elif observations.ndim == 2: # Batch of 1D feature vectors\n",
        "                batch_size = observations.shape[0]\n",
        "          else:\n",
        "              raise ValueError(f\"Unsupported observation dimension: {observations.ndim}. Expected 3 (HWC) or 4 (BHWC) or 1 (flat) or 2 (batch_flat).\")\n",
        "\n",
        "      actions = jax.random.randint(rng, shape=(batch_size,), minval=0, maxval=self.num_actions)\n",
        "      return actions, {}\n"
      ],
      "metadata": {
        "id": "X5r-6Xg_mpLz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XlandDeepSACBayesianExplorer(UnsupervisedExplorer):\n",
        "  # ent?\n",
        "  def __init__(self, obs_dim_raw_shape, oof_dim, num_actions, hidden_dim, rngs: nnx.Rngs,\n",
        "               l_prec=1.0, wd=1e-2, ent_lambda=1e-3, depth=2, num_title_types: int=13, num_colors: int=12):\n",
        "\n",
        "    self.obs_dim_raw_shape = obs_dim_raw_shape\n",
        "    self.num_actions = num_actions\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_obs_feature_dim = oof_dim\n",
        "\n",
        "\n",
        "    self.prec_w = nnx.Variable(jnp.zeros((hidden_dim, self.output_obs_feature_dim)), name='prec_w')\n",
        "    self.mean_w = nnx.Variable(jnp.zeros((hidden_dim, self.output_obs_feature_dim)), name='mean_w')\n",
        "\n",
        "    self.obs_embed = ImageOneHotEncoder(self.output_obs_feature_dim, num_title_types, num_colors, rngs, hidden_dim)\n",
        "    # what is trainable here\n",
        "    self.trainable_likelihood_prec = Likelihood_Prec(self.output_obs_feature_dim, hidden_dim, rngs)\n",
        "    self.weight_decay = wd\n",
        "    self.action_embeds = ActionEncoder(num_actions, hidden_dim, rngs)\n",
        "    self.joint_embeds = JointEncoder(hidden_dim, rngs)\n",
        "    self.depth = depth\n",
        "    self.ent_lambda = ent_lambda\n",
        "\n",
        "  def __call__(self,observations,rng):\n",
        "    return self.recursive_mi(observations,rng,self.depth)\n",
        "\n",
        "  def update(self, rng, obs, action, next_obs, done, info):\n",
        "    mean = info[\"mean\"]\n",
        "    prec = info[\"prec\"]\n",
        "\n",
        "    def _likelihood_loss(rng, T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "      mu = mean\n",
        "      # model var + inherent var\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "      next_obs_features = self.obs_embed(next_obs)\n",
        "      log_prob = dist_distrax.log_prob(next_obs_features)\n",
        "      return -log_prob, l_prec\n",
        "\n",
        "    # jit here\n",
        "    predictive_loss, l_prec = _likelihood_loss(rng, info[\"T\"], mean, prec, next_obs)\n",
        "    # originally jnp.sum\n",
        "    mean_error = jnp.mean((mean - next_obs)**2)\n",
        "    deepkl, delta_mean = compute_info_gain_normal(mean, prec, l_prec, next_obs)\n",
        "    # batch x num_hidden\n",
        "    T = info[\"T\"].reshape(-1, self.hidden_dim)\n",
        "\n",
        "    # batch x obs_dim\n",
        "    l_prec = l_prec.reshape(-1, self.obs_dim)\n",
        "    delta_mean = delta_mean.reshape(-1, self.obs_dim)\n",
        "\n",
        "    T_T = jnp.transpose(T)\n",
        "    covariance = T @ T_T\n",
        "    inv_covariance = jnp.linalg.pinv(covariance)\n",
        "\n",
        "    T_Map = T_T @ inv_covariance\n",
        "\n",
        "    delta_precW = T_Map @ l_prec\n",
        "    self.prec_w.value = (self.prec_w.value + delta_precW) * (1-self.weight_decay)\n",
        "    delta_meanW = T_Map @ delta_mean\n",
        "    self.mean_w.value = (self.mean_w.value + delta_meanW) * (1-self.weight_decay)\n",
        "\n",
        "    return {\"kl\":deepkl,  \"predictive_loss\": predictive_loss, \"mean_error\":mean_error}\n",
        "\n",
        "  # jitable\n",
        "  def loss(self, rng, obs, action, next_obs, done, info):\n",
        "    def _likelihood_loss(T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "      mu = mean\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "      next_obs_features = self.obs_embed(next_obs)\n",
        "      log_prob = dist_distrax.log_prob(next_obs_features)\n",
        "      return -log_prob\n",
        "\n",
        "    T, mean, prec = info[\"T\"], info[\"mean\"], info[\"prec\"]\n",
        "    likelihood_loss = _likelihood_loss(T, mean, prec, next_obs)\n",
        "    return likelihood_loss\n",
        "\n",
        "  def batch_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "    vmapped = jax.vmap(self.loss)\n",
        "    return vmapped(rng, obs, actions, next_obs, dones, info)\n",
        "\n",
        "  def recursive_mi(self, observations, rng, depth):\n",
        "    obs_embed = self.obs_embed(observations)\n",
        "    action_embed = self.action_embed(jnp.arange(self.num_actions))\n",
        "    # possible shape issue\n",
        "    embed = action_embed + jnp.expand_dims(obs_embed, axis=0)\n",
        "\n",
        "    # num_actions x embed_size\n",
        "    T = self.joint_embeds(embed, rng)\n",
        "    prec = jnp.maximum(T @ self.prec_w, 1e-3)\n",
        "    # num_actions x obs_dim\n",
        "    mean = T @ self.mean_w\n",
        "    l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "    MI = compute_expected_info_gain_normal(prec, l_prec)\n",
        "\n",
        "    if depth > 0:\n",
        "      vmapped = jax.vmap(self.recursive_mi, in_axes=(0,None,None))\n",
        "      # num_actions x 1\n",
        "      actions, info = vmapped(mean, rng, depth-1)\n",
        "      MI = MI + info[\"mi\"]\n",
        "\n",
        "    actions = jnp.argmax(MI, axis=0)\n",
        "    T = T[actions]\n",
        "    MI = MI[actions]\n",
        "    l_prec = l_prec[actions]\n",
        "    prec = prec[actions]\n",
        "    mean = mean[actions]\n",
        "    return actions, {\"mi\":MI,\"T\":T,\"obs_embed\":obs_embed,\"l_prec\":l_prec,\n",
        "                        \"prec\":prec,\"mean\":mean}"
      ],
      "metadata": {
        "id": "2qY8wdGtaff6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper"
      ],
      "metadata": {
        "id": "g0HO9tmL6n3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xminigrid.environment import Environment\n",
        "from typing import Union,Optional,Any\n",
        "import abc\n",
        "\n",
        "class CustomRolloutWrapper:\n",
        "    \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_or_name: Union[str,Environment] = \"Pendulum-v1\",\n",
        "        num_env_steps: Optional[int] = None,\n",
        "        env_kwargs: Any | None = None,\n",
        "        env_params: Any | None = None,\n",
        "    ):\n",
        "        \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "        # Define the RL environment & network forward function\n",
        "        if env_kwargs is None:\n",
        "            env_kwargs = {}\n",
        "        if env_params is None:\n",
        "            env_params = {}\n",
        "        if isinstance(env_or_name,Environment):\n",
        "            self.env = env_or_name\n",
        "            self.env_params = env_or_name.default_params\n",
        "        else:\n",
        "            self.env, self.env_params = xminigrid.make(env_or_name, **env_kwargs)\n",
        "        self.env_params = self.env_params.replace(**env_params)\n",
        "\n",
        "        if num_env_steps is None:\n",
        "            self.num_env_steps = self.env_params.max_steps\n",
        "        else:\n",
        "            self.num_env_steps = num_env_steps\n",
        "\n",
        "    def batch_reset(self, rng_input):\n",
        "        batch_rest = jax.vmap(self.single_reset_state)\n",
        "        return batch_rest(rng_input)\n",
        "\n",
        "    # state vs. timestep, potential issue here\n",
        "    def single_reset_state(self, rng_input):\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "        timestep = self.env.reset(self.env_params, rng_reset)\n",
        "        return timestep\n",
        "\n",
        "    def batch_rollout(self, rng_eval, model:UnsupervisedExplorer, timestep=None, num_steps=1):\n",
        "        batch_rollout = jax.vmap(self.single_rollout, in_axes=(0,None,None,None))\n",
        "        return batch_rollout(rng_eval, model, timestep, num_steps)\n",
        "\n",
        "    def single_rollout(self, rng_eval, model:UnsupervisedExplorer, timestep=None, num_steps=1):\n",
        "        rng_reset, rng_episode = jax.random.split(rng_eval)\n",
        "\n",
        "        if timestep is None:\n",
        "          timestep = self.env.reset(rng_reset, self.env_params)\n",
        "        else:\n",
        "          obs = timestep.observation\n",
        "\n",
        "        def policy_step(state_input, _):\n",
        "          obs, timestep, rng, cum_reward, valid_mask = state_input\n",
        "          rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "          if model is not None:\n",
        "            action, info = model(obs, rng_net)\n",
        "          else:\n",
        "            # not action space?\n",
        "            action = action = jax.random.randint(rng_step, shape=(), minval=0, maxval=self.env.num_actions(env_params))\n",
        "            info = {}\n",
        "\n",
        "          next_timestep = self.env.step(self.env_params, timestep, action)\n",
        "          next_obs = next_timestep.observation\n",
        "          reward = next_timestep.reward\n",
        "          done = next_timestep.step_type == 2\n",
        "\n",
        "          info.update({\"discount\": next_timestep.discount})\n",
        "          new_cum_reward = cum_reward + reward * valid_mask\n",
        "          new_valid_mask = valid_mask * (1- done)\n",
        "          carry = [next_obs, next_timestep, rng, new_cum_reward, new_valid_mask]\n",
        "          y = [obs, action, reward, next_obs, done, timestep, info]\n",
        "\n",
        "          return carry, y\n",
        "\n",
        "        carry_out, scan_out = jax.lax.scan(policy_step, [obs, timestep, rng_episode, jnp.array([0.0]), jnp.array([1.0])], (), num_steps)\n",
        "        obs, action, reward, next_obs, done, timestep, info = scan_out\n",
        "        cum_return = carry_out[-2]\n",
        "        info[\"last_timestep\"] = carry_out[1]\n",
        "\n",
        "        return obs, action, reward, next_obs, done, timestep, info, cum_return\n",
        "\n"
      ],
      "metadata": {
        "id": "IgZSLAUASvq3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnsupervisedRolloutWrapper(CustomRolloutWrapper):\n",
        "  def batch_update(self, rng_update, model, obs, action, next_obs, done, info):\n",
        "    if model is None: return {}\n",
        "    return model.update(rng_update, obs, action, next_obs, done, info)"
      ],
      "metadata": {
        "id": "Oppb9pHQXgpG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration"
      ],
      "metadata": {
        "id": "pj1F4BxZi6zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jnp.set_printoptions(precision=2,suppress=True)\n",
        "from jax.scipy.special import digamma, gammaln, kl_div\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "import time\n",
        "import flax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from typing import Sequence, NamedTuple, Any, Dict\n",
        "import distrax\n",
        "import gymnax\n",
        "import functools\n",
        "from gymnax.environments import spaces\n",
        "from gymnax.wrappers import FlattenObservationWrapper, LogWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optax\n",
        "from flax.nnx.helpers import TrainState\n"
      ],
      "metadata": {
        "id": "N51zTDR4uVBz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "QWRVL5GouaZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTrainState(TrainState):\n",
        "    vars: nnx.Variable\n",
        "    others: nnx.State\n",
        "\n",
        "    @property\n",
        "    def need_train(self):\n",
        "        return len(self.params) > 0\n",
        "\n",
        "is_trainable = lambda path, node: ( node.type == nnx.Param and\n",
        "    True in [ 'trainable' in t for t in path] )"
      ],
      "metadata": {
        "id": "_JNx-HGKjCFg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_state_from_model(model, tx=optax.adam(0.02)):\n",
        "  graphdef, trainable_params, vars, others = nnx.split(model, is_trainable, nnx.Variable,...)\n",
        "  return MyTrainState.create(params=trainable_params, tx=tx, vars=vars, others=others, graphdef=graphdef)\n",
        "\n",
        "def train_state_update_model(model,state):\n",
        "    graphdef, trainable_params, vars, others = nnx.split(model,is_trainable, nnx.Variable,...)\n",
        "    return state.replace(vars=vars,others=others)\n",
        "\n",
        "def model_from_train_state(state):\n",
        "    return nnx.merge(state.graphdef, state.params, state.vars,state.others)"
      ],
      "metadata": {
        "id": "qqWkZ-RSuiwE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}"
      ],
      "metadata": {
        "id": "QQ6gCwmLvhLi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "L3evYa485dgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train(config):\n",
        "  config[\"NUM_UPDATES\"] = (config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"]// config[\"NUM_ENVS\"])\n",
        "\n",
        "  rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "  rng_batch = jax.random.split(rng, config[\"NUM_ENVS\"])\n",
        "\n",
        "  manager = UnsupervisedRolloutWrapper(env_or_name=config[\"ENV_NAME\"])\n",
        "  num_actions = manager.env.num_actions(manager.env_params)\n",
        "  obs_dim = manager.env.observation_shape(manager.env_params)\n",
        "\n",
        "  # model\n",
        "  if config[\"MODEL_NAME\"] == \"XlandDeepSACBayesianExplorer\":\n",
        "    model = XlandDeepSACBayesianExplorer(obs_dim,\n",
        "                                         oof_dim=config[\"NUM_OOF\"],\n",
        "                                         num_actions=num_actions,\n",
        "                                         hidden_dim=config[\"NUM_HIDDEN\"],\n",
        "                                         rngs=nnx.Rngs(config[\"SEED\"]),\n",
        "                                         wd=config[\"WD\"],\n",
        "                                         depth=config[\"DEPTH\"])\n",
        "  else:\n",
        "    model = XlandRandomExplorer(num_actions)\n",
        "\n",
        "  @nnx.jit\n",
        "  def _train_step(state:MyTrainState, rng_loss, obs, action,next_obs,done,info):\n",
        "\n",
        "    def loss_fn(graphdef, params, vars, others):\n",
        "      model = nnx.merge(graphdef, params, vars, others)\n",
        "      return model.batch_loss(rng_loss,obs, action,next_obs,done,info).mean()\n",
        "\n",
        "    def opt_step(state:MyTrainState, unused):\n",
        "      grads = jax.grad(loss_fn, 1)(state.graphdef, state.params, state.vars, state.others)\n",
        "      return state.apply_gradients(grads=grads), None\n",
        "\n",
        "    state, _ = jax.lax.scan(opt_step, state, None, config[\"OPT_STEPS\"])\n",
        "    return state\n",
        "\n",
        "  @nnx.jit\n",
        "  def _rollout_and_update_step(runner_state, unused):\n",
        "    train_state, rng_batch, last_timestep= runner_state\n",
        "\n",
        "    model = model_from_train_state(train_state)\n",
        "    rng_batch, rng_step, rng_update, rng_loss = batch_random_split(rng_batch, 4)\n",
        "\n",
        "    rollout_results = manager.batch_rollout(rng_batch, model, timestep=last_timestep, num_steps=config[\"NUM_STEPS\"])\n",
        "    obs, action, reward, next_obs, done, timestep, info, cum_return = rollout_results\n",
        "\n",
        "    transition = Transition(obs, action, reward, next_obs, done, info)\n",
        "    last_timestep = info[\"last_timestep\"]\n",
        "\n",
        "    update_info = manager.batch_update(rng_update, model, obs, action, next_obs, done, info)\n",
        "    info.update(update_info)\n",
        "    train_state = train_state_update_model(model, train_state)\n",
        "\n",
        "    if train_state.need_train:\n",
        "      train_state = _train_step(train_state, rng_loss, obs, action, next_obs, done, info)\n",
        "\n",
        "    runner_state = (train_state, rng_batch, last_timestep)\n",
        "    return runner_state, (transition, timestep)\n",
        "\n",
        "  def train(rng_batch, model, manager):\n",
        "\n",
        "    rng_batch, rng_reset = batch_random_split(rng_batch, 2)\n",
        "    start_timestep = manager.batch_reset(rng_reset)\n",
        "\n",
        "    if config[\"TX\"] == \"adamw\":\n",
        "      tx = optax.adamw(config[\"LR\"])\n",
        "    elif config[\"TX\"] == \"sgd\":\n",
        "      tx = optax.sgd(config[\"LR\"])\n",
        "    else:\n",
        "      tx = None\n",
        "      assert False, config[\"TX\"] + \"is not available\"\n",
        "    train_state = train_state_from_model(model, tx)\n",
        "    runner_state = (train_state, rng_batch, start_timestep)\n",
        "    runner_state, output = jax.lax.scan(_rollout_and_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
        "\n",
        "    transitions, timesteps = output\n",
        "    return {\"runner_state\": runner_state, \"transitions\": transitions, \"timesteps\": timesteps}\n",
        "\n",
        "  return train, model, manager, rng_batch\n"
      ],
      "metadata": {
        "id": "KadyzIyZ5f3J"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(config):\n",
        "  print(config)\n",
        "  train_fn, model, manager, rng_batch = make_train(config)\n",
        "  train_jit = nnx.jit(train_fn)\n",
        "\n",
        "  out = jax.block_until_ready(train_fn(rng_batch, model, manager))\n",
        "  print(\"data shape:\", jax.tree_util.tree_map(lambda x: x.shape, out[\"transitions\"]))\n",
        "\n",
        "  train_state, rng_batch, last_timestep = out[\"runner_state\"]\n",
        "\n",
        "  model = model_from_train_state(train_state)\n",
        "\n",
        "  if \"mi\" in out[\"transitions\"].info:\n",
        "    # Create figure and axis\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Sample JAX NumPy arrays (replace these with your actual arrays)\n",
        "        #  print (out[\"transitions\"].info)\n",
        "        eig_array = out[\"transitions\"].info[\"mi\"].reshape(-1)\n",
        "        big_array = out[\"transitions\"].info[\"kl\"].reshape(-1)\n",
        "        # Plot both arrays\n",
        "        plt.plot(eig_array, label='EIG', marker='o', linestyle='-', color='blue')\n",
        "        plt.plot(big_array, label='BIG', marker='s', linestyle='-', color='red')\n",
        "\n",
        "        if \"smi\" in out[\"transitions\"].info:\n",
        "          smi_array = out[\"transitions\"].info[\"smi\"].reshape(-1)\n",
        "          plt.plot(smi_array, label='SMI', marker='^', linestyle='-', color='green')\n",
        "\n",
        "        # add labels and title\n",
        "        plt.xlabel('Num of Updates')\n",
        "        plt.ylabel('Information Gain')\n",
        "        Title = \"InfoGains for\" + config[\"MODEL_NAME\"]\n",
        "        Title = Title + \"Total InfoGains\" + \"{:10.4f}\".format(big_array.sum().item())\n",
        "        Title = Title +  \" with Seed\" +str(config[\"SEED\"])\n",
        "        plt.title(Title)\n",
        "\n",
        "        # add grid and legend\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "  if \"l_prec\" in out[\"transitions\"].info:\n",
        "      l_prec_mean = out[\"transitions\"].info[\"l_prec\"].mean(axis=(1,2,3), keepdims=False)\n",
        "      mean_error = out[\"transitions\"].info[\"mean_error\"].mean(axis=(1,2), keepdims=False)\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.plot(l_prec_mean, label='L_prec', marker='o', linestyle='-', color='blue')\n",
        "      plt.plot(mean_error, label='Mean Error', marker='s', linestyle='-', color='yellow')\n",
        "\n",
        "      plt.xlabel('Num of Updates')\n",
        "      plt.ylabel('Mean Precision')\n",
        "      Title = \"Comparison of Mean Precisions\"\n",
        "\n",
        "      plt.title(Title)\n",
        "\n",
        "      plt.grid(alpha=0.3)\n",
        "      plt.legend()\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "bdrRSnBzwQfd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"MiniGrid-EmptyRandom-8x8\"\n",
        "NUM_ENVS = 1 # @param[1,2,4,8,16,32]\n",
        "TOTAL_TIMESTEPS = 16384 # @param [2048,16384,131072,1048576] {\"type\":\"raw\"}\n",
        "DEPTH = 1 # @param [1,2,4] {\"type\":\"raw\"}\n",
        "NUM_STEPS = 8 # @param [1,2,4,8,16] {\"type\":\"raw\"}\n",
        "NUM_HIDDEN = 128 # @param [32,64,128,256] {\"type\":\"raw\"}\n",
        "WD = 0.1 # @param [0,0.1,0.01,0.001] {\"type\":\"raw\"}\n",
        "MODEL_NAME = \"XlandDeepSACBayesianExplorer\"  #@param [\"DeepSACBayesianExplorer\",\"RandomExplorer\",\"XlandDeepSACBayesianExplorer\"]\n",
        "config = {\n",
        "    \"NUM_ENVS\": NUM_ENVS,    #\n",
        "    \"WD\": WD,\n",
        "    \"NUM_STEPS\": NUM_STEPS,   #steps of roll out between update\n",
        "    \"NUM_OOF\": NUM_HIDDEN, # num hidden for now\n",
        "    \"SAC_D_STEPS\": 4,\n",
        "    \"ENV_NAME\":env_name,\n",
        "    \"SAC_STEP_SIZE\": 1.0,\n",
        "    \"SEED\": 423,         #highly stochastic\n",
        "    \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,   #total steps for all envs\n",
        "    \"NUM_HIDDEN\":NUM_HIDDEN,\n",
        "    \"TX\":\"adamw\",\n",
        "    \"DEPTH\":DEPTH,\n",
        "    \"LR\":2e-4,\n",
        "    \"OPT_STEPS\":8,\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"DEBUG\": False,\n",
        "}"
      ],
      "metadata": {
        "id": "Ae4851mIw5tt"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = experiment(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D58-pcyQxuA2",
        "outputId": "5033f19a-4405-460b-fb46-f4df0e748c6e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NUM_ENVS': 1, 'WD': 0.1, 'NUM_STEPS': 8, 'NUM_OOF': 128, 'SAC_D_STEPS': 4, 'ENV_NAME': 'MiniGrid-EmptyRandom-8x8', 'SAC_STEP_SIZE': 1.0, 'SEED': 423, 'TOTAL_TIMESTEPS': 16384, 'NUM_HIDDEN': 128, 'TX': 'adamw', 'DEPTH': 1, 'LR': 0.0002, 'OPT_STEPS': 8, 'MODEL_NAME': 'XlandDeepSACBayesianExplorer', 'DEBUG': False, 'NUM_UPDATES': 2048}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Linear.__init__() takes 3 positional arguments but 4 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-41-692769128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-28-1715616816.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-23-458204080.py\u001b[0m in \u001b[0;36mmake_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MODEL_NAME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"XlandDeepSACBayesianExplorer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     model = XlandDeepSACBayesianExplorer(obs_dim,\n\u001b[0m\u001b[1;32m     14\u001b[0m                                          \u001b[0moof_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NUM_OOF\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                          \u001b[0mnum_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_node_meta_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_object__state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m_object_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-40-1235044071.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obs_dim_raw_shape, oof_dim, num_actions, hidden_dim, rngs, l_prec, wd, ent_lambda, depth, num_title_types, num_colors)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_obs_feature_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_title_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_colors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# what is trainable here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_likelihood_prec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLikelihood_Prec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_obs_feature_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActionEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_node_meta_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_object__state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m_object_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-11-4058657834.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obs_dim, hidden_dim, rngs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_node_meta_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_object__state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/object.py\u001b[0m in \u001b[0;36m_object_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_object_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Linear.__init__() takes 3 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IQL"
      ],
      "metadata": {
        "id": "yQSQh8M2lsdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import distrax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n"
      ],
      "metadata": {
        "id": "WNXxwUJSmMkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "0NstW8rlmhcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"IQL\"\n",
        "    project: str = \"train-IQL\"\n",
        "    env_name: str = \"MiniGrid-EmptyRandom-6x6\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = False\n",
        "    normalize_reward: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Tuple[int, int] = (256, 256)\n",
        "    actor_lr: float = 3e-4\n",
        "    value_lr: float = 3e-4\n",
        "    critic_lr: float = 3e-4\n",
        "    layer_norm: bool = True\n",
        "    opt_decay_schedule: bool = True\n",
        "    # IQL SPECIFIC\n",
        "    expectile: float = (\n",
        "        0.7  # FYI: for Hopper-me, 0.5 produce better result. (antmaze: expectile=0.9)\n",
        "    )\n",
        "    beta: float = (\n",
        "        3.0  # FYI: for Hopper-me, 6.0 produce better result. (antmaze: beta=10.0)\n",
        "    )\n",
        "    tau: float = 0.005\n",
        "    discount: float = 0.99\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "\n",
        "conf_dict = OmegaConf.from_cli()\n",
        "config = IQLConfig(**conf_dict)"
      ],
      "metadata": {
        "id": "WcEIlxtMmkI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Networks"
      ],
      "metadata": {
        "id": "FW1HYQS-m0BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray, actions: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        actions = jax.nn.one_hot(actions, num_classes=4) #one-hot encoding\n",
        "        flat_observations = observations.reshape(batch_size, -1)\n",
        "        inputs = jnp.concatenate([flat_observations, actions], axis=-1)\n",
        "        critic = MLP((*self.hidden_dims, 1), activations=self.activations)(inputs)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    split_rngs = kwargs.pop(\"split_rngs\", {})\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={\"params\": 0},\n",
        "        split_rngs={**split_rngs, \"params\": True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "class ValueCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        obs_flat = observations.reshape(batch_size, -1)\n",
        "        critic = MLP((*self.hidden_dims, 1), layer_norm=self.layer_norm)(obs_flat)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    log_std_min: Optional[float] = -5.0\n",
        "    log_std_max: Optional[float] = 2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observations: jnp.ndarray, temperature: float = 1.0\n",
        "    ) -> distrax.Distribution:\n",
        "        outputs = MLP(\n",
        "            self.hidden_dims,\n",
        "            activate_final=True,\n",
        "        )(observations)\n",
        "\n",
        "        means = nn.Dense(\n",
        "            self.action_dim, kernel_init=default_init()\n",
        "        )(outputs)\n",
        "        log_stds = self.param(\"log_stds\", nn.initializers.zeros, (self.action_dim,))\n",
        "        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        distribution = distrax.MultivariateNormalDiag(\n",
        "            loc=means, scale_diag=jnp.exp(log_stds) * temperature\n",
        "        )\n",
        "        return distribution\n",
        "\n",
        "class CatPolicy(nn.Module):\n",
        "  hidden_dims : Sequence[int]\n",
        "  action_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, observations: jnp.ndarray, temperature: float = 1.0) -> distrax.Distribution:\n",
        "    x = observations.reshape(observations.shape[0], -1) # flatten\n",
        "    outputs = MLP(self.hidden_dims, activate_final=True)(x)\n",
        "    logits = nn.Dense(self.action_dim, kernel_init=default_init())(outputs)\n",
        "    distribution = distrax.Categorical(logits=logits)\n",
        "    return distribution\n"
      ],
      "metadata": {
        "id": "9nqPOs5Umxhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "QvkJdJ9EnpNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jtu.tree_map(jnp.shape, replay_buffer))\n",
        "print(type(replay_buffer))\n",
        "print(replay_buffer[\"dones\"])"
      ],
      "metadata": {
        "id": "dtarzRCfprRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "    dones_float: jnp.ndarray"
      ],
      "metadata": {
        "id": "tJxWv8DinrFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalization(dataset: Transition) -> float:\n",
        "    # into numpy.ndarray\n",
        "    dataset = jax.tree_util.tree_map(lambda x: np.array(x), dataset)\n",
        "    returns = []\n",
        "    ret = 0\n",
        "    for r, term in zip(dataset.rewards, dataset.dones_float):\n",
        "        ret += r\n",
        "        if term:\n",
        "            returns.append(ret)\n",
        "            ret = 0\n",
        "    return (max(returns) - min(returns)) / 1000"
      ],
      "metadata": {
        "id": "ga3QaSnJntei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(\n",
        "     dataset: dict, config: IQLConfig, clip_to_eps: bool = True, eps: float = 1e-5\n",
        ") -> Transition:\n",
        "\n",
        "    if clip_to_eps:\n",
        "        lim = 1 - eps\n",
        "        dataset[\"actions\"] = jnp.clip(dataset[\"actions\"], -lim, lim)\n",
        "\n",
        "    # dones_float = np.zeros_like(dataset['dones'])\n",
        "\n",
        "    # # for i in range(len(dones_float) - 1):\n",
        "    # #     print(i)\n",
        "    # #     if np.linalg.norm(dataset['observations'][i + 1] -\n",
        "    # #                         dataset['next_observations'][i]\n",
        "    # #                         ) > 1e-6 or dataset['dones'][i] == True:\n",
        "    # #         dones_float[i] = 1\n",
        "    # #     else:\n",
        "    # #         dones_float[i] = 0\n",
        "    # dones_float[-1] = 1\n",
        "\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    next_obs = dataset['next_observations']  # shape: (N, 7, 7, 2)\n",
        "    dones = dataset['dones']              # shape: (N,)\n",
        "\n",
        "    # 展平每个 observation\n",
        "    obs_flat = obs[1:].reshape((obs.shape[0] - 1, -1))           # shape: (N-1, 98)\n",
        "    next_obs_flat = next_obs[:-1].reshape((next_obs.shape[0] - 1, -1))  # shape: (N-1, 98)\n",
        "\n",
        "    # 对每个样本求 L2 范数\n",
        "    obs_diff = jnp.linalg.norm(obs_flat - next_obs_flat, axis=1)   # shape: (N-1,)\n",
        "    obs_flag = obs_diff > 1e-6\n",
        "    done_flag = dones[:-1] == True\n",
        "\n",
        "    dones_float = jnp.zeros_like(dones, dtype=jnp.float32)\n",
        "    dones_float = dones_float.at[:-1].set(jnp.logical_or(obs_flag, done_flag).astype(jnp.float32))\n",
        "    dones_float = dones_float.at[-1].set(1.0)\n",
        "\n",
        "    dataset = Transition(\n",
        "        observations=jnp.array(dataset[\"observations\"], dtype=jnp.float32),\n",
        "        actions=jnp.array(dataset[\"actions\"], dtype=jnp.float32),\n",
        "        rewards=jnp.array(dataset[\"rewards\"], dtype=jnp.float32),\n",
        "        next_observations=jnp.array(dataset[\"next_observations\"], dtype=jnp.float32),\n",
        "        dones=jnp.array(dataset[\"dones\"], dtype=jnp.float32),\n",
        "        dones_float=jnp.array(dones_float, dtype=jnp.float32),\n",
        "    )\n",
        "\n",
        "    # normalize states\n",
        "    # obs_mean, obs_std = 0, 1\n",
        "    # if config.normalize_state:\n",
        "    #     obs_mean = dataset.observations.mean(0)\n",
        "    #     obs_std = dataset.observations.std(0)\n",
        "    #     dataset = dataset._replace(\n",
        "    #         observations=(dataset.observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #         next_observations=(dataset.next_observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #     )\n",
        "    # # normalize rewards\n",
        "    # if config.normalize_reward:\n",
        "    #     normalizing_factor = get_normalization(dataset)\n",
        "    #     dataset = dataset._replace(rewards=dataset.rewards / normalizing_factor)\n",
        "\n",
        "    # shuffle data and select the first data_size samples\n",
        "    # data_size = min(config.data_size, len(dataset.observations))\n",
        "    # rng = jax.random.PRNGKey(config.seed)\n",
        "    # rng, rng_permute, rng_select = jax.random.split(rng, 3)\n",
        "    # perm = jax.random.permutation(rng_permute, len(dataset.observations))\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[perm], dataset)\n",
        "    # assert len(dataset.observations) >= data_size\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[:data_size], dataset)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Q4-51Dpin7az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectile_loss(diff, expectile=0.8) -> jnp.ndarray:\n",
        "    weight = jnp.where(diff > 0, expectile, (1 - expectile))\n",
        "    return weight * (diff**2)\n",
        "\n",
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "qlDMKO3toAvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "3jfs8WxhoP3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLTrainState(NamedTuple):\n",
        "    rng: jax.random.PRNGKey\n",
        "    critic: TrainState\n",
        "    target_critic: TrainState\n",
        "    value: TrainState\n",
        "    actor: TrainState\n",
        "\n",
        "class IQL(object):\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        next_v = train_state.value.apply_fn(\n",
        "            train_state.value.params, batch.next_observations\n",
        "        )\n",
        "        target_q = batch.rewards + config.discount * (1 - batch.dones) * next_v\n",
        "\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q1, q2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            critic_loss = ((q1 - target_q) ** 2 + (q2 - target_q) ** 2).mean()\n",
        "            return critic_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_value(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        q1, q2 = train_state.target_critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jax.lax.stop_gradient(jnp.minimum(q1, q2))\n",
        "        def value_loss_fn(value_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            v = train_state.value.apply_fn(value_params, batch.observations)\n",
        "            value_loss = expectile_loss(q - v, config.expectile).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_value, value_loss = update_by_loss_grad(train_state.value, value_loss_fn)\n",
        "        return train_state._replace(value=new_value), value_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        v = train_state.value.apply_fn(train_state.value.params, batch.observations)\n",
        "        q1, q2 = train_state.critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jnp.minimum(q1, q2)\n",
        "        exp_a = jnp.exp((q - v) * config.beta)\n",
        "        exp_a = jnp.minimum(exp_a, 100.0)\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            dist = train_state.actor.apply_fn(actor_params, batch.observations)\n",
        "            log_probs = dist.log_prob(batch.actions.astype(jnp.int32))\n",
        "            actor_loss = -(exp_a * log_probs).mean()\n",
        "            return actor_loss\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        dataset: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: IQLConfig,\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        for _ in range(config.n_jitted_updates):\n",
        "            rng, subkey = jax.random.split(rng)\n",
        "            batch_indices = jax.random.randint(\n",
        "                subkey, (config.batch_size,), 0, len(dataset.observations)\n",
        "            )\n",
        "            batch = jax.tree_util.tree_map(lambda x: x[batch_indices], dataset)\n",
        "\n",
        "            train_state, value_loss = self.update_value(train_state, batch, config)\n",
        "            train_state, actor_loss = self.update_actor(train_state, batch, config)\n",
        "            train_state, critic_loss = self.update_critic(train_state, batch, config)\n",
        "            new_target_critic = target_update(\n",
        "                train_state.critic, train_state.target_critic, config.tau\n",
        "            )\n",
        "            train_state = train_state._replace(target_critic=new_target_critic)\n",
        "        return train_state, {\n",
        "            \"value_loss\": value_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "            \"critic_loss\": critic_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        observations: np.ndarray,\n",
        "        seed: jax.random.PRNGKey,\n",
        "        temperature: float = 1.0,\n",
        "        max_action: float = 1.0,\n",
        "    ) -> jnp.ndarray:\n",
        "\n",
        "        # modified for discrete actions\n",
        "        dist = train_state.actor.apply_fn(\n",
        "            train_state.actor.params, observations, temperature=temperature\n",
        "        )\n",
        "        actions = jnp.argmax(dist.logits, axis=-1)\n",
        "        return actions"
      ],
      "metadata": {
        "id": "S3ZiQLAwoV_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Evaluate"
      ],
      "metadata": {
        "id": "pJXpO90HoiJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_iql_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: IQLConfig,\n",
        ") -> IQLTrainState:\n",
        "    rng, actor_rng, critic_rng, value_rng = jax.random.split(rng, 4)\n",
        "    # initialize actor\n",
        "    action_dim = 4\n",
        "\n",
        "    # Gaussian Model\n",
        "    # actor_model = GaussianPolicy(\n",
        "    #     config.hidden_dims,\n",
        "    #     action_dim=action_dim,\n",
        "    #     log_std_min=-5.0,\n",
        "    # )\n",
        "\n",
        "    # Cat Model\n",
        "    actor_model = CatPolicy(\n",
        "        config.hidden_dims,\n",
        "        action_dim = action_dim\n",
        "    )\n",
        "\n",
        "    if config.opt_decay_schedule:\n",
        "        schedule_fn = optax.cosine_decay_schedule(-config.actor_lr, config.max_steps)\n",
        "        actor_tx = optax.chain(optax.scale_by_adam(), optax.scale_by_schedule(schedule_fn))\n",
        "    else:\n",
        "        actor_tx = optax.adam(learning_rate=config.actor_lr)\n",
        "    actor = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=actor_tx,\n",
        "    )\n",
        "    # initialize critic\n",
        "    critic_model = ensemblize(Critic, num_qs=2)(config.hidden_dims)\n",
        "    critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    target_critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    # initialize value\n",
        "    value_model = ValueCritic(config.hidden_dims, layer_norm=config.layer_norm)\n",
        "    value = TrainState.create(\n",
        "        apply_fn=value_model.apply,\n",
        "        params=value_model.init(value_rng, observations),\n",
        "        tx=optax.adam(learning_rate=config.value_lr),\n",
        "    )\n",
        "    return IQLTrainState(\n",
        "        rng,\n",
        "        critic=critic,\n",
        "        target_critic=target_critic,\n",
        "        value=value,\n",
        "        actor=actor,\n",
        "    )"
      ],
      "metadata": {
        "id": "yZvRadfAokla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn, env, env_params, num_episodes: int, rng\n",
        ") -> float:\n",
        "    print(\"evaluation started\")\n",
        "    episode_returns = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      episode_return = 0\n",
        "\n",
        "      timestep = env.reset(env_params, _rng)\n",
        "      done = timestep.step_type == 2\n",
        "      observation = timestep.observation\n",
        "\n",
        "      while not done:\n",
        "          # potential case issue\n",
        "          obs = observation[None, ...]\n",
        "          action = policy_fn(observations=obs)\n",
        "\n",
        "          if isinstance(action, (jnp.ndarray, np.ndarray)) and action.shape == (1,):\n",
        "            action = int(action[0])\n",
        "\n",
        "          timestep = env.step(env_params, timestep, action)\n",
        "          reward = timestep.reward\n",
        "          done = timestep.step_type == 2\n",
        "          observation = timestep.observation\n",
        "\n",
        "          episode_return += reward\n",
        "      episode_returns.append(episode_return)\n",
        "    return float(jnp.mean(jnp.array(episode_returns)))"
      ],
      "metadata": {
        "id": "9B6VH9JgoptW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(config=config, project=config.project)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "\n",
        "    env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-6x6\")\n",
        "    env = GymAutoResetWrapper(env)\n",
        "\n",
        "    dataset= preprocess_dataset(replay_buffer, config)\n",
        "\n",
        "    # create train_state\n",
        "    example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state: IQLTrainState = create_iql_train_state(\n",
        "        _rng,\n",
        "        example_batch.observations[None, ...],\n",
        "        example_batch.actions[None, ...],\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    algo = IQL()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(train_state, dataset, subkey, config)\n",
        "\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        # if i % eval_interval == 0:\n",
        "        #     policy_fn = partial(\n",
        "        #         act_fn,\n",
        "        #         temperature=0.0,\n",
        "        #         seed=jax.random.PRNGKey(0),\n",
        "        #         train_state=train_state,\n",
        "        #     )\n",
        "        #     normalized_score = evaluate(\n",
        "        #         policy_fn,\n",
        "        #         env,\n",
        "        #         env_params,\n",
        "        #         rng = _rng,\n",
        "        #         num_episodes=config.eval_episodes,\n",
        "        #     )\n",
        "        #     print(i, normalized_score)\n",
        "        #     eval_metrics = {f\"{config.env_name}/normalized_score\": normalized_score}\n",
        "        #     wandb.log(eval_metrics, step=i)\n",
        "    # final evaluation\n",
        "    policy_fn = partial(\n",
        "        act_fn,\n",
        "        temperature=0.0,\n",
        "        seed=jax.random.PRNGKey(0),\n",
        "        train_state=train_state,\n",
        "    )\n",
        "    normalized_score = evaluate(\n",
        "        policy_fn,\n",
        "        env,\n",
        "        env_params,\n",
        "        rng = _rng,\n",
        "        num_episodes=config.eval_episodes,\n",
        "    )\n",
        "    print(\"Final Evaluation\", normalized_score)\n",
        "    wandb.log({f\"{config.env_name}/final_normalized_score\": normalized_score})\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "mQkNH7rvo2mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect Rollouts"
      ],
      "metadata": {
        "id": "hNLDJZfm4Hx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xminigrid.wrappers import GymAutoResetWrapper\n",
        "\n",
        "def build_rollout(env, env_params, num_steps):\n",
        "  def rollout(rng):\n",
        "    def _step_fn(carry, _):\n",
        "      rng, timestep = carry\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      action = jax.random.randint(_rng, shape=(), minval=0, maxval=env.num_actions(env_params))\n",
        "\n",
        "      timestep = env.step(env_params, timestep, action)\n",
        "\n",
        "      return (rng, timestep), (timestep,action)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    timestep = env.reset(env_params, _rng)\n",
        "    rng, (transitions, actions) = jax.lax.scan(_step_fn, (rng, timestep), None, length=num_steps)\n",
        "\n",
        "    return transitions, actions\n",
        "  return rollout"
      ],
      "metadata": {
        "id": "SOVYwUxm70Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-8x8\")\n",
        "env = GymAutoResetWrapper(env)\n",
        "\n",
        "rollout_fn = jax.jit(build_rollout(env, env_params, num_steps=1e6))\n",
        "\n",
        "transitions, actions = rollout_fn(jax.random.key(0))"
      ],
      "metadata": {
        "id": "bAmVT6PtTAnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs_dim = env.observation_shape(env_params)"
      ],
      "metadata": {
        "id": "IIgnrL0od9Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(obs_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iqTFGUoeAM0",
        "outputId": "1cc6ea1d-4808-4844-d557-207a29162c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 7, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transitions shapes: \\n\", jtu.tree_map(jnp.shape, transitions))\n",
        "print(\"Actions shape:\", actions.shape)\n",
        "print(type(actions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp44_CUcTpXV",
        "outputId": "fda8aa41-dbf5-4c14-aa02-5df08aa3ca8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transitions shapes: \n",
            " TimeStep(state=State(key=(1000000,), step_num=(1000000,), grid=(1000000, 8, 8, 2), agent=AgentState(position=(1000000, 2), direction=(1000000,), pocket=(1000000, 2)), goal_encoding=(1000000, 5), rule_encoding=(1000000, 1, 7), carry=EnvCarry()), step_type=(1000000,), reward=(1000000,), discount=(1000000,), observation=(1000000, 7, 7, 2))\n",
            "Actions shape: (1000000,)\n",
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_replay_buffer(transitions, actions):\n",
        "\n",
        "  observations = transitions.observation # (T, 7, 7, 2)\n",
        "  rewards = transitions.reward # (T,)\n",
        "  dones = transitions.step_type == 2 # (T,)\n",
        "  next_observations = jnp.concatenate([observations[1:], observations[-1:]], axis=0) #(T, 7, 7, 2)\n",
        "  actions = jnp.array(actions, dtype=jnp.int32) #(T,)\n",
        "\n",
        "  replay_buffer = {'observations': observations,\n",
        "                   'actions': actions,\n",
        "                   'rewards': rewards,\n",
        "                   'next_observations': next_observations,\n",
        "                   'dones': dones}\n",
        "\n",
        "  print(\"=== Replay Buffer 构建完成 ===\")\n",
        "  print(f\"数据点数量: {len(observations)}\")\n",
        "  print(f\"平均奖励: {jnp.mean(rewards):.4f}\")\n",
        "  print(f\"Episode结束次数: {jnp.sum(dones)}\")\n",
        "  print(f\"动作分布: {jnp.bincount(actions)}\")\n",
        "  return replay_buffer"
      ],
      "metadata": {
        "id": "pcUGKy_aSeK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potential issue with sparse reward"
      ],
      "metadata": {
        "id": "0JzlW7Ht0M9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = create_replay_buffer(transitions, actions)"
      ],
      "metadata": {
        "id": "QygjqK1SzZr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942a3f37-6a2f-46f1-835c-38973dba6dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Replay Buffer 构建完成 ===\n",
            "数据点数量: 1000000\n",
            "平均奖励: 0.0028\n",
            "Episode结束次数: 9572\n",
            "动作分布: [167326 166610 166812 166592 166378 166282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(replay_buffer, batch_size=32, num_batches=None):\n",
        "  data_size = len(replay_buffer['observations'])\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = max(1, data_size // batch_size)\n",
        "\n",
        "  batches = []\n",
        "\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "-1lsLd27zekf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD3BC"
      ],
      "metadata": {
        "id": "UPNRnO8D4DqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "XvngrFj44C2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions"
      ],
      "metadata": {
        "id": "qqcNMUJUuGu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "hU9mkQlN3qNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BCConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"TD3-BC\"\n",
        "    project: str = \"train-TD3-BC\"\n",
        "    env_name: str = \"MiniGrid-Empty-8x8\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100000\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Sequence[int] = (256, 256)\n",
        "    critic_lr: float = 1e-3\n",
        "    actor_lr: float = 1e-3\n",
        "    # TD3-BC SPECIFIC\n",
        "    policy_freq: int = 2  # update actor every policy_freq updates\n",
        "    alpha: float = 2.5  # BC loss weight\n",
        "    policy_noise_std: float = 0.2  # std of policy noise\n",
        "    policy_noise_clip: float = 0.5  # clip policy noise\n",
        "    tau: float = 0.005  # target network update rate\n",
        "    discount: float = 0.99  # discount factor\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "conf_dict = OmegaConf.from_cli() # CLI Input\n",
        "config = TD3BCConfig(**conf_dict)\n",
        "\n",
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    if i + 1 < len(self.hidden_dims):\n",
        "                        x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "class DoubleCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observation: jnp.ndarray, action: jnp.ndarray\n",
        "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "        x = jnp.concatenate([observation, action], axis=-1)\n",
        "        q1 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        q2 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "class TD3Actor(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    max_action: float = 1.0  # In D4RL, action is scaled to [-1, 1]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observation: jnp.ndarray) -> jnp.ndarray:\n",
        "        action = MLP((*self.hidden_dims, self.action_dim))(observation)\n",
        "        action = self.max_action * jnp.tanh(\n",
        "            action\n",
        "        )  # scale to [-max_action, max_action]\n",
        "        return action\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "\n",
        "class TD3BCTrainState(NamedTuple):\n",
        "    actor: TrainState\n",
        "    critic: TrainState\n",
        "    target_actor: TrainState\n",
        "    target_critic: TrainState\n",
        "    max_action: float = 1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "Wyj1sj8G1O0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3BC Object"
      ],
      "metadata": {
        "id": "eJ50iJGFuprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BC(object):\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            predicted_action = train_state.actor.apply_fn(\n",
        "                actor_params, batch.observations\n",
        "            )\n",
        "            critic_params = jax.lax.stop_gradient(train_state.critic.params)\n",
        "            q_value, _ = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, predicted_action\n",
        "            )\n",
        "\n",
        "            mean_abs_q = jax.lax.stop_gradient(jnp.abs(q_value).mean())\n",
        "            loss_lambda = config.alpha / mean_abs_q\n",
        "\n",
        "            bc_loss = jnp.square(predicted_action - batch.actions).mean()\n",
        "            loss_actor = -1.0 * q_value.mean() * loss_lambda + bc_loss\n",
        "            return loss_actor\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q_pred_1, q_pred_2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            target_next_action = train_state.target_actor.apply_fn(\n",
        "                train_state.target_actor.params, batch.next_observations\n",
        "            )\n",
        "            policy_noise = (\n",
        "                config.policy_noise_std\n",
        "                * train_state.max_action\n",
        "                * jax.random.normal(rng, batch.actions.shape)\n",
        "            )\n",
        "            target_next_action = target_next_action + policy_noise.clip(\n",
        "                -config.policy_noise_clip, config.policy_noise_clip\n",
        "            )\n",
        "            target_next_action = target_next_action.clip(\n",
        "                -train_state.max_action, train_state.max_action\n",
        "            )\n",
        "            q_next_1, q_next_2 = train_state.target_critic.apply_fn(\n",
        "                train_state.target_critic.params,\n",
        "                batch.next_observations,\n",
        "                target_next_action,\n",
        "            )\n",
        "            target = batch.rewards[..., None] + config.discount * jnp.minimum(\n",
        "                q_next_1, q_next_2\n",
        "            ) * (1 - batch.dones[..., None])\n",
        "            target = jax.lax.stop_gradient(target)  # stop gradient for target\n",
        "            value_loss_1 = jnp.square(q_pred_1 - target)\n",
        "            value_loss_2 = jnp.square(q_pred_2 - target)\n",
        "            value_loss = (value_loss_1 + value_loss_2).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        data: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", Dict]:\n",
        "        for _ in range(\n",
        "            config.n_jitted_updates\n",
        "        ):  # we can jit for roop for static unroll\n",
        "            rng, batch_rng = jax.random.split(rng, 2)\n",
        "            batch_idx = jax.random.randint(\n",
        "                batch_rng, (config.batch_size,), 0, len(data.observations)\n",
        "            )\n",
        "            batch: Transition = jax.tree_util.tree_map(lambda x: x[batch_idx], data)\n",
        "            rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "            train_state, critic_loss = self.update_critic(\n",
        "                train_state, batch, critic_rng, config\n",
        "            )\n",
        "            if _ % config.policy_freq == 0:\n",
        "                train_state, actor_loss = self.update_actor(\n",
        "                    train_state, batch, actor_rng, config\n",
        "                )\n",
        "                new_target_critic = target_update(\n",
        "                    train_state.critic, train_state.target_critic, config.tau\n",
        "                )\n",
        "                new_target_actor = target_update(\n",
        "                    train_state.actor, train_state.target_actor, config.tau\n",
        "                )\n",
        "                train_state = train_state._replace(\n",
        "                    target_critic=new_target_critic,\n",
        "                    target_actor=new_target_actor,\n",
        "                )\n",
        "        return train_state, {\n",
        "            \"critic_loss\": critic_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        obs: jnp.ndarray,\n",
        "        max_action: float = 1.0,  # In D4RL, action is scaled to [-1, 1]\n",
        "    ) -> jnp.ndarray:\n",
        "        action = train_state.actor.apply_fn(train_state.actor.params, obs)\n",
        "        action = action.clip(-max_action, max_action)\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "aAkZZy3ouKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TrainState"
      ],
      "metadata": {
        "id": "MBarr3TLvEuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_td3bc_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: TD3BCConfig,\n",
        ") -> TD3BCTrainState:\n",
        "    critic_model = DoubleCritic(\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    action_dim = actions.shape[-1]\n",
        "    actor_model = TD3Actor(\n",
        "        action_dim=action_dim,\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "    # initialize critic\n",
        "    critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    target_critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    # initialize actor\n",
        "    actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    target_actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    return TD3BCTrainState(\n",
        "        actor=actor_train_state,\n",
        "        critic=critic_train_state,\n",
        "        target_actor=target_actor_train_state,\n",
        "        target_critic=target_critic_train_state,\n",
        "    )"
      ],
      "metadata": {
        "id": "VllbSO2Bu7oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "b64_CT78vHk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn: Callable[[jnp.ndarray], jnp.ndarray],\n",
        "    env_name: str,\n",
        "    num_episodes: int,\n",
        "    obs_mean,\n",
        "    obs_std,\n",
        "    max_steps_per_episode: int = 100,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    评估策略\n",
        "\n",
        "    Args:\n",
        "        policy_fn: 策略函数\n",
        "        env_name: 环境名称\n",
        "        num_episodes: episode数量\n",
        "        obs_mean: observation均值\n",
        "        obs_std: observation标准差\n",
        "        max_steps_per_episode: 每个episode的最大步数\n",
        "\n",
        "    Returns:\n",
        "        平均episode回报\n",
        "    \"\"\"\n",
        "    # 创建环境\n",
        "    env, env_params = xminigrid.make(env_name)\n",
        "\n",
        "    episode_returns = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        episode_return = 0\n",
        "        timestep = env.reset(env_params, jax.random.PRNGKey(episode))\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            # 处理observation - xminigrid的observation是直接的JAX数组\n",
        "            obs_array = timestep.observation\n",
        "            obs_numpy = np.array(obs_array)\n",
        "\n",
        "            # xminigrid的observation形状是(7, 7, 2)\n",
        "            if obs_numpy.shape == (7, 7, 2):\n",
        "                # 将(7, 7, 2)转换为(7, 7, 3)的RGB图像\n",
        "                object_types = obs_numpy[:, :, 0]\n",
        "                colors = obs_numpy[:, :, 1]\n",
        "\n",
        "                rgb_image = np.zeros((7, 7, 3), dtype=np.uint8)\n",
        "                rgb_image[:, :, 0] = colors\n",
        "                rgb_image[:, :, 1] = object_types\n",
        "                rgb_image[:, :, 2] = 0\n",
        "\n",
        "                # 上采样到22x22\n",
        "                from scipy.ndimage import zoom\n",
        "                try:\n",
        "                    rgb_image = zoom(rgb_image, (22/7, 22/7, 1), order=0)\n",
        "                except ImportError:\n",
        "                    rgb_image = np.repeat(np.repeat(rgb_image, 3, axis=0), 3, axis=1)\n",
        "                    rgb_image = rgb_image[:22, :22, :]\n",
        "\n",
        "                direction = np.array([0.0])\n",
        "            else:\n",
        "                rgb_image = obs_numpy\n",
        "                direction = np.array([0.0])\n",
        "\n",
        "            obs_dict = {\n",
        "                'image': rgb_image,\n",
        "                'direction': direction\n",
        "            }\n",
        "            processed_obs = processor.process_observation(obs_dict)\n",
        "\n",
        "            # 归一化observation\n",
        "            if obs_mean is not None and obs_std is not None:\n",
        "                processed_obs = (processed_obs - obs_mean) / obs_std\n",
        "\n",
        "            # 获取动作\n",
        "            action = policy_fn(obs=processed_obs)\n",
        "\n",
        "            # 执行动作\n",
        "            timestep = env.step(env_params, timestep, action)\n",
        "            episode_return += timestep.reward\n",
        "\n",
        "            if timestep.is_done():\n",
        "                break\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "    return np.mean(episode_returns)\n"
      ],
      "metadata": {
        "id": "ISV0wZG6vBMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # wandb.init(project=config.project, config=config)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    # dataset, obs_mean, obs_std = get_dataset(config)\n",
        "\n",
        "    # create train_state\n",
        "    rng, subkey = jax.random.split(rng)\n",
        "    # example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state = create_td3bc_train_state(\n",
        "        subkey, example_batch.observations, example_batch.actions, config\n",
        "    )\n",
        "    algo = TD3BC()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, update_rng = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(\n",
        "            train_state,\n",
        "            dataset,\n",
        "            update_rng,\n",
        "            config,\n",
        "        )  # update parameters\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            # wandb.log(train_metrics, step=i)\n",
        "\n",
        "        if i % eval_interval == 0:\n",
        "            policy_fn = partial(act_fn, train_state=train_state)\n",
        "            normalized_score = evaluate(\n",
        "                policy_fn,\n",
        "                config.env_name,\n",
        "                num_episodes=config.eval_episodes,\n",
        "                obs_mean=obs_mean,\n",
        "                obs_std=obs_std,\n",
        "            )\n",
        "            print(i, normalized_score)\n",
        "            eval_metrics = {f\"{config.env_name}/episode_return\": normalized_score}\n",
        "            # wandb.log(eval_metrics, step=i)\n",
        "\n",
        "    # # final evaluation\n",
        "    # policy_fn = partial(act_fn, train_state=train_state)\n",
        "    # normalized_score = evaluate(\n",
        "    #     policy_fn,\n",
        "    #     config.env_name,\n",
        "    #     num_episodes=config.eval_episodes,\n",
        "    #     obs_mean=obs_mean,\n",
        "    #     obs_std=obs_std,\n",
        "    # )\n",
        "    # print(\"Final Evaluation Score:\", normalized_score)\n",
        "    # wandb.log({f\"{config.env_name}/final_episode_return\": normalized_score})\n",
        "    # wandb.finish()"
      ],
      "metadata": {
        "id": "5eHE93XFvDk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5ea458-9592-42f1-dedc-dd0a82c43e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_td3bc_train_state' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-66-3683242256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train_state = create_td3bc_train_state(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0msubkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_td3bc_train_state' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j06BRkx7luFa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}