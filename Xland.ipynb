{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UFZ3e361Y6hb",
        "-qfKrd1lkNHM",
        "MBDZtOMH5FSQ",
        "JsW95dg75Jij",
        "Z6TMiJUFZ_uY",
        "yQSQh8M2lsdJ",
        "hNLDJZfm4Hx7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJjXifWFe9K"
      },
      "outputs": [],
      "source": [
        "%pip install jax\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install xminigrid\n",
        "%pip install gymnax\n",
        "%pip install distrax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as jtu\n",
        "import numpy as np\n",
        "import distrax\n",
        "\n",
        "import timeit\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "from flax import nnx\n",
        "import xminigrid"
      ],
      "metadata": {
        "id": "0gdi3TeiF481"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TimeStep(struct.PyTreeNode):\n",
        "#     # hidden environment state, such as grid, agent, goal, etc\n",
        "#     state: State\n",
        "\n",
        "#     # similar to the dm_env enterface\n",
        "#     step_type: StepType\n",
        "#     reward: jax.Array\n",
        "#     discount: jax.Array\n",
        "#     observation: jax.Array"
      ],
      "metadata": {
        "id": "4L7duDLiJ9Wa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "oSjqwPwiXxCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrapper"
      ],
      "metadata": {
        "id": "JYB7Sa8iEGod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnax.environments.environment import Environment\n",
        "import abc\n",
        "from typing import Any, Generic, Optional, TypeVar\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "import xminigrid\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState:\n",
        "    time: int\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams:\n",
        "    max_steps_in_episode: int = 1\n",
        "\n",
        "class XMiniGridGymnaxWrapper(Environment):\n",
        "    \"\"\"\n",
        "    将 xminigrid 环境包装为 gymnax 风格环境\n",
        "    \"\"\"\n",
        "    def __init__(self, xminigrid_env):\n",
        "        super().__init__()\n",
        "        self.xminigrid_env = xminigrid_env\n",
        "\n",
        "    @property\n",
        "    def default_params(self):\n",
        "        # return EnvParams()\n",
        "        return self.xminigrid_env.default_params()\n",
        "\n",
        "    def step_env(self, key, state, action, params):\n",
        "        timestep = self.xminigrid_env.step(params, state, action)\n",
        "        obs = timestep.observation\n",
        "        reward = timestep.reward\n",
        "        done = (timestep.step_type == 2)  # StepType.LAST == 2\n",
        "        info = {\"discount\": timestep.discount}\n",
        "        state = timestep\n",
        "        return obs, state, reward, done, info\n",
        "\n",
        "    def reset_env(self, key, params):\n",
        "        timestep = self.xminigrid_env.reset(params, key)\n",
        "        obs = timestep.observation\n",
        "        return obs, timestep\n",
        "\n",
        "    def get_obs(self, state, params=None, key=None):\n",
        "        return state.observation\n",
        "\n",
        "    def is_terminal(self, state, params):\n",
        "        return state.step_type == 2\n",
        "\n",
        "    # useless here\n",
        "    @property\n",
        "    def name(self):\n",
        "      return \"xminigrid\"\n",
        "\n",
        "    @property\n",
        "    # potential issue here\n",
        "    #implementation of xland:\n",
        "      #def num_actions(self, params: EnvParamsT) -> int:\n",
        "      # return int(NUM_ACTIONS)\n",
        "    def num_actions(self):\n",
        "        return 6\n",
        "\n",
        "    def action_space(self, params):\n",
        "        return spaces.Discrete(6)\n",
        "\n",
        "    def observation_shape(self, params):\n",
        "        return self.xminigrid_env.observation_shape(params)\n",
        "\n",
        "    def observation_space(self, params):\n",
        "        shape = self.observation_shape(params)\n",
        "        return spaces.Box(low=0, high=255, shape=shape, dtype=jnp.float32)"
      ],
      "metadata": {
        "id": "lJHHxdaGEJan"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoders"
      ],
      "metadata": {
        "id": "fYSuZTE1Y4NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.linear(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class ActionEncoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.embed = nnx.Embed(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.embed(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class JointEncoder(nnx.Module):\n",
        "  def __init__(self, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear1 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm1 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm2 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm3 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array, rng):\n",
        "    dist_distrax = distrax.MultivariateNormalDiag(loc=x, scale_diag=1e-1*jnp.ones_like(x))\n",
        "    # potential shape issue\n",
        "    x = dist_distrax.sample(seed=rng, sample_shape=(1,))\n",
        "    x = self.layer_norm0(x)\n",
        "    h0 = self.linear1(x)\n",
        "    h = nn.relu(h0)\n",
        "    h = self.layer_norm1(h) + h0\n",
        "    h0 = self.linear2(h)\n",
        "    h = self.layer_norm2(h) + h0\n",
        "    return self.layer_norm3(h)"
      ],
      "metadata": {
        "id": "kZlsjAvVXzLc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor"
      ],
      "metadata": {
        "id": "UFZ3e361Y6hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import lax\n",
        "import distrax\n",
        "\n",
        "class Actor(nnx.Module):\n",
        "  # environment related ???\n",
        "  log_std_min: float = -4\n",
        "  log_std_max: float = 2\n",
        "\n",
        "  def __init__(self, obs_dim, action_dim, hidden_dim, rngs: nnx.Rngs):\n",
        "    self.mean = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "    self.log_std = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    mean = self.mean(x)\n",
        "    log_std = jnp.clip(self.log_std(x), self.log_std_min, self.log_std_max)\n",
        "    return mean, log_std"
      ],
      "metadata": {
        "id": "jODz_HXsY_MY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "-qfKrd1lkNHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### computaion"
      ],
      "metadata": {
        "id": "MBDZtOMH5FSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_info_gain_normal(mean, prec, l_prec, next_obs):\n",
        "  prec = jnp.maximum(prec, 1e-6)\n",
        "  posterior_prec = prec + l_prec\n",
        "  prec_ratio = prec / posterior_prec\n",
        "\n",
        "  posterior_mean = (prec * mean + l_prec * next_obs) / posterior_prec\n",
        "\n",
        "  delta_mean = next_obs - posterior_mean\n",
        "  kl = delta_mean * delta_mean * prec\n",
        "  kl = kl + prec_ratio - jnp.log(prec_ratio) - 1\n",
        "  kl = 0.5 * jnp.sum(kl, axis=-1)\n",
        "  return kl, delta_mean\n",
        "\n",
        "@jax.jit\n",
        "def compute_expected_info_gain_normal(prec, l_prec):\n",
        "  prec = jnp.maximum(prec, 1e-6)\n",
        "  prec_ratio = l_prec / prec\n",
        "  mi_matrix = 0.5 * jnp.sum(jnp.log(1+prec_ratio), axis=-1)\n",
        "  return mi_matrix\n",
        "\n",
        "jnp.set_printoptions(precision=3,suppress=True)\n",
        "from flax.training import train_state\n",
        "from jax.scipy.special import gamma,digamma, gammaln, kl_div\n",
        "\n",
        "def batch_random_split(batch_key,num=2):\n",
        "    split_keys = jax.vmap(jax.random.split,in_axes=(0,None))(batch_key,num)\n",
        "    return [split_keys[:, i]  for i in range(num) ]\n"
      ],
      "metadata": {
        "id": "5HFYlDrokQXw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### shape manipulation"
      ],
      "metadata": {
        "id": "JsW95dg75Jij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reshape(arr):\n",
        "  if arr.dim < 3:\n",
        "    raise ValueError(\"Input array must have at least 3 dimensions (n, b, c, ...).\")\n",
        "\n",
        "  n, b, c, *x_dims = arr.shapes\n",
        "  # Transpose the first two axes (n, b) to (b, n)\n",
        "  # We construct the axes tuple dynamically for flexibility\n",
        "  transpose_axes = (1, 0) + tuple(range(2, arr.ndim))\n",
        "  transposed_arr = jnp.transpose(arr, axes=transpose_axes)\n",
        "  # Reshape into (b, n*c, x0, x1, ...)\n",
        "  new_shape = (b, n * c, *x_dims)\n",
        "  reshaped_arr = jnp.reshape(transposed_arr, new_shape)\n",
        "\n",
        "  return reshaped_arr\n",
        "\n",
        "from typing import List, Any\n",
        "\n",
        "# Define a type alias for PyTree for better readability\n",
        "PyTree = Any\n",
        "from typing import List, Any\n",
        "\n",
        "# Define a type alias for PyTree for better readability\n",
        "PyTree = Any\n",
        "def unpack_pytree_by_first_index(pytree: PyTree) -> List[PyTree]:\n",
        "    \"\"\"\n",
        "    Unpacks a PyTree of JAX arrays along their first dimension (id).\n",
        "\n",
        "    This function assumes that all JAX arrays within the PyTree\n",
        "    have a consistent first dimension (the 'id' dimension) and that\n",
        "    you want to create a separate PyTree for each 'id'.\n",
        "\n",
        "    Args:\n",
        "        pytree: A JAX PyTree where the leaves are JAX arrays\n",
        "                with a leading 'id' dimension.\n",
        "\n",
        "    Returns:\n",
        "        A list of PyTrees, where each PyTree corresponds to a single\n",
        "        'id' from the original PyTree.\n",
        "    \"\"\"\n",
        "    # Get the size of the first dimension from any leaf array\n",
        "    # We assume all arrays have the same first dimension size.\n",
        "    first_leaf = jax.tree_util.tree_leaves(pytree)[0]\n",
        "    num_ids = first_leaf.shape[0]\n",
        "\n",
        "    # Create a list to store the unpacked PyTrees\n",
        "    unpacked_pytrees = []\n",
        "\n",
        "    # Iterate through each ID\n",
        "    for i in range(num_ids):\n",
        "        # Use tree_map to slice each array in the PyTree at the current ID\n",
        "        sliced_pytree = jax.tree_util.tree_map(lambda x: x[i], pytree)\n",
        "        unpacked_pytrees.append(sliced_pytree)\n",
        "\n",
        "    return unpacked_pytrees\n",
        "\n",
        "def unpack_states(pytree):\n",
        "    return unpack_pytree_by_first_index(jax.tree.map(reshape, pytree))"
      ],
      "metadata": {
        "id": "D9NZ-QNk5AKs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Others"
      ],
      "metadata": {
        "id": "Z6TMiJUFZ_uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Likelihood_Prec(nnx.Module):\n",
        "  log_std_min: float = -2\n",
        "  log_std_max: float = 2\n",
        "\n",
        "  def __init__(self, obs_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear = nnx.Linear(hidden_dim, obs_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    log_std = jnp.clip(self.linear(x), self.log_std_min, self.log_std_max)\n",
        "    return jnp.exp(-log_std)"
      ],
      "metadata": {
        "id": "Tqfsq_SyaBKk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computation"
      ],
      "metadata": {
        "id": "mszBnNX2Ue5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from flax import nnx\n",
        "\n",
        "jnp.set_printoptions(precision=2,suppress=True)\n",
        "from flax.training import train_state\n",
        "from jax.scipy.special import gamma,digamma, gammaln, kl_div\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "@nnx.jit\n",
        "def compute_info_gain_dirichlet(alpha,next_obs):\n",
        "    \"\"\"\n",
        "    计算互信息矩阵\n",
        "    alpha: (batch, num_states)\n",
        "    next_obs: (batch)  index\n",
        "    输出形状: (batch)\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = jnp.maximum(alpha, 1e-6)\n",
        "    sum_alpha = jnp.sum(alpha, axis=-1,keepdims=False)\n",
        "\n",
        "    next_obs = jnp.expand_dims(next_obs, -1).astype(\"int\")\n",
        "    post_alpha = jnp.take_along_axis(alpha,next_obs,-1)\n",
        "    post_alpha = post_alpha.squeeze(-1)\n",
        "\n",
        "    probs = post_alpha / sum_alpha\n",
        "    log_probs = jnp.log(probs)\n",
        "\n",
        "    # ( batch)\n",
        "    entropy = - log_probs\n",
        "\n",
        "    # ( batch)\n",
        "    posterior_digamma = digamma(post_alpha+1)\n",
        "\n",
        "    # ( batch)\n",
        "    sum_digamma = digamma(sum_alpha+1)\n",
        "\n",
        "    # ( batch)\n",
        "    negative_posterior_entropy = posterior_digamma - sum_digamma\n",
        "\n",
        "    # (batch)\n",
        "    posterior_kl = entropy + negative_posterior_entropy\n",
        "    return posterior_kl\n",
        "\n",
        "@nnx.jit\n",
        "def compute_mi_dirichlet(alpha):\n",
        "    \"\"\"\n",
        "    计算互信息矩阵\n",
        "    输入形状: (num_actions, num_states)\n",
        "    输出形状: (num_actions)\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = jnp.maximum(alpha, 1e-6)\n",
        "    num_states = alpha.shape[-1]\n",
        "    sum_alpha = jnp.sum(alpha, axis=-1,keepdims=True)\n",
        "\n",
        "    probs = alpha / sum_alpha\n",
        "\n",
        "    log_probs = jnp.log(probs)\n",
        "\n",
        "    # ( num_actions)\n",
        "    entropy = - jnp.sum(probs * log_probs,axis=-1)\n",
        "\n",
        "    # ( num_actions, num_states)\n",
        "    posterior_digamma = digamma(alpha+1)\n",
        "\n",
        "    # ( num_actions)\n",
        "    sum_digamma = digamma(sum_alpha+1).squeeze(-1)\n",
        "\n",
        "    # ( num_actions)\n",
        "    negative_posterior_entropy = (probs * posterior_digamma).sum(axis=-1) - sum_digamma\n",
        "\n",
        "    # (num_actions)\n",
        "    mi_matrix = entropy + negative_posterior_entropy\n",
        "    return mi_matrix\n",
        "\n",
        "@nnx.jit\n",
        "def optimal_action_and_MI_from_alpha(alphas,rng):\n",
        "\n",
        "    # 计算互信息矩阵\n",
        "    # 4x8x8x2\n",
        "    mi_matrix = compute_mi_dirichlet(alphas)  # ( num_actions)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    random_perturb = 1e-4*jax.random.normal(_rng,mi_matrix.shape)\n",
        "    mi_matrix = mi_matrix + random_perturb\n",
        "\n",
        "    # sum over\n",
        "    mi_matrix_sum = mi_matrix.sum(axis=(1,2,3))\n",
        "\n",
        "    optimal_actions = jnp.argmax(mi_matrix_sum, axis=-1)\n",
        "    return optimal_actions, mi_matrix"
      ],
      "metadata": {
        "id": "yNvIDPb1Uhpz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Explorer"
      ],
      "metadata": {
        "id": "ck6iG5qLTJTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnax.experimental import RolloutWrapper\n",
        "# action = self.model_forward(policy_params, obs, rng_net)\n",
        "import functools\n",
        "import gymnax\n",
        "from typing import Union,Optional,Any\n",
        "import abc"
      ],
      "metadata": {
        "id": "XYxtYAbZTLyM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnsupervisedExplorer(nnx.Module):\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def update(self, obs, actions, next_obs, dones, info):\n",
        "    # update variable parameters\n",
        "    return #{'kl':KL} MI= E[KL]\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self, observations, rng):\n",
        "    return #actions, {\"mi\":mi_matrix}\n",
        "\n",
        "class RandomExplorer(UnsupervisedExplorer):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "  def update(self, rng, obs, actions, next_obs, dones, info):\n",
        "    return {}\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "    if observations.ndim == 1:\n",
        "      # possible shape issue here\n",
        "      actions = jax.random.randint(rng, shape=(1,), minval=0, maxval=self.num_actions)\n",
        "      return actions, {}\n",
        "    actions = jax.random.randint(rng, shape=(observations.shape[0],), minval=0, maxval=self.num_actions)\n",
        "    return actions, {}\n",
        "\n",
        "class BayesianExplorer(UnsupervisedExplorer):\n",
        "\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_actions = num_actions\n",
        "        self.num_states = num_states\n",
        "        self.alphas = nnx.Variable(jnp.ones((num_states, num_actions, num_states))/2)\n",
        "\n",
        "    def update(self,obs,action,next_obs,done,info):\n",
        "\n",
        "        prior_alphas = self.alphas[obs, action]\n",
        "        kl=compute_info_gain_dirichlet(prior_alphas,next_obs)\n",
        "        self.alphas.value = self.alphas.value.at[obs, action,next_obs].add(1)\n",
        "        return {\"kl\":kl}\n",
        "        #big =\n",
        "       # return {\"big\":}\n",
        "\n",
        "\n",
        "    def __call__(self,observations,rng):\n",
        "\n",
        "      #  alpha = jnp.take(self.alphas,observations.astype(jnp.int32),axis=0)\n",
        "        alpha = self.alphas[observations.astype(jnp.int32)]\n",
        "        actions, mi_matrix = optimal_action_and_MI_from_alpha(alpha,rng)\n",
        "        MI = mi_matrix[actions]\n",
        "        return actions, {\"mi\":MI}"
      ],
      "metadata": {
        "id": "cN03-aDZUGex"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class obs_embedder(nnx.Module):\n",
        "  def __init__(self, num_type: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "    self.embed = nnx.Embed(num_type, embed_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, obs):\n",
        "    t_ids = obs[..., 0]\n",
        "    c_ids = obs[..., 1]\n",
        "\n",
        "    t_embed = self.embed(t_ids)\n",
        "    c_embed = self.embed(c_ids)\n",
        "\n",
        "    return jnp.concatenate([t_embed, c_embed], axis=-1)\n",
        "\n",
        "class Joint_MLP(nnx.Module):\n",
        "  def __init__(self, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear1 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(hidden_dim, 2*hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    h = self.linear1(x)\n",
        "    h = nn.relu(h)\n",
        "    out = self.linear2(h)\n",
        "    return out"
      ],
      "metadata": {
        "id": "yjCAqwE5W8Qr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepBayesianExplorer(BayesianExplorer):\n",
        "\n",
        "    def __init__(self, num_states, num_actions,num_hidden):\n",
        "        super().__init__(num_states, num_actions)\n",
        "        self.embed_size = num_states + num_actions\n",
        "        self.num_hidden = num_hidden\n",
        "        obs_dim = num_states\n",
        "        self.obs_embeds = obs_embedder(num_states,num_hidden,rngs=nnx.Rngs(0))\n",
        "        self.action_embeds = nnx.Embed(num_embeddings=num_actions, features=8*8*2*num_hidden, rngs=nnx.Rngs(0))\n",
        "        self.joint_embeds = Joint_MLP(8*8*2*num_hidden,rngs=nnx.Rngs(0))\n",
        "    #    self.linear = nnx.Linear(self.embed_size,num_hidden,rngs=nnx.Rngs(0))\n",
        "        self.w = nnx.Variable(jnp.zeros((8*8*4*num_hidden, 8*8*2*num_states)))\n",
        "        self.b = nnx.Variable(jnp.ones((8, 8, 2, num_states)) / 2)\n",
        "\n",
        "    def update(self,obs,action,next_obs,done,info):\n",
        "        #not necessary just here to log the kl if we were classical bayesian\n",
        "        kl = super().update(obs,action,next_obs,done,info)[\"kl\"]\n",
        "        alpha = info[\"alpha\"]\n",
        "        # alpha 8 x 8 x 2 x 13\n",
        "        # next_obs 8 x 8 x 2\n",
        "        deepkl=compute_info_gain_dirichlet(alpha,next_obs)\n",
        "        #batch x  num_hidden\n",
        "        T = info[\"T\"].reshape(-1,self.num_hidden)\n",
        "        ones = jnp.ones_like(T[:,:1])\n",
        "\n",
        "        #batch x (num_hidden+1)\n",
        "        T = jnp.concatenate([T,ones],axis=-1)\n",
        "        #batch x num_states\n",
        "        #batch x\n",
        "        #w 512 x 1664\n",
        "        #so it should acts seperately on each block\n",
        "        y = jax.nn.one_hot(next_obs.astype(jnp.int32),self.num_states)\n",
        "        y = y.reshape(-1,self.num_states)\n",
        "        # jax.debug.print(\"{}\", T_theta)\n",
        "        T_T = jnp.transpose(T)\n",
        "\n",
        "        covariance = T @ T_T\n",
        "        inv_covariance = jnp.linalg.pinv(covariance)\n",
        "        delta =  T_T @ inv_covariance @ y\n",
        "\n",
        "        delta_W = delta[:-1]\n",
        "        delta_b = delta[-1]\n",
        "        self.w.value = self.w.value + delta_W\n",
        "        self.b.value = self.b.value + delta_b\n",
        "        return {\"kl\":kl,\"deepkl\":deepkl}\n",
        "  #  @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def embed_joint(self,action_embed,obs_embed):\n",
        "        join = lambda x,y :  jnp.concatenate([x,y],axis=-1)\n",
        "        vmapped = jax.vmap(join,in_axes=(0,None))\n",
        "        return vmapped(action_embed,obs_embed)\n",
        "\n",
        "    def __call__(self,observations,rng):\n",
        "        #( num_actions x num_hidden )\n",
        "\n",
        "        # 8x8x2\n",
        "        observations = observations.astype(jnp.int32)\n",
        "\n",
        "      #  print (\"observations\",observations.shape)\n",
        "        # observations: 8 x 8 x 2\n",
        "        # obs_embed: 8 x 8 x 4 use nn.Embedding\n",
        "        # flattent to 256\n",
        "\n",
        "        # 8x8x4\n",
        "        obs_embed = self.obs_embeds(observations)\n",
        "        # 256\n",
        "        obs_embed_flat = obs_embed.reshape(-1)\n",
        "\n",
        "        # 4 x 256\n",
        "        action_embed = self.action_embeds(jnp.arange(self.num_actions))\n",
        "\n",
        "        #num_actions x embed_size    4 x 256\n",
        "        embed = action_embed+jnp.expand_dims(obs_embed_flat,0)\n",
        "\n",
        "        # num_actions x num_hidden\n",
        "        # 4 x 512 using mlp\n",
        "        T = self.joint_embeds(embed)\n",
        "\n",
        "        #  num_actions x num_states\n",
        "       # print (\"self.b \",self.b .shape)\n",
        "\n",
        "        # self.b  8 x 8 x 2 x 13\n",
        "        # alpha  4 x  8 x 8 x 2 x 13\n",
        "        # self.w 512 x 1664\n",
        "        # T @ self.w needs reshape\n",
        "\n",
        "        # T @ self.w : 4 x 1664\n",
        "        Tw = (T @ self.w).reshape(4,8,8,2,13)\n",
        "        alpha = Tw + self.b\n",
        "\n",
        "        #the current code will give\n",
        "        # MI the shape of 4 x  8 x 8 x 2, you need to sum over 8 x 8 x 2\n",
        "        #then take the argmax over action\n",
        "        actions, mi_matrix = optimal_action_and_MI_from_alpha(alpha,rng)\n",
        "        # 512\n",
        "        T = T[actions]\n",
        "        # 8 x 8 x 2 x 13\n",
        "        alpha = alpha[actions]\n",
        "        # 1\n",
        "        mi_matrix = mi_matrix[actions]\n",
        "      #  mi_matrix = jnp.take_along_axis(mi_matrix, actions.reshape((-1,1)), axis=1)\n",
        "      #  actions = actions.squeeze()\n",
        "        return actions, {\"mi\":mi_matrix,\"T\":T,\"alpha\":alpha}\n"
      ],
      "metadata": {
        "id": "Qyx2BuVaT_Gi"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# 假设 obs_embedder, JointEncoder, compute_info_gain_dirichlet, optimal_action_and_MI_from_alpha 已经定义\n",
        "\n",
        "def test_deep_bayesian_explorer():\n",
        "    # 假设状态空间为 13（如13种类型），动作空间为 4，隐藏层为 8\n",
        "    num_states = 13\n",
        "    num_actions = 4\n",
        "    num_hidden = 2\n",
        "\n",
        "    # 构造 explorer\n",
        "    explorer = DeepBayesianExplorer(num_states, num_actions, num_hidden)\n",
        "\n",
        "    # 构造一个假的观测 (8, 8, 2)，值在 [0, num_states-1] 之间\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    obs = jax.random.randint(rng, (8, 8, 2), 0, num_states)\n",
        "\n",
        "    # 调用 __call__ 方法\n",
        "    actions, info = explorer(obs, rng)\n",
        "\n",
        "    print(\"actions:\", actions)\n",
        "    print(\"info keys:\", info.keys())\n",
        "    print(\"mi shape:\", info[\"mi\"].shape)\n",
        "    print(\"T shape:\", info[\"T\"].shape)\n",
        "    print(\"alpha shape:\", info[\"alpha\"].shape)\n",
        "\n",
        "# 运行测试\n",
        "test_deep_bayesian_explorer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSjCA6MLY2qU",
        "outputId": "e565cd61-1a20-4603-d439-cb10cf02c8b9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actions: 1\n",
            "info keys: dict_keys(['mi', 'T', 'alpha'])\n",
            "mi shape: (8, 8, 2)\n",
            "T shape: (512,)\n",
            "alpha shape: (8, 8, 2, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class DeepSACBayesianExplorer(UnsupervisedExplorer):\n",
        "  # ent?\n",
        "  def __init__(self, obs_dim, num_actions, hidden_dim, rngs: nnx.Rngs,\n",
        "               l_prec=1.0, wd=1e-2, ent_lambda=1e-3, depth=2):\n",
        "\n",
        "    self.obs_dim = obs_dim\n",
        "    self.num_actions = num_actions\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.prec_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)), name='prec_w')\n",
        "    self.mean_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)), name='mean_w')\n",
        "    # what is trainable here\n",
        "    self.trainable_likelihood_prec = Likelihood_Prec(obs_dim, hidden_dim, rngs)\n",
        "    self.weight_decay = wd\n",
        "    self.obs_embeds = Encoder(obs_dim, hidden_dim, rngs)\n",
        "    self.action_embeds = ActionEncoder(num_actions, hidden_dim, rngs)\n",
        "    self.joint_embeds = JointEncoder(hidden_dim, rngs)\n",
        "    self.depth = depth\n",
        "    self.ent_lambda = ent_lambda\n",
        "\n",
        "  def __call__(self,observations,rng):\n",
        "      return self.recursive_mi(observations,rng,self.depth)\n",
        "\n",
        "  def update(self, rng, obs, action, next_obs, done, info):\n",
        "    mean = info[\"mean\"]\n",
        "    prec = info[\"prec\"]\n",
        "\n",
        "    def _likelihood_loss(rng, T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "      mu = mean\n",
        "      # model var + inherent var\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "      log_prob = dist_distrax.log_prob(next_obs)\n",
        "      return -log_prob, l_prec\n",
        "\n",
        "    # jit here\n",
        "    predictive_loss, l_prec = _likelihood_loss(rng, info[\"T\"], mean, prec, next_obs)\n",
        "    # originally jnp.sum\n",
        "    mean_error = jnp.mean((mean - next_obs)**2)\n",
        "    deepkl, delta_mean = compute_info_gain_normal(mean, prec, l_prec, next_obs)\n",
        "    # batch x num_hidden\n",
        "    T = info[\"T\"].reshape(-1, self.hidden_dim)\n",
        "\n",
        "    # batch x obs_dim\n",
        "    l_prec = l_prec.reshape(-1, self.obs_dim)\n",
        "    delta_mean = delta_mean.reshape(-1, self.obs_dim)\n",
        "\n",
        "    T_T = jnp.transpose(T)\n",
        "    covariance = T @ T_T\n",
        "    inv_covariance = jnp.linalg.pinv(covariance)\n",
        "\n",
        "    T_Map = T_T @ inv_covariance\n",
        "\n",
        "    delta_precW = T_Map @ l_prec\n",
        "    self.prec_w.value = (self.prec_w.value + delta_precW) * (1-self.weight_decay)\n",
        "    delta_meanW = T_Map @ delta_mean\n",
        "    self.mean_w.value = (self.mean_w.value + delta_meanW) * (1-self.weight_decay)\n",
        "\n",
        "    return {\"kl\":deepkl,  \"predictive_loss\": predictive_loss, \"mean_error\":mean_error}\n",
        "\n",
        "  # jitable\n",
        "  def loss(self, rng, obs, action, next_obs, done, info):\n",
        "    def _likelihood_loss(T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "      mu = mean\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "\n",
        "      log_prob = dist_distrax.log_prob(next_obs)\n",
        "      return -log_prob\n",
        "\n",
        "    T, mean, prec = info[\"T\"], info[\"mean\"], info[\"prec\"]\n",
        "    likelihood_loss = _likelihood_loss(T, mean, prec, next_obs)\n",
        "    return likelihood_loss\n",
        "\n",
        "  def batch_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "    vmapped = jax.vmap(self.loss)\n",
        "    return vmapped(rng, obs, actions, next_obs, dones, info)\n",
        "\n",
        "  def recursive_mi(self, observations, rng, depth):\n",
        "    obs_embed = self.obs_embed(observations)\n",
        "    action_embed = self.action_embed(jnp.arange(self.num_actions))\n",
        "    # possible shape issue\n",
        "    embed = action_embed + jnp.expand_dims(obs_embed, axis=0)\n",
        "\n",
        "    # num_actions x embed_size\n",
        "    T = self.joint_embeds(embed, rng)\n",
        "    prec = jnp.maximum(T @ self.prec_w, 1e-3)\n",
        "    # num_actions x obs_dim\n",
        "    mean = T @ self.mean_w\n",
        "    l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "    MI = compute_expected_info_gain_normal(prec, l_prec)\n",
        "\n",
        "    if depth > 0:\n",
        "      vmapped = jax.vmap(self.recursive_mi, in_axes=(0,None,None))\n",
        "      # num_actions x 1\n",
        "      actions, info = vmapped(mean, rng, depth-1)\n",
        "      MI = MI + info[\"mi\"]\n",
        "\n",
        "    actions = jnp.argmax(MI, axis=0)\n",
        "    T = T[actions]\n",
        "    MI = MI[actions]\n",
        "    l_prec = l_prec[actions]\n",
        "    prec = prec[actions]\n",
        "    mean = mean[actions]\n",
        "    return actions, {\"mi\":MI,\"T\":T,\"obs_embed\":obs_embed,\"l_prec\":l_prec,\n",
        "                        \"prec\":prec,\"mean\":mean}\n",
        "\n",
        "def show_variable(model, text):\n",
        "\n",
        "    graphdef, params, vars,others = nnx.split(model, nnx.Param, nnx.Variable,...)\n",
        "\n",
        "    print(text,vars)\n"
      ],
      "metadata": {
        "id": "fqBtGnP6TewM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    alphas = jnp.zeros((16, 4, 16))\n",
        "    # Define rollout manager for pendulum env\n",
        "    manager = CustomRolloutWrapper(env_or_name=\"MiniGrid-EmptyRandom-8x8\", num_env_steps=3)\n",
        "\n",
        "    # Simple single episode rollout for policy\n",
        "    obs, action, reward, next_obs, done, timestep, info, cum_ret = manager.single_rollout(rng,None)\n",
        "\n",
        "    print (\"single action\",action)\n",
        "    print (\"obs\",obs)\n",
        "    print (\"next_obs\",next_obs)\n",
        "    # Multiple rollouts for same network (different rng, e.g. eval)\n",
        "    rng_batch = jax.random.split(rng, 2)\n",
        "    print (\"reset_state\",manager.batch_reset(rng_batch))\n",
        "\n",
        "    obs, action, reward, next_obs, done,timestep, info, cum_ret = manager.batch_rollout(\n",
        "        rng_batch,None\n",
        "    )\n",
        "\n",
        "    print (\"batch action\",action)\n",
        "    print (\"obs\",obs)\n",
        "    print (\"next_obs\",next_obs)\n",
        "    print (\"info\",info)\n",
        "\n",
        "    # next_state = info[\"next_state\"]\n",
        "    print (\"next_state\",next_state)\n",
        "    print (\"next_state.time[:,-1]\",next_state.time[:,-1])\n",
        "    last_state = EnvState(\n",
        "        time=next_state.time[:,-1],  # Becomes shape (2,)\n",
        "        state=next_state.state[:,-1]  # Becomes shape (2,)\n",
        "    )\n",
        "    obs, action, reward, next_obs, done,info, cum_ret = manager.batch_rollout(\n",
        "        rng_batch, None, timestep\n",
        "    )\n",
        "    # Multiple rollouts for different networks + rng (e.g. for ES)\n",
        "    batch_params = jax.tree_map(  # Stack parameters or use different\n",
        "        lambda x: jnp.tile(x, (2, 1)).reshape(2, *x.shape), alphas\n",
        "    )\n",
        "\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "hO2bGKfkO9uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper"
      ],
      "metadata": {
        "id": "g0HO9tmL6n3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xminigrid.environment import Environment\n",
        "from typing import Union,Optional,Any\n",
        "import abc\n",
        "\n",
        "class CustomRolloutWrapper:\n",
        "    \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_or_name: Union[str,Environment] = \"Pendulum-v1\",\n",
        "        num_env_steps: Optional[int] = None,\n",
        "        env_kwargs: Any | None = None,\n",
        "        env_params: Any | None = None,\n",
        "    ):\n",
        "        \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "        # Define the RL environment & network forward function\n",
        "        if env_kwargs is None:\n",
        "            env_kwargs = {}\n",
        "        if env_params is None:\n",
        "            env_params = {}\n",
        "        if isinstance(env_or_name,Environment):\n",
        "            self.env = env_or_name\n",
        "            self.env_params = env_or_name.default_params\n",
        "        else:\n",
        "            self.env, self.env_params = xminigrid.make(env_or_name, **env_kwargs)\n",
        "        self.env_params = self.env_params.replace(**env_params)\n",
        "\n",
        "        if num_env_steps is None:\n",
        "            self.num_env_steps = self.env_params.max_steps\n",
        "        else:\n",
        "            self.num_env_steps = num_env_steps\n",
        "\n",
        "    def batch_reset(self, rng_input):\n",
        "        batch_rest = jax.vmap(self.single_reset_state)\n",
        "        return batch_rest(rng_input)\n",
        "\n",
        "    # state vs. timestep, potential issue here\n",
        "    def single_reset_state(self, rng_input):\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "        timestep = self.env.reset(self.env_params, rng_reset)\n",
        "        return timestep\n",
        "\n",
        "    def batch_rollout(self, rng_eval, model:UnsupervisedExplorer, timestep=None, num_steps=1):\n",
        "        batch_rollout = jax.vmap(self.single_rollout, in_axes=(0,None,None,None))\n",
        "        return batch_rollout(rng_eval, model, timestep, num_steps)\n",
        "\n",
        "    def single_rollout(self, rng_eval, model:UnsupervisedExplorer, timestep=None, num_steps=1):\n",
        "        rng_reset, rng_episode = jax.random.split(rng_eval)\n",
        "\n",
        "        if timestep is None:\n",
        "          timestep = self.env.reset(self.env_params, rng_reset)\n",
        "\n",
        "        obs = timestep.observation\n",
        "\n",
        "        def policy_step(state_input, _):\n",
        "          obs, timestep, rng, cum_reward, valid_mask = state_input\n",
        "          rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "          if model is not None:\n",
        "              temp,info = model( obs, rng_net)\n",
        "              action = self.env.action_space(self.env_params).sample(rng_net)\n",
        "          else:\n",
        "            # not action space?\n",
        "            action = action = jax.random.randint(rng_step, shape=(), minval=0, maxval=self.env.num_actions(env_params))\n",
        "            info = {}\n",
        "\n",
        "          next_timestep = self.env.step(self.env_params, timestep, action)\n",
        "          next_obs = next_timestep.observation\n",
        "          reward = next_timestep.reward\n",
        "          done = next_timestep.step_type == 2\n",
        "\n",
        "          info.update({\"discount\": next_timestep.discount})\n",
        "          new_cum_reward = cum_reward + reward * valid_mask\n",
        "          new_valid_mask = valid_mask * (1- done)\n",
        "          carry = [next_obs, next_timestep, rng, new_cum_reward, new_valid_mask]\n",
        "          y = [obs, action, reward, next_obs, done, timestep, info]\n",
        "\n",
        "          return carry, y\n",
        "\n",
        "        carry_out, scan_out = jax.lax.scan(policy_step, [obs, timestep, rng_episode, jnp.array([0.0]), jnp.array([1.0])], (), num_steps)\n",
        "        obs, action, reward, next_obs, done, timestep, info = scan_out\n",
        "        cum_return = carry_out[-2]\n",
        "        info[\"last_timestep\"] = carry_out[1]\n",
        "\n",
        "        return obs, action, reward, next_obs, done, timestep, info, cum_return\n",
        "\n"
      ],
      "metadata": {
        "id": "IgZSLAUASvq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnsupervisedRolloutWrapper(CustomRolloutWrapper):\n",
        "  def batch_update(self, rng_update, model, obs, action, next_obs, done, info):\n",
        "    if model is None: return {}\n",
        "    return model.update(rng_update, obs, action, next_obs, done, info)"
      ],
      "metadata": {
        "id": "Oppb9pHQXgpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration"
      ],
      "metadata": {
        "id": "pj1F4BxZi6zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jnp.set_printoptions(precision=2,suppress=True)\n",
        "from jax.scipy.special import digamma, gammaln, kl_div\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "import time\n",
        "import flax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from typing import Sequence, NamedTuple, Any, Dict\n",
        "import distrax\n",
        "import gymnax\n",
        "import functools\n",
        "from gymnax.environments import spaces\n",
        "from gymnax.wrappers import FlattenObservationWrapper, LogWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optax\n",
        "from flax.nnx.helpers import TrainState\n"
      ],
      "metadata": {
        "id": "N51zTDR4uVBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "QWRVL5GouaZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTrainState(TrainState):\n",
        "    vars: nnx.Variable\n",
        "    others: nnx.State\n",
        "\n",
        "    @property\n",
        "    def need_train(self):\n",
        "        return len(self.params) > 0\n",
        "\n",
        "is_trainable = lambda path, node: ( node.type == nnx.Param and\n",
        "    True in [ 'trainable' in t for t in path] )"
      ],
      "metadata": {
        "id": "_JNx-HGKjCFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_state_from_model(model, tx=optax.adam(0.02)):\n",
        "  graphdef, trainable_params, vars, others = nnx.split(model, is_trainable, nnx.Variable,...)\n",
        "  return MyTrainState.create(params=trainable_params, tx=tx, vars=vars, others=others, graphdef=graphdef)\n",
        "\n",
        "def train_state_update_model(model,state):\n",
        "    graphdef, trainable_params, vars, others = nnx.split(model,is_trainable, nnx.Variable,...)\n",
        "    return state.replace(vars=vars,others=others)\n",
        "\n",
        "def model_from_train_state(state):\n",
        "    return nnx.merge(state.graphdef, state.params, state.vars,state.others)"
      ],
      "metadata": {
        "id": "qqWkZ-RSuiwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}"
      ],
      "metadata": {
        "id": "QQ6gCwmLvhLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "L3evYa485dgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train(config):\n",
        "  config[\"NUM_UPDATES\"] = (config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"]// config[\"NUM_ENVS\"])\n",
        "\n",
        "  rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "  rng_batch = jax.random.split(rng, config[\"NUM_ENVS\"])\n",
        "\n",
        "  manager = UnsupervisedRolloutWrapper(env_or_name=config[\"ENV_NAME\"])\n",
        "  num_actions = manager.env.num_actions(manager.env_params)\n",
        "  obs_dim = manager.env.observation_shape(manager.env_params)\n",
        "\n",
        "  # model\n",
        "  if config[\"MODEL_NAME\"] == \"XlandDeepSACBayesianExplorer\":\n",
        "    NUM_TITLE_TYPES = 13\n",
        "    NUM_COLORS = 12\n",
        "    NUM_CLASSES = NUM_TITLE_TYPES * NUM_COLORS\n",
        "    model = XlandDeepSACBayesianExplorer(obs_raw_shape=obs_dim,\n",
        "                                         num_actions=num_actions,\n",
        "                                         hidden_dim=config[\"NUM_HIDDEN\"],\n",
        "                                         rngs=nnx.Rngs(config[\"SEED\"]),\n",
        "                                         wd=config[\"WD\"],\n",
        "                                         depth=config[\"DEPTH\"])\n",
        "  else:\n",
        "    model = XlandRandomExplorer(num_actions)\n",
        "\n",
        "  @nnx.jit\n",
        "  def _train_step(state:MyTrainState, rng_loss, obs, action,next_obs,done,info):\n",
        "\n",
        "    def loss_fn(graphdef, params, vars, others):\n",
        "      model = nnx.merge(graphdef, params, vars, others)\n",
        "      return model.batch_loss(rng_loss,obs, action,next_obs,done,info).mean()\n",
        "\n",
        "    def opt_step(state:MyTrainState, unused):\n",
        "      grads = jax.grad(loss_fn, 1)(state.graphdef, state.params, state.vars, state.others)\n",
        "      return state.apply_gradients(grads=grads), None\n",
        "\n",
        "    state, _ = jax.lax.scan(opt_step, state, None, config[\"OPT_STEPS\"])\n",
        "    return state\n",
        "\n",
        "  @nnx.jit\n",
        "  def _rollout_and_update_step(runner_state, unused):\n",
        "    train_state, rng_batch, last_timestep= runner_state\n",
        "\n",
        "    model = model_from_train_state(train_state)\n",
        "    rng_batch, rng_step, rng_update, rng_loss = batch_random_split(rng_batch, 4)\n",
        "\n",
        "    rollout_results = manager.batch_rollout(rng_batch, model, timestep=last_timestep, num_steps=config[\"NUM_STEPS\"])\n",
        "    obs, action, reward, next_obs, done, timestep, info, cum_return = rollout_results\n",
        "\n",
        "    transition = Transition(obs, action, reward, next_obs, done, info)\n",
        "    last_timestep = info[\"last_timestep\"]\n",
        "\n",
        "    update_info = manager.batch_update(rng_update, model, obs, action, next_obs, done, info)\n",
        "    info.update(update_info)\n",
        "    train_state = train_state_update_model(model, train_state)\n",
        "\n",
        "    if train_state.need_train:\n",
        "      train_state = _train_step(train_state, rng_loss, obs, action, next_obs, done, info)\n",
        "\n",
        "    runner_state = (train_state, rng_batch, last_timestep)\n",
        "    return runner_state, (transition, timestep)\n",
        "\n",
        "  def train(rng_batch, model, manager):\n",
        "\n",
        "    rng_batch, rng_reset = batch_random_split(rng_batch, 2)\n",
        "    start_timestep = manager.batch_reset(rng_reset)\n",
        "\n",
        "    if config[\"TX\"] == \"adamw\":\n",
        "      tx = optax.adamw(config[\"LR\"])\n",
        "    elif config[\"TX\"] == \"sgd\":\n",
        "      tx = optax.sgd(config[\"LR\"])\n",
        "    else:\n",
        "      tx = None\n",
        "      assert False, config[\"TX\"] + \"is not available\"\n",
        "    train_state = train_state_from_model(model, tx)\n",
        "    runner_state = (train_state, rng_batch, start_timestep)\n",
        "    runner_state, output = jax.lax.scan(_rollout_and_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
        "\n",
        "    transitions, timesteps = output\n",
        "    return {\"runner_state\": runner_state, \"transitions\": transitions, \"timesteps\": timesteps}\n",
        "\n",
        "  return train, model, manager, rng_batch\n"
      ],
      "metadata": {
        "id": "KadyzIyZ5f3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(config):\n",
        "  print(config)\n",
        "  train_fn, model, manager, rng_batch = make_train(config)\n",
        "  train_jit = nnx.jit(train_fn)\n",
        "\n",
        "  out = jax.block_until_ready(train_fn(rng_batch, model, manager))\n",
        "  print(\"data shape:\", jax.tree_util.tree_map(lambda x: x.shape, out[\"transitions\"]))\n",
        "\n",
        "  train_state, rng_batch, last_timestep = out[\"runner_state\"]\n",
        "\n",
        "  model = model_from_train_state(train_state)\n",
        "\n",
        "  if \"mi\" in out[\"transitions\"].info:\n",
        "    # Create figure and axis\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Sample JAX NumPy arrays (replace these with your actual arrays)\n",
        "        #  print (out[\"transitions\"].info)\n",
        "        eig_array = out[\"transitions\"].info[\"mi\"].reshape(-1)\n",
        "        big_array = out[\"transitions\"].info[\"kl\"].reshape(-1)\n",
        "        # Plot both arrays\n",
        "        plt.plot(eig_array, label='EIG', marker='o', linestyle='-', color='blue')\n",
        "        plt.plot(big_array, label='BIG', marker='s', linestyle='-', color='red')\n",
        "\n",
        "        if \"smi\" in out[\"transitions\"].info:\n",
        "          smi_array = out[\"transitions\"].info[\"smi\"].reshape(-1)\n",
        "          plt.plot(smi_array, label='SMI', marker='^', linestyle='-', color='green')\n",
        "\n",
        "        # add labels and title\n",
        "        plt.xlabel('Num of Updates')\n",
        "        plt.ylabel('Information Gain')\n",
        "        Title = \"InfoGains for\" + config[\"MODEL_NAME\"]\n",
        "        Title = Title + \"Total InfoGains\" + \"{:10.4f}\".format(big_array.sum().item())\n",
        "        Title = Title +  \" with Seed\" +str(config[\"SEED\"])\n",
        "        plt.title(Title)\n",
        "\n",
        "        # add grid and legend\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "  if \"l_prec\" in out[\"transitions\"].info:\n",
        "      l_prec_mean = out[\"transitions\"].info[\"l_prec\"].mean(axis=(1,2,3), keepdims=False)\n",
        "      mean_error = out[\"transitions\"].info[\"mean_error\"].mean(axis=(1,2), keepdims=False)\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.plot(l_prec_mean, label='L_prec', marker='o', linestyle='-', color='blue')\n",
        "      plt.plot(mean_error, label='Mean Error', marker='s', linestyle='-', color='yellow')\n",
        "\n",
        "      plt.xlabel('Num of Updates')\n",
        "      plt.ylabel('Mean Precision')\n",
        "      Title = \"Comparison of Mean Precisions\"\n",
        "\n",
        "      plt.title(Title)\n",
        "\n",
        "      plt.grid(alpha=0.3)\n",
        "      plt.legend()\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "bdrRSnBzwQfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_AQONO5O4Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"MiniGrid-EmptyRandom-8x8\"\n",
        "NUM_ENVS = 1 # @param[1,2,4,8,16,32]\n",
        "TOTAL_TIMESTEPS = 16384 # @param [2048,16384,131072,1048576] {\"type\":\"raw\"}\n",
        "DEPTH = 1 # @param [1,2,4] {\"type\":\"raw\"}\n",
        "NUM_STEPS = 8 # @param [1,2,4,8,16] {\"type\":\"raw\"}\n",
        "NUM_HIDDEN = 128 # @param [32,64,128,256] {\"type\":\"raw\"}\n",
        "WD = 0.1 # @param [0,0.1,0.01,0.001] {\"type\":\"raw\"}\n",
        "MODEL_NAME = \"XlandDeepSACBayesianExplorer\"  #@param [\"DeepSACBayesianExplorer\",\"RandomExplorer\",\"XlandDeepSACBayesianExplorer\"]\n",
        "config = {\n",
        "    \"NUM_ENVS\": NUM_ENVS,    #\n",
        "    \"WD\": WD,\n",
        "    \"NUM_STEPS\": NUM_STEPS,   #steps of roll out between update\n",
        "    \"NUM_OOF\": NUM_HIDDEN, # num hidden for now\n",
        "    \"SAC_D_STEPS\": 4,\n",
        "    \"ENV_NAME\":env_name,\n",
        "    \"SAC_STEP_SIZE\": 1.0,\n",
        "    \"SEED\": 423,         #highly stochastic\n",
        "    \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,   #total steps for all envs\n",
        "    \"NUM_HIDDEN\":NUM_HIDDEN,\n",
        "    \"TX\":\"adamw\",\n",
        "    \"DEPTH\":DEPTH,\n",
        "    \"LR\":2e-4,\n",
        "    \"OPT_STEPS\":8,\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"DEBUG\": False,\n",
        "}"
      ],
      "metadata": {
        "id": "Ae4851mIw5tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = experiment(config)"
      ],
      "metadata": {
        "id": "D58-pcyQxuA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IQL"
      ],
      "metadata": {
        "id": "yQSQh8M2lsdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import distrax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n"
      ],
      "metadata": {
        "id": "WNXxwUJSmMkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "0NstW8rlmhcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"IQL\"\n",
        "    project: str = \"train-IQL\"\n",
        "    env_name: str = \"MiniGrid-EmptyRandom-6x6\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = False\n",
        "    normalize_reward: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Tuple[int, int] = (256, 256)\n",
        "    actor_lr: float = 3e-4\n",
        "    value_lr: float = 3e-4\n",
        "    critic_lr: float = 3e-4\n",
        "    layer_norm: bool = True\n",
        "    opt_decay_schedule: bool = True\n",
        "    # IQL SPECIFIC\n",
        "    expectile: float = (\n",
        "        0.7  # FYI: for Hopper-me, 0.5 produce better result. (antmaze: expectile=0.9)\n",
        "    )\n",
        "    beta: float = (\n",
        "        3.0  # FYI: for Hopper-me, 6.0 produce better result. (antmaze: beta=10.0)\n",
        "    )\n",
        "    tau: float = 0.005\n",
        "    discount: float = 0.99\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "\n",
        "conf_dict = OmegaConf.from_cli()\n",
        "config = IQLConfig(**conf_dict)"
      ],
      "metadata": {
        "id": "WcEIlxtMmkI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Networks"
      ],
      "metadata": {
        "id": "FW1HYQS-m0BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray, actions: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        actions = jax.nn.one_hot(actions, num_classes=4) #one-hot encoding\n",
        "        flat_observations = observations.reshape(batch_size, -1)\n",
        "        inputs = jnp.concatenate([flat_observations, actions], axis=-1)\n",
        "        critic = MLP((*self.hidden_dims, 1), activations=self.activations)(inputs)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    split_rngs = kwargs.pop(\"split_rngs\", {})\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={\"params\": 0},\n",
        "        split_rngs={**split_rngs, \"params\": True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "class ValueCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        obs_flat = observations.reshape(batch_size, -1)\n",
        "        critic = MLP((*self.hidden_dims, 1), layer_norm=self.layer_norm)(obs_flat)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    log_std_min: Optional[float] = -5.0\n",
        "    log_std_max: Optional[float] = 2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observations: jnp.ndarray, temperature: float = 1.0\n",
        "    ) -> distrax.Distribution:\n",
        "        outputs = MLP(\n",
        "            self.hidden_dims,\n",
        "            activate_final=True,\n",
        "        )(observations)\n",
        "\n",
        "        means = nn.Dense(\n",
        "            self.action_dim, kernel_init=default_init()\n",
        "        )(outputs)\n",
        "        log_stds = self.param(\"log_stds\", nn.initializers.zeros, (self.action_dim,))\n",
        "        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        distribution = distrax.MultivariateNormalDiag(\n",
        "            loc=means, scale_diag=jnp.exp(log_stds) * temperature\n",
        "        )\n",
        "        return distribution\n",
        "\n",
        "class CatPolicy(nn.Module):\n",
        "  hidden_dims : Sequence[int]\n",
        "  action_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, observations: jnp.ndarray, temperature: float = 1.0) -> distrax.Distribution:\n",
        "    x = observations.reshape(observations.shape[0], -1) # flatten\n",
        "    outputs = MLP(self.hidden_dims, activate_final=True)(x)\n",
        "    logits = nn.Dense(self.action_dim, kernel_init=default_init())(outputs)\n",
        "    distribution = distrax.Categorical(logits=logits)\n",
        "    return distribution\n"
      ],
      "metadata": {
        "id": "9nqPOs5Umxhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "QvkJdJ9EnpNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jtu.tree_map(jnp.shape, replay_buffer))\n",
        "print(type(replay_buffer))\n",
        "print(replay_buffer[\"dones\"])"
      ],
      "metadata": {
        "id": "dtarzRCfprRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "    dones_float: jnp.ndarray"
      ],
      "metadata": {
        "id": "tJxWv8DinrFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalization(dataset: Transition) -> float:\n",
        "    # into numpy.ndarray\n",
        "    dataset = jax.tree_util.tree_map(lambda x: np.array(x), dataset)\n",
        "    returns = []\n",
        "    ret = 0\n",
        "    for r, term in zip(dataset.rewards, dataset.dones_float):\n",
        "        ret += r\n",
        "        if term:\n",
        "            returns.append(ret)\n",
        "            ret = 0\n",
        "    return (max(returns) - min(returns)) / 1000"
      ],
      "metadata": {
        "id": "ga3QaSnJntei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(\n",
        "     dataset: dict, config: IQLConfig, clip_to_eps: bool = True, eps: float = 1e-5\n",
        ") -> Transition:\n",
        "\n",
        "    if clip_to_eps:\n",
        "        lim = 1 - eps\n",
        "        dataset[\"actions\"] = jnp.clip(dataset[\"actions\"], -lim, lim)\n",
        "\n",
        "    # dones_float = np.zeros_like(dataset['dones'])\n",
        "\n",
        "    # # for i in range(len(dones_float) - 1):\n",
        "    # #     print(i)\n",
        "    # #     if np.linalg.norm(dataset['observations'][i + 1] -\n",
        "    # #                         dataset['next_observations'][i]\n",
        "    # #                         ) > 1e-6 or dataset['dones'][i] == True:\n",
        "    # #         dones_float[i] = 1\n",
        "    # #     else:\n",
        "    # #         dones_float[i] = 0\n",
        "    # dones_float[-1] = 1\n",
        "\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    next_obs = dataset['next_observations']  # shape: (N, 7, 7, 2)\n",
        "    dones = dataset['dones']              # shape: (N,)\n",
        "\n",
        "    # 展平每个 observation\n",
        "    obs_flat = obs[1:].reshape((obs.shape[0] - 1, -1))           # shape: (N-1, 98)\n",
        "    next_obs_flat = next_obs[:-1].reshape((next_obs.shape[0] - 1, -1))  # shape: (N-1, 98)\n",
        "\n",
        "    # 对每个样本求 L2 范数\n",
        "    obs_diff = jnp.linalg.norm(obs_flat - next_obs_flat, axis=1)   # shape: (N-1,)\n",
        "    obs_flag = obs_diff > 1e-6\n",
        "    done_flag = dones[:-1] == True\n",
        "\n",
        "    dones_float = jnp.zeros_like(dones, dtype=jnp.float32)\n",
        "    dones_float = dones_float.at[:-1].set(jnp.logical_or(obs_flag, done_flag).astype(jnp.float32))\n",
        "    dones_float = dones_float.at[-1].set(1.0)\n",
        "\n",
        "    dataset = Transition(\n",
        "        observations=jnp.array(dataset[\"observations\"], dtype=jnp.float32),\n",
        "        actions=jnp.array(dataset[\"actions\"], dtype=jnp.float32),\n",
        "        rewards=jnp.array(dataset[\"rewards\"], dtype=jnp.float32),\n",
        "        next_observations=jnp.array(dataset[\"next_observations\"], dtype=jnp.float32),\n",
        "        dones=jnp.array(dataset[\"dones\"], dtype=jnp.float32),\n",
        "        dones_float=jnp.array(dones_float, dtype=jnp.float32),\n",
        "    )\n",
        "\n",
        "    # normalize states\n",
        "    # obs_mean, obs_std = 0, 1\n",
        "    # if config.normalize_state:\n",
        "    #     obs_mean = dataset.observations.mean(0)\n",
        "    #     obs_std = dataset.observations.std(0)\n",
        "    #     dataset = dataset._replace(\n",
        "    #         observations=(dataset.observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #         next_observations=(dataset.next_observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #     )\n",
        "    # # normalize rewards\n",
        "    # if config.normalize_reward:\n",
        "    #     normalizing_factor = get_normalization(dataset)\n",
        "    #     dataset = dataset._replace(rewards=dataset.rewards / normalizing_factor)\n",
        "\n",
        "    # shuffle data and select the first data_size samples\n",
        "    # data_size = min(config.data_size, len(dataset.observations))\n",
        "    # rng = jax.random.PRNGKey(config.seed)\n",
        "    # rng, rng_permute, rng_select = jax.random.split(rng, 3)\n",
        "    # perm = jax.random.permutation(rng_permute, len(dataset.observations))\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[perm], dataset)\n",
        "    # assert len(dataset.observations) >= data_size\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[:data_size], dataset)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Q4-51Dpin7az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectile_loss(diff, expectile=0.8) -> jnp.ndarray:\n",
        "    weight = jnp.where(diff > 0, expectile, (1 - expectile))\n",
        "    return weight * (diff**2)\n",
        "\n",
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "qlDMKO3toAvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "3jfs8WxhoP3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLTrainState(NamedTuple):\n",
        "    rng: jax.random.PRNGKey\n",
        "    critic: TrainState\n",
        "    target_critic: TrainState\n",
        "    value: TrainState\n",
        "    actor: TrainState\n",
        "\n",
        "class IQL(object):\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        next_v = train_state.value.apply_fn(\n",
        "            train_state.value.params, batch.next_observations\n",
        "        )\n",
        "        target_q = batch.rewards + config.discount * (1 - batch.dones) * next_v\n",
        "\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q1, q2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            critic_loss = ((q1 - target_q) ** 2 + (q2 - target_q) ** 2).mean()\n",
        "            return critic_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_value(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        q1, q2 = train_state.target_critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jax.lax.stop_gradient(jnp.minimum(q1, q2))\n",
        "        def value_loss_fn(value_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            v = train_state.value.apply_fn(value_params, batch.observations)\n",
        "            value_loss = expectile_loss(q - v, config.expectile).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_value, value_loss = update_by_loss_grad(train_state.value, value_loss_fn)\n",
        "        return train_state._replace(value=new_value), value_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        v = train_state.value.apply_fn(train_state.value.params, batch.observations)\n",
        "        q1, q2 = train_state.critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jnp.minimum(q1, q2)\n",
        "        exp_a = jnp.exp((q - v) * config.beta)\n",
        "        exp_a = jnp.minimum(exp_a, 100.0)\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            dist = train_state.actor.apply_fn(actor_params, batch.observations)\n",
        "            log_probs = dist.log_prob(batch.actions.astype(jnp.int32))\n",
        "            actor_loss = -(exp_a * log_probs).mean()\n",
        "            return actor_loss\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        dataset: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: IQLConfig,\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        for _ in range(config.n_jitted_updates):\n",
        "            rng, subkey = jax.random.split(rng)\n",
        "            batch_indices = jax.random.randint(\n",
        "                subkey, (config.batch_size,), 0, len(dataset.observations)\n",
        "            )\n",
        "            batch = jax.tree_util.tree_map(lambda x: x[batch_indices], dataset)\n",
        "\n",
        "            train_state, value_loss = self.update_value(train_state, batch, config)\n",
        "            train_state, actor_loss = self.update_actor(train_state, batch, config)\n",
        "            train_state, critic_loss = self.update_critic(train_state, batch, config)\n",
        "            new_target_critic = target_update(\n",
        "                train_state.critic, train_state.target_critic, config.tau\n",
        "            )\n",
        "            train_state = train_state._replace(target_critic=new_target_critic)\n",
        "        return train_state, {\n",
        "            \"value_loss\": value_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "            \"critic_loss\": critic_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        observations: np.ndarray,\n",
        "        seed: jax.random.PRNGKey,\n",
        "        temperature: float = 1.0,\n",
        "        max_action: float = 1.0,\n",
        "    ) -> jnp.ndarray:\n",
        "\n",
        "        # modified for discrete actions\n",
        "        dist = train_state.actor.apply_fn(\n",
        "            train_state.actor.params, observations, temperature=temperature\n",
        "        )\n",
        "        actions = jnp.argmax(dist.logits, axis=-1)\n",
        "        return actions"
      ],
      "metadata": {
        "id": "S3ZiQLAwoV_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Evaluate"
      ],
      "metadata": {
        "id": "pJXpO90HoiJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_iql_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: IQLConfig,\n",
        ") -> IQLTrainState:\n",
        "    rng, actor_rng, critic_rng, value_rng = jax.random.split(rng, 4)\n",
        "    # initialize actor\n",
        "    action_dim = 4\n",
        "\n",
        "    # Gaussian Model\n",
        "    # actor_model = GaussianPolicy(\n",
        "    #     config.hidden_dims,\n",
        "    #     action_dim=action_dim,\n",
        "    #     log_std_min=-5.0,\n",
        "    # )\n",
        "\n",
        "    # Cat Model\n",
        "    actor_model = CatPolicy(\n",
        "        config.hidden_dims,\n",
        "        action_dim = action_dim\n",
        "    )\n",
        "\n",
        "    if config.opt_decay_schedule:\n",
        "        schedule_fn = optax.cosine_decay_schedule(-config.actor_lr, config.max_steps)\n",
        "        actor_tx = optax.chain(optax.scale_by_adam(), optax.scale_by_schedule(schedule_fn))\n",
        "    else:\n",
        "        actor_tx = optax.adam(learning_rate=config.actor_lr)\n",
        "    actor = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=actor_tx,\n",
        "    )\n",
        "    # initialize critic\n",
        "    critic_model = ensemblize(Critic, num_qs=2)(config.hidden_dims)\n",
        "    critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    target_critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    # initialize value\n",
        "    value_model = ValueCritic(config.hidden_dims, layer_norm=config.layer_norm)\n",
        "    value = TrainState.create(\n",
        "        apply_fn=value_model.apply,\n",
        "        params=value_model.init(value_rng, observations),\n",
        "        tx=optax.adam(learning_rate=config.value_lr),\n",
        "    )\n",
        "    return IQLTrainState(\n",
        "        rng,\n",
        "        critic=critic,\n",
        "        target_critic=target_critic,\n",
        "        value=value,\n",
        "        actor=actor,\n",
        "    )"
      ],
      "metadata": {
        "id": "yZvRadfAokla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn, env, env_params, num_episodes: int, rng\n",
        ") -> float:\n",
        "    print(\"evaluation started\")\n",
        "    episode_returns = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      episode_return = 0\n",
        "\n",
        "      timestep = env.reset(env_params, _rng)\n",
        "      done = timestep.step_type == 2\n",
        "      observation = timestep.observation\n",
        "\n",
        "      while not done:\n",
        "          # potential case issue\n",
        "          obs = observation[None, ...]\n",
        "          action = policy_fn(observations=obs)\n",
        "\n",
        "          if isinstance(action, (jnp.ndarray, np.ndarray)) and action.shape == (1,):\n",
        "            action = int(action[0])\n",
        "\n",
        "          timestep = env.step(env_params, timestep, action)\n",
        "          reward = timestep.reward\n",
        "          done = timestep.step_type == 2\n",
        "          observation = timestep.observation\n",
        "\n",
        "          episode_return += reward\n",
        "      episode_returns.append(episode_return)\n",
        "    return float(jnp.mean(jnp.array(episode_returns)))"
      ],
      "metadata": {
        "id": "9B6VH9JgoptW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(config=config, project=config.project)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "\n",
        "    env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-6x6\")\n",
        "    env = GymAutoResetWrapper(env)\n",
        "\n",
        "    dataset= preprocess_dataset(replay_buffer, config)\n",
        "\n",
        "    # create train_state\n",
        "    example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state: IQLTrainState = create_iql_train_state(\n",
        "        _rng,\n",
        "        example_batch.observations[None, ...],\n",
        "        example_batch.actions[None, ...],\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    algo = IQL()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(train_state, dataset, subkey, config)\n",
        "\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        # if i % eval_interval == 0:\n",
        "        #     policy_fn = partial(\n",
        "        #         act_fn,\n",
        "        #         temperature=0.0,\n",
        "        #         seed=jax.random.PRNGKey(0),\n",
        "        #         train_state=train_state,\n",
        "        #     )\n",
        "        #     normalized_score = evaluate(\n",
        "        #         policy_fn,\n",
        "        #         env,\n",
        "        #         env_params,\n",
        "        #         rng = _rng,\n",
        "        #         num_episodes=config.eval_episodes,\n",
        "        #     )\n",
        "        #     print(i, normalized_score)\n",
        "        #     eval_metrics = {f\"{config.env_name}/normalized_score\": normalized_score}\n",
        "        #     wandb.log(eval_metrics, step=i)\n",
        "    # final evaluation\n",
        "    policy_fn = partial(\n",
        "        act_fn,\n",
        "        temperature=0.0,\n",
        "        seed=jax.random.PRNGKey(0),\n",
        "        train_state=train_state,\n",
        "    )\n",
        "    normalized_score = evaluate(\n",
        "        policy_fn,\n",
        "        env,\n",
        "        env_params,\n",
        "        rng = _rng,\n",
        "        num_episodes=config.eval_episodes,\n",
        "    )\n",
        "    print(\"Final Evaluation\", normalized_score)\n",
        "    wandb.log({f\"{config.env_name}/final_normalized_score\": normalized_score})\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "mQkNH7rvo2mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect Rollouts"
      ],
      "metadata": {
        "id": "hNLDJZfm4Hx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xminigrid.wrappers import GymAutoResetWrapper\n",
        "\n",
        "def build_rollout(env, env_params, num_steps):\n",
        "  def rollout(rng):\n",
        "    def _step_fn(carry, _):\n",
        "      rng, timestep = carry\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      action = jax.random.randint(_rng, shape=(), minval=0, maxval=env.num_actions(env_params))\n",
        "\n",
        "      timestep = env.step(env_params, timestep, action)\n",
        "\n",
        "      return (rng, timestep), (timestep,action)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    timestep = env.reset(env_params, _rng)\n",
        "    rng, (transitions, actions) = jax.lax.scan(_step_fn, (rng, timestep), None, length=num_steps)\n",
        "\n",
        "    return transitions, actions\n",
        "  return rollout"
      ],
      "metadata": {
        "id": "SOVYwUxm70Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-8x8\")\n",
        "env = GymAutoResetWrapper(env)\n",
        "\n",
        "rollout_fn = jax.jit(build_rollout(env, env_params, num_steps=1e6))\n",
        "\n",
        "transitions, actions = rollout_fn(jax.random.key(0))"
      ],
      "metadata": {
        "id": "bAmVT6PtTAnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs_dim = env.observation_shape(env_params)"
      ],
      "metadata": {
        "id": "IIgnrL0od9Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(obs_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iqTFGUoeAM0",
        "outputId": "4b542db7-cada-41a2-a827-16d81c24a33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 7, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transitions shapes: \\n\", jtu.tree_map(jnp.shape, transitions))\n",
        "print(\"Actions shape:\", actions.shape)\n",
        "print(type(actions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp44_CUcTpXV",
        "outputId": "fda8aa41-dbf5-4c14-aa02-5df08aa3ca8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transitions shapes: \n",
            " TimeStep(state=State(key=(1000000,), step_num=(1000000,), grid=(1000000, 8, 8, 2), agent=AgentState(position=(1000000, 2), direction=(1000000,), pocket=(1000000, 2)), goal_encoding=(1000000, 5), rule_encoding=(1000000, 1, 7), carry=EnvCarry()), step_type=(1000000,), reward=(1000000,), discount=(1000000,), observation=(1000000, 7, 7, 2))\n",
            "Actions shape: (1000000,)\n",
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_replay_buffer(transitions, actions):\n",
        "\n",
        "  observations = transitions.observation # (T, 7, 7, 2)\n",
        "  rewards = transitions.reward # (T,)\n",
        "  dones = transitions.step_type == 2 # (T,)\n",
        "  next_observations = jnp.concatenate([observations[1:], observations[-1:]], axis=0) #(T, 7, 7, 2)\n",
        "  actions = jnp.array(actions, dtype=jnp.int32) #(T,)\n",
        "\n",
        "  replay_buffer = {'observations': observations,\n",
        "                   'actions': actions,\n",
        "                   'rewards': rewards,\n",
        "                   'next_observations': next_observations,\n",
        "                   'dones': dones}\n",
        "\n",
        "  print(\"=== Replay Buffer 构建完成 ===\")\n",
        "  print(f\"数据点数量: {len(observations)}\")\n",
        "  print(f\"平均奖励: {jnp.mean(rewards):.4f}\")\n",
        "  print(f\"Episode结束次数: {jnp.sum(dones)}\")\n",
        "  print(f\"动作分布: {jnp.bincount(actions)}\")\n",
        "  return replay_buffer"
      ],
      "metadata": {
        "id": "pcUGKy_aSeK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potential issue with sparse reward"
      ],
      "metadata": {
        "id": "0JzlW7Ht0M9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = create_replay_buffer(transitions, actions)"
      ],
      "metadata": {
        "id": "QygjqK1SzZr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942a3f37-6a2f-46f1-835c-38973dba6dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Replay Buffer 构建完成 ===\n",
            "数据点数量: 1000000\n",
            "平均奖励: 0.0028\n",
            "Episode结束次数: 9572\n",
            "动作分布: [167326 166610 166812 166592 166378 166282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(replay_buffer, batch_size=32, num_batches=None):\n",
        "  data_size = len(replay_buffer['observations'])\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = max(1, data_size // batch_size)\n",
        "\n",
        "  batches = []\n",
        "\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "-1lsLd27zekf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}