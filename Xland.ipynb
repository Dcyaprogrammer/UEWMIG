{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hNLDJZfm4Hx7",
        "yQSQh8M2lsdJ",
        "UPNRnO8D4DqD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJjXifWFe9K"
      },
      "outputs": [],
      "source": [
        "%pip install jax\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install xminigrid\n",
        "%pip install gymnax\n",
        "%pip install distrax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as jtu\n",
        "import numpy as np\n",
        "import distrax\n",
        "\n",
        "import timeit\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "from flax import nnx\n",
        "import xminigrid"
      ],
      "metadata": {
        "id": "0gdi3TeiF481"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TimeStep(struct.PyTreeNode):\n",
        "#     # hidden environment state, such as grid, agent, goal, etc\n",
        "#     state: State\n",
        "\n",
        "#     # similar to the dm_env enterface\n",
        "#     step_type: StepType\n",
        "#     reward: jax.Array\n",
        "#     discount: jax.Array\n",
        "#     observation: jax.Array"
      ],
      "metadata": {
        "id": "4L7duDLiJ9Wa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper"
      ],
      "metadata": {
        "id": "g0HO9tmL6n3R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRQate_D6ur2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "oSjqwPwiXxCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "fYSuZTE1Y4NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.linear(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class ActionEncoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.embed = nnx.Embed(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.embed(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class JointEncoder(nnx.Module):\n",
        "  def __init__(self, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear1 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm1 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm2 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm3 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array, rng):\n",
        "    dist_distrax = distrax.MultivariateNormalDiag(loc=x, scale_diag=1e-1*jnp.ones_like(x))\n",
        "    # potential shape issue\n",
        "    x = dist_distrax.sample(seed=rng, sample_shape=(1,))\n",
        "    x = self.layer_norm0(x)\n",
        "    h0 = self.linear1(x)\n",
        "    h = nn.relu(h0)\n",
        "    h = self.layer_norm1(h) + h0\n",
        "    h0 = self.linear2(h)\n",
        "    h = self.layer_norm2(h) + h0\n",
        "    return self.layer_norm3(h)"
      ],
      "metadata": {
        "id": "kZlsjAvVXzLc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor"
      ],
      "metadata": {
        "id": "UFZ3e361Y6hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import lax\n",
        "import distrax\n",
        "\n",
        "class Actor(nnx.Module):\n",
        "  # environment related ???\n",
        "  log_std_min: float = -4\n",
        "  log_std_max: float = 2\n",
        "\n",
        "  def __init__(self, obs_dim, action_dim, hidden_dim, rngs: nnx.Rngs):\n",
        "    self.mean = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "    self.log_std = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    mean = self.mean(x)\n",
        "    log_std = jnp.clip(self.log_std(x), self.log_std_min, self.log_std_max)\n",
        "    return mean, log_std"
      ],
      "metadata": {
        "id": "jODz_HXsY_MY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "-qfKrd1lkNHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_info_gain_normal(mean, prec, l_prec, next_obs):\n",
        "  prec = jnp.maximum(prec, 1e-6)\n",
        "  posterior_prec = prec + l_prec\n",
        "  prec_ratio = prec / posterior_prec\n",
        "\n",
        "  posterior_mean = (prec * mean + l_prec * next_obs) / posterior_prec\n",
        "\n",
        "  delta_mean = next_obs - posterior_mean\n",
        "  kl = delta_mean * delta_mean * prec\n",
        "  kl = kl + prec_ratio - jnp.log(prec_ratio) - 1\n",
        "  kl = 0.5 * jnp.sum(kl, axis=-1)\n",
        "  return kl, delta_mean\n",
        "\n",
        "@jax.jit\n",
        "def compute_expected_info_gain_normal(prec, l_prec):\n",
        "  prec = jnp.maximum(prec, 1e-6)\n",
        "  prec_ratio = l_prec / prec\n",
        "  mi_matrix = 0.5 * jnp.sum(jnp.log(1+prec_ratio), axis=-1)\n",
        "  return mi_matrix"
      ],
      "metadata": {
        "id": "5HFYlDrokQXw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Others"
      ],
      "metadata": {
        "id": "Z6TMiJUFZ_uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Likelihood_Prec(nnx.Module):\n",
        "  log_std_min: float = -2\n",
        "  log_std_max: float = 2\n",
        "\n",
        "  def __init__(self, obs_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear = nnx.Linear(hidden_dim, obs_dim, rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    log_std = jnp.clip(self.linear(x), self.log_std_min, self.log_std_max)\n",
        "    return jnp.exp(-log_std)"
      ],
      "metadata": {
        "id": "Tqfsq_SyaBKk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Explorer"
      ],
      "metadata": {
        "id": "ck6iG5qLTJTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnax.experimental import RolloutWrapper\n",
        "# action = self.model_forward(policy_params, obs, rng_net)\n",
        "import functools\n",
        "import gymnax\n",
        "from typing import Union,Optional,Any\n",
        "import abc"
      ],
      "metadata": {
        "id": "XYxtYAbZTLyM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnsupervisedExplorer(nnx.Module):\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def update(self, obs, actions, next_obs, dones, info):\n",
        "    # update variable parameters\n",
        "    return #{'kl':KL} MI= E[KL]\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self, observations, rng):\n",
        "    return #actions, {\"mi\":mi_matrix}\n",
        "\n",
        "class RandomExplorer(UnsupervisedExplorer):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "  def update(self, rng, obs, actions, next_obs, dones, info):\n",
        "    return {}\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "    if observations.ndim == 1:\n",
        "      # possible shape issue here\n",
        "      actions = jax.random.randint(rng, shape=(1,), minval=0, maxval=self.num_actions)\n",
        "      return actions, {}\n",
        "    actions = jax.random.randint(rng, shape=(observations.shape[0],), minval=0, maxval=self.num_actions)\n",
        "    return actions, {}\n",
        "\n",
        "class DeepSACBayesianExplorer(UnsupervisedExplorer):\n",
        "  # ent?\n",
        "  def __init__(self, obs_dim, num_actions, hidden_dim, rngs: nnx.Rngs,\n",
        "               l_prec=1.0, wd=1e-2, ent_lambda=1e-3, depth=2):\n",
        "    self.obs_dim = obs_dim\n",
        "    self.num_actions = num_actions\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.prec_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)), name='prec_w')\n",
        "    self.mean_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)), name='mean_w')\n",
        "    # what is trainable here\n",
        "    self.trainable_likelihood_prec = Likelihood_Prec(obs_dim, hidden_dim, rngs)\n",
        "    self.weight_decay = wd\n",
        "    self.obs_embeds = Encoder(obs_dim, hidden_dim, rngs)\n",
        "    self.action_embeds = ActionEncoder(num_actions, hidden_dim, rngs)\n",
        "    self.joint_embeds = JointEncoder(hidden_dim, rngs)\n",
        "    self.depth = depth\n",
        "    self.ent_lambda = ent_lambda\n",
        "\n",
        "  def update(self, rng, obs, action, next_obs, done, info):\n",
        "    mean = info[\"mean\"]\n",
        "    prec = info[\"prec\"]\n",
        "\n",
        "    def _likelihood_loss(rng, T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "      mu = mean\n",
        "      # model var + inherent var\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "      log_prob = dist_distrax.log_prob(next_obs)\n",
        "      return -log_prob, l_prec\n",
        "\n",
        "    # jit here\n",
        "    predictive_loss, l_prec = _likelihood_loss(rng, info[\"T\"], mean, prec, next_obs)\n",
        "    # originally jnp.sum\n",
        "    mean_error = jnp.mean((mean - next_obs)**2)\n",
        "    deepkl, delta_mean = compute_info_gain_normal(mean, prec, l_prec, next_obs)\n",
        "    # batch x num_hidden\n",
        "    T = info[\"T\"].reshape(-1, self.hidden_dim)\n",
        "\n",
        "    # batch x obs_dim\n",
        "    l_prec = l_prec.reshape(-1, self.obs_dim)\n",
        "    delta_mean = delta_mean.reshape(-1, self.obs_dim)\n",
        "\n",
        "    T_T = jnp.transpose(T)\n",
        "    covariance = T @ T_T\n",
        "    inv_covariance = jnp.linalg.pinv(covariance)\n",
        "\n",
        "    T_Map = T_T @ inv_covariance\n",
        "\n",
        "    delta_precW = T_Map @ l_prec\n",
        "    self.prec_w.value = (self.prec_w.value + delta_precW) * (1-self.weight_decay)\n",
        "    delta_meanW = T_Map @ delta_mean\n",
        "    self.mean_w.value = (self.mean_w.value + delta_meanW) * (1-self.weight_decay)\n",
        "\n",
        "    return {\"kl\":deepkl,  \"predictive_loss\": predictive_loss, \"mean_error\":mean_error}\n",
        "\n",
        "  # jitable\n",
        "  def loss(self, rng, obs, action, next_obs, done, info):\n",
        "    def _likelihood_loss(T, mean, prec, next_obs):\n",
        "      l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "      mu = mean\n",
        "      sigma = jnp.sqrt(1 / l_prec + 1 / prec)\n",
        "      dist_distrax = distrax.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
        "\n",
        "      log_prob = dist_distrax.log_prob(next_obs)\n",
        "      return -log_prob\n",
        "\n",
        "    T, mean, prec = info[\"T\"], info[\"mean\"], info[\"prec\"]\n",
        "    likelihood_loss = _likelihood_loss(T, mean, prec, next_obs)\n",
        "    return likelihood_loss\n",
        "\n",
        "  def batch_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "    vmapped = jax.vmap(self.loss)\n",
        "    return vmapped(rng, obs, actions, next_obs, dones, info)\n",
        "\n",
        "  def recursive_mi(self, observations, rng, depth):\n",
        "    obs_embed = self.obs_embed(observations)\n",
        "    action_embed = self.action_embed(jnp.arange(self.num_actions))\n",
        "    # possible shape issue\n",
        "    embed = action_embed + jnp.expand_dims(obs_embed, axis=0)\n",
        "\n",
        "    # num_actions x embed_size\n",
        "    T = self.joint_embeds(embed, rng)\n",
        "    prec = jnp.maximum(T @ self.prec_w, 1e-3)\n",
        "    # num_actions x obs_dim\n",
        "    mean = T @ self.mean_w\n",
        "    l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "    MI = compute_expected_info_gain_normal(prec, l_prec)\n",
        "\n",
        "    if depth > 0:\n",
        "      vmapped = jax.vmap(self.recursive_mi, in_axes=(0,None,None))\n",
        "      # num_actions x 1\n",
        "      actions, info = vmapped(mean, rng, depth-1)\n",
        "      MI = MI + info[\"mi\"]\n",
        "\n",
        "    actions = jnp.argmax(MI, axis=0)\n",
        "    T = T[actions]\n",
        "    MI = MI[actions]\n",
        "    l_prec = l_prec[actions]\n",
        "    prec = prec[actions]\n",
        "    mean = mean[actions]\n",
        "    return actions, {\"mi\":MI,\"T\":T,\"obs_embed\":obs_embed,\"l_prec\":l_prec,\n",
        "                        \"prec\":prec,\"mean\":mean}\n",
        "\n",
        "def show_variable(model, text):\n",
        "\n",
        "    graphdef, params, vars,others = nnx.split(model, nnx.Param, nnx.Variable,...)\n",
        "\n",
        "    print(text,vars)\n"
      ],
      "metadata": {
        "id": "fqBtGnP6TewM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect Rollouts"
      ],
      "metadata": {
        "id": "hNLDJZfm4Hx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xminigrid.wrappers import GymAutoResetWrapper\n",
        "\n",
        "def build_rollout(env, env_params, num_steps):\n",
        "  def rollout(rng):\n",
        "    def _step_fn(carry, _):\n",
        "      rng, timestep = carry\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      action = jax.random.randint(_rng, shape=(), minval=0, maxval=env.num_actions(env_params))\n",
        "\n",
        "      timestep = env.step(env_params, timestep, action)\n",
        "\n",
        "      return (rng, timestep), (timestep,action)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    timestep = env.reset(env_params, _rng)\n",
        "    rng, (transitions, actions) = jax.lax.scan(_step_fn, (rng, timestep), None, length=num_steps)\n",
        "\n",
        "    return transitions, actions\n",
        "  return rollout"
      ],
      "metadata": {
        "id": "SOVYwUxm70Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-6x6\")\n",
        "env = GymAutoResetWrapper(env)\n",
        "\n",
        "rollout_fn = jax.jit(build_rollout(env, env_params, num_steps=1e6))\n",
        "\n",
        "transitions, actions = rollout_fn(jax.random.key(0))"
      ],
      "metadata": {
        "id": "bAmVT6PtTAnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transitions shapes: \\n\", jtu.tree_map(jnp.shape, transitions))\n",
        "print(\"Actions shape:\", actions.shape)\n",
        "print(type(actions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp44_CUcTpXV",
        "outputId": "c739656f-030d-4e82-cfe3-08b19c01abef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transitions shapes: \n",
            " TimeStep(state=State(key=(1000000,), step_num=(1000000,), grid=(1000000, 6, 6, 2), agent=AgentState(position=(1000000, 2), direction=(1000000,), pocket=(1000000, 2)), goal_encoding=(1000000, 5), rule_encoding=(1000000, 1, 7), carry=EnvCarry()), step_type=(1000000,), reward=(1000000,), discount=(1000000,), observation=(1000000, 7, 7, 2))\n",
            "Actions shape: (1000000,)\n",
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_replay_buffer(transitions, actions):\n",
        "\n",
        "  observations = transitions.observation # (T, 7, 7, 2)\n",
        "  rewards = transitions.reward # (T,)\n",
        "  dones = transitions.step_type == 2 # (T,)\n",
        "  next_observations = jnp.concatenate([observations[1:], observations[-1:]], axis=0) #(T, 7, 7, 2)\n",
        "  actions = jnp.array(actions, dtype=jnp.int32) #(T,)\n",
        "\n",
        "  replay_buffer = {'observations': observations,\n",
        "                   'actions': actions,\n",
        "                   'rewards': rewards,\n",
        "                   'next_observations': next_observations,\n",
        "                   'dones': dones}\n",
        "\n",
        "  print(\"=== Replay Buffer 构建完成 ===\")\n",
        "  print(f\"数据点数量: {len(observations)}\")\n",
        "  print(f\"平均奖励: {jnp.mean(rewards):.4f}\")\n",
        "  print(f\"Episode结束次数: {jnp.sum(dones)}\")\n",
        "  print(f\"动作分布: {jnp.bincount(actions)}\")\n",
        "  return replay_buffer"
      ],
      "metadata": {
        "id": "pcUGKy_aSeK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potential issue with sparse reward"
      ],
      "metadata": {
        "id": "0JzlW7Ht0M9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = create_replay_buffer(transitions, actions)"
      ],
      "metadata": {
        "id": "QygjqK1SzZr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942a3f37-6a2f-46f1-835c-38973dba6dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Replay Buffer 构建完成 ===\n",
            "数据点数量: 1000000\n",
            "平均奖励: 0.0028\n",
            "Episode结束次数: 9572\n",
            "动作分布: [167326 166610 166812 166592 166378 166282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(replay_buffer, batch_size=32, num_batches=None):\n",
        "  data_size = len(replay_buffer['observations'])\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = max(1, data_size // batch_size)\n",
        "\n",
        "  batches = []\n",
        "\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "-1lsLd27zekf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IQL"
      ],
      "metadata": {
        "id": "yQSQh8M2lsdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import distrax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n"
      ],
      "metadata": {
        "id": "WNXxwUJSmMkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "0NstW8rlmhcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"IQL\"\n",
        "    project: str = \"train-IQL\"\n",
        "    env_name: str = \"MiniGrid-EmptyRandom-6x6\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = False\n",
        "    normalize_reward: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Tuple[int, int] = (256, 256)\n",
        "    actor_lr: float = 3e-4\n",
        "    value_lr: float = 3e-4\n",
        "    critic_lr: float = 3e-4\n",
        "    layer_norm: bool = True\n",
        "    opt_decay_schedule: bool = True\n",
        "    # IQL SPECIFIC\n",
        "    expectile: float = (\n",
        "        0.7  # FYI: for Hopper-me, 0.5 produce better result. (antmaze: expectile=0.9)\n",
        "    )\n",
        "    beta: float = (\n",
        "        3.0  # FYI: for Hopper-me, 6.0 produce better result. (antmaze: beta=10.0)\n",
        "    )\n",
        "    tau: float = 0.005\n",
        "    discount: float = 0.99\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "\n",
        "conf_dict = OmegaConf.from_cli()\n",
        "config = IQLConfig(**conf_dict)"
      ],
      "metadata": {
        "id": "WcEIlxtMmkI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Networks"
      ],
      "metadata": {
        "id": "FW1HYQS-m0BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray, actions: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        actions = jax.nn.one_hot(actions, num_classes=4) #one-hot encoding\n",
        "        flat_observations = observations.reshape(batch_size, -1)\n",
        "        inputs = jnp.concatenate([flat_observations, actions], axis=-1)\n",
        "        critic = MLP((*self.hidden_dims, 1), activations=self.activations)(inputs)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    split_rngs = kwargs.pop(\"split_rngs\", {})\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={\"params\": 0},\n",
        "        split_rngs={**split_rngs, \"params\": True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "class ValueCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        obs_flat = observations.reshape(batch_size, -1)\n",
        "        critic = MLP((*self.hidden_dims, 1), layer_norm=self.layer_norm)(obs_flat)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    log_std_min: Optional[float] = -5.0\n",
        "    log_std_max: Optional[float] = 2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observations: jnp.ndarray, temperature: float = 1.0\n",
        "    ) -> distrax.Distribution:\n",
        "        outputs = MLP(\n",
        "            self.hidden_dims,\n",
        "            activate_final=True,\n",
        "        )(observations)\n",
        "\n",
        "        means = nn.Dense(\n",
        "            self.action_dim, kernel_init=default_init()\n",
        "        )(outputs)\n",
        "        log_stds = self.param(\"log_stds\", nn.initializers.zeros, (self.action_dim,))\n",
        "        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        distribution = distrax.MultivariateNormalDiag(\n",
        "            loc=means, scale_diag=jnp.exp(log_stds) * temperature\n",
        "        )\n",
        "        return distribution\n",
        "\n",
        "class CatPolicy(nn.Module):\n",
        "  hidden_dims : Sequence[int]\n",
        "  action_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, observations: jnp.ndarray, temperature: float = 1.0) -> distrax.Distribution:\n",
        "    x = observations.reshape(observations.shape[0], -1) # flatten\n",
        "    outputs = MLP(self.hidden_dims, activate_final=True)(x)\n",
        "    logits = nn.Dense(self.action_dim, kernel_init=default_init())(outputs)\n",
        "    distribution = distrax.Categorical(logits=logits)\n",
        "    return distribution\n"
      ],
      "metadata": {
        "id": "9nqPOs5Umxhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "QvkJdJ9EnpNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jtu.tree_map(jnp.shape, replay_buffer))\n",
        "print(type(replay_buffer))\n",
        "print(replay_buffer[\"dones\"])"
      ],
      "metadata": {
        "id": "dtarzRCfprRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "    dones_float: jnp.ndarray"
      ],
      "metadata": {
        "id": "tJxWv8DinrFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalization(dataset: Transition) -> float:\n",
        "    # into numpy.ndarray\n",
        "    dataset = jax.tree_util.tree_map(lambda x: np.array(x), dataset)\n",
        "    returns = []\n",
        "    ret = 0\n",
        "    for r, term in zip(dataset.rewards, dataset.dones_float):\n",
        "        ret += r\n",
        "        if term:\n",
        "            returns.append(ret)\n",
        "            ret = 0\n",
        "    return (max(returns) - min(returns)) / 1000"
      ],
      "metadata": {
        "id": "ga3QaSnJntei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(\n",
        "     dataset: dict, config: IQLConfig, clip_to_eps: bool = True, eps: float = 1e-5\n",
        ") -> Transition:\n",
        "\n",
        "    if clip_to_eps:\n",
        "        lim = 1 - eps\n",
        "        dataset[\"actions\"] = jnp.clip(dataset[\"actions\"], -lim, lim)\n",
        "\n",
        "    # dones_float = np.zeros_like(dataset['dones'])\n",
        "\n",
        "    # # for i in range(len(dones_float) - 1):\n",
        "    # #     print(i)\n",
        "    # #     if np.linalg.norm(dataset['observations'][i + 1] -\n",
        "    # #                         dataset['next_observations'][i]\n",
        "    # #                         ) > 1e-6 or dataset['dones'][i] == True:\n",
        "    # #         dones_float[i] = 1\n",
        "    # #     else:\n",
        "    # #         dones_float[i] = 0\n",
        "    # dones_float[-1] = 1\n",
        "\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    next_obs = dataset['next_observations']  # shape: (N, 7, 7, 2)\n",
        "    dones = dataset['dones']              # shape: (N,)\n",
        "\n",
        "    # 展平每个 observation\n",
        "    obs_flat = obs[1:].reshape((obs.shape[0] - 1, -1))           # shape: (N-1, 98)\n",
        "    next_obs_flat = next_obs[:-1].reshape((next_obs.shape[0] - 1, -1))  # shape: (N-1, 98)\n",
        "\n",
        "    # 对每个样本求 L2 范数\n",
        "    obs_diff = jnp.linalg.norm(obs_flat - next_obs_flat, axis=1)   # shape: (N-1,)\n",
        "    obs_flag = obs_diff > 1e-6\n",
        "    done_flag = dones[:-1] == True\n",
        "\n",
        "    dones_float = jnp.zeros_like(dones, dtype=jnp.float32)\n",
        "    dones_float = dones_float.at[:-1].set(jnp.logical_or(obs_flag, done_flag).astype(jnp.float32))\n",
        "    dones_float = dones_float.at[-1].set(1.0)\n",
        "\n",
        "    dataset = Transition(\n",
        "        observations=jnp.array(dataset[\"observations\"], dtype=jnp.float32),\n",
        "        actions=jnp.array(dataset[\"actions\"], dtype=jnp.float32),\n",
        "        rewards=jnp.array(dataset[\"rewards\"], dtype=jnp.float32),\n",
        "        next_observations=jnp.array(dataset[\"next_observations\"], dtype=jnp.float32),\n",
        "        dones=jnp.array(dataset[\"dones\"], dtype=jnp.float32),\n",
        "        dones_float=jnp.array(dones_float, dtype=jnp.float32),\n",
        "    )\n",
        "\n",
        "    # normalize states\n",
        "    # obs_mean, obs_std = 0, 1\n",
        "    # if config.normalize_state:\n",
        "    #     obs_mean = dataset.observations.mean(0)\n",
        "    #     obs_std = dataset.observations.std(0)\n",
        "    #     dataset = dataset._replace(\n",
        "    #         observations=(dataset.observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #         next_observations=(dataset.next_observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #     )\n",
        "    # # normalize rewards\n",
        "    # if config.normalize_reward:\n",
        "    #     normalizing_factor = get_normalization(dataset)\n",
        "    #     dataset = dataset._replace(rewards=dataset.rewards / normalizing_factor)\n",
        "\n",
        "    # shuffle data and select the first data_size samples\n",
        "    # data_size = min(config.data_size, len(dataset.observations))\n",
        "    # rng = jax.random.PRNGKey(config.seed)\n",
        "    # rng, rng_permute, rng_select = jax.random.split(rng, 3)\n",
        "    # perm = jax.random.permutation(rng_permute, len(dataset.observations))\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[perm], dataset)\n",
        "    # assert len(dataset.observations) >= data_size\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[:data_size], dataset)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Q4-51Dpin7az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectile_loss(diff, expectile=0.8) -> jnp.ndarray:\n",
        "    weight = jnp.where(diff > 0, expectile, (1 - expectile))\n",
        "    return weight * (diff**2)\n",
        "\n",
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "qlDMKO3toAvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "3jfs8WxhoP3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IQLTrainState(NamedTuple):\n",
        "    rng: jax.random.PRNGKey\n",
        "    critic: TrainState\n",
        "    target_critic: TrainState\n",
        "    value: TrainState\n",
        "    actor: TrainState\n",
        "\n",
        "class IQL(object):\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        next_v = train_state.value.apply_fn(\n",
        "            train_state.value.params, batch.next_observations\n",
        "        )\n",
        "        target_q = batch.rewards + config.discount * (1 - batch.dones) * next_v\n",
        "\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q1, q2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            critic_loss = ((q1 - target_q) ** 2 + (q2 - target_q) ** 2).mean()\n",
        "            return critic_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_value(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        q1, q2 = train_state.target_critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jax.lax.stop_gradient(jnp.minimum(q1, q2))\n",
        "        def value_loss_fn(value_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            v = train_state.value.apply_fn(value_params, batch.observations)\n",
        "            value_loss = expectile_loss(q - v, config.expectile).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_value, value_loss = update_by_loss_grad(train_state.value, value_loss_fn)\n",
        "        return train_state._replace(value=new_value), value_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        v = train_state.value.apply_fn(train_state.value.params, batch.observations)\n",
        "        q1, q2 = train_state.critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jnp.minimum(q1, q2)\n",
        "        exp_a = jnp.exp((q - v) * config.beta)\n",
        "        exp_a = jnp.minimum(exp_a, 100.0)\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            dist = train_state.actor.apply_fn(actor_params, batch.observations)\n",
        "            log_probs = dist.log_prob(batch.actions.astype(jnp.int32))\n",
        "            actor_loss = -(exp_a * log_probs).mean()\n",
        "            return actor_loss\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        dataset: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: IQLConfig,\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        for _ in range(config.n_jitted_updates):\n",
        "            rng, subkey = jax.random.split(rng)\n",
        "            batch_indices = jax.random.randint(\n",
        "                subkey, (config.batch_size,), 0, len(dataset.observations)\n",
        "            )\n",
        "            batch = jax.tree_util.tree_map(lambda x: x[batch_indices], dataset)\n",
        "\n",
        "            train_state, value_loss = self.update_value(train_state, batch, config)\n",
        "            train_state, actor_loss = self.update_actor(train_state, batch, config)\n",
        "            train_state, critic_loss = self.update_critic(train_state, batch, config)\n",
        "            new_target_critic = target_update(\n",
        "                train_state.critic, train_state.target_critic, config.tau\n",
        "            )\n",
        "            train_state = train_state._replace(target_critic=new_target_critic)\n",
        "        return train_state, {\n",
        "            \"value_loss\": value_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "            \"critic_loss\": critic_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        observations: np.ndarray,\n",
        "        seed: jax.random.PRNGKey,\n",
        "        temperature: float = 1.0,\n",
        "        max_action: float = 1.0,\n",
        "    ) -> jnp.ndarray:\n",
        "\n",
        "        # modified for discrete actions\n",
        "        dist = train_state.actor.apply_fn(\n",
        "            train_state.actor.params, observations, temperature=temperature\n",
        "        )\n",
        "        actions = jnp.argmax(dist.logits, axis=-1)\n",
        "        return actions"
      ],
      "metadata": {
        "id": "S3ZiQLAwoV_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Evaluate"
      ],
      "metadata": {
        "id": "pJXpO90HoiJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_iql_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: IQLConfig,\n",
        ") -> IQLTrainState:\n",
        "    rng, actor_rng, critic_rng, value_rng = jax.random.split(rng, 4)\n",
        "    # initialize actor\n",
        "    action_dim = 4\n",
        "\n",
        "    # Gaussian Model\n",
        "    # actor_model = GaussianPolicy(\n",
        "    #     config.hidden_dims,\n",
        "    #     action_dim=action_dim,\n",
        "    #     log_std_min=-5.0,\n",
        "    # )\n",
        "\n",
        "    # Cat Model\n",
        "    actor_model = CatPolicy(\n",
        "        config.hidden_dims,\n",
        "        action_dim = action_dim\n",
        "    )\n",
        "\n",
        "    if config.opt_decay_schedule:\n",
        "        schedule_fn = optax.cosine_decay_schedule(-config.actor_lr, config.max_steps)\n",
        "        actor_tx = optax.chain(optax.scale_by_adam(), optax.scale_by_schedule(schedule_fn))\n",
        "    else:\n",
        "        actor_tx = optax.adam(learning_rate=config.actor_lr)\n",
        "    actor = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=actor_tx,\n",
        "    )\n",
        "    # initialize critic\n",
        "    critic_model = ensemblize(Critic, num_qs=2)(config.hidden_dims)\n",
        "    critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    target_critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    # initialize value\n",
        "    value_model = ValueCritic(config.hidden_dims, layer_norm=config.layer_norm)\n",
        "    value = TrainState.create(\n",
        "        apply_fn=value_model.apply,\n",
        "        params=value_model.init(value_rng, observations),\n",
        "        tx=optax.adam(learning_rate=config.value_lr),\n",
        "    )\n",
        "    return IQLTrainState(\n",
        "        rng,\n",
        "        critic=critic,\n",
        "        target_critic=target_critic,\n",
        "        value=value,\n",
        "        actor=actor,\n",
        "    )"
      ],
      "metadata": {
        "id": "yZvRadfAokla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn, env, env_params, num_episodes: int, rng\n",
        ") -> float:\n",
        "    print(\"evaluation started\")\n",
        "    episode_returns = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      episode_return = 0\n",
        "\n",
        "      timestep = env.reset(env_params, _rng)\n",
        "      done = timestep.step_type == 2\n",
        "      observation = timestep.observation\n",
        "\n",
        "      while not done:\n",
        "          # potential case issue\n",
        "          obs = observation[None, ...]\n",
        "          action = policy_fn(observations=obs)\n",
        "\n",
        "          if isinstance(action, (jnp.ndarray, np.ndarray)) and action.shape == (1,):\n",
        "            action = int(action[0])\n",
        "\n",
        "          timestep = env.step(env_params, timestep, action)\n",
        "          reward = timestep.reward\n",
        "          done = timestep.step_type == 2\n",
        "          observation = timestep.observation\n",
        "\n",
        "          episode_return += reward\n",
        "      episode_returns.append(episode_return)\n",
        "    return float(jnp.mean(jnp.array(episode_returns)))"
      ],
      "metadata": {
        "id": "9B6VH9JgoptW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(config=config, project=config.project)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "\n",
        "    env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-6x6\")\n",
        "    env = GymAutoResetWrapper(env)\n",
        "\n",
        "    dataset= preprocess_dataset(replay_buffer, config)\n",
        "\n",
        "    # create train_state\n",
        "    example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state: IQLTrainState = create_iql_train_state(\n",
        "        _rng,\n",
        "        example_batch.observations[None, ...],\n",
        "        example_batch.actions[None, ...],\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    algo = IQL()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(train_state, dataset, subkey, config)\n",
        "\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        # if i % eval_interval == 0:\n",
        "        #     policy_fn = partial(\n",
        "        #         act_fn,\n",
        "        #         temperature=0.0,\n",
        "        #         seed=jax.random.PRNGKey(0),\n",
        "        #         train_state=train_state,\n",
        "        #     )\n",
        "        #     normalized_score = evaluate(\n",
        "        #         policy_fn,\n",
        "        #         env,\n",
        "        #         env_params,\n",
        "        #         rng = _rng,\n",
        "        #         num_episodes=config.eval_episodes,\n",
        "        #     )\n",
        "        #     print(i, normalized_score)\n",
        "        #     eval_metrics = {f\"{config.env_name}/normalized_score\": normalized_score}\n",
        "        #     wandb.log(eval_metrics, step=i)\n",
        "    # final evaluation\n",
        "    policy_fn = partial(\n",
        "        act_fn,\n",
        "        temperature=0.0,\n",
        "        seed=jax.random.PRNGKey(0),\n",
        "        train_state=train_state,\n",
        "    )\n",
        "    normalized_score = evaluate(\n",
        "        policy_fn,\n",
        "        env,\n",
        "        env_params,\n",
        "        rng = _rng,\n",
        "        num_episodes=config.eval_episodes,\n",
        "    )\n",
        "    print(\"Final Evaluation\", normalized_score)\n",
        "    wandb.log({f\"{config.env_name}/final_normalized_score\": normalized_score})\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "mQkNH7rvo2mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD3BC"
      ],
      "metadata": {
        "id": "UPNRnO8D4DqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "XvngrFj44C2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions"
      ],
      "metadata": {
        "id": "qqcNMUJUuGu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ],
      "metadata": {
        "id": "hU9mkQlN3qNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BCConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"TD3-BC\"\n",
        "    project: str = \"train-TD3-BC\"\n",
        "    env_name: str = \"MiniGrid-Empty-8x8\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100000\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Sequence[int] = (256, 256)\n",
        "    critic_lr: float = 1e-3\n",
        "    actor_lr: float = 1e-3\n",
        "    # TD3-BC SPECIFIC\n",
        "    policy_freq: int = 2  # update actor every policy_freq updates\n",
        "    alpha: float = 2.5  # BC loss weight\n",
        "    policy_noise_std: float = 0.2  # std of policy noise\n",
        "    policy_noise_clip: float = 0.5  # clip policy noise\n",
        "    tau: float = 0.005  # target network update rate\n",
        "    discount: float = 0.99  # discount factor\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "conf_dict = OmegaConf.from_cli() # CLI Input\n",
        "config = TD3BCConfig(**conf_dict)\n",
        "\n",
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    if i + 1 < len(self.hidden_dims):\n",
        "                        x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "class DoubleCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observation: jnp.ndarray, action: jnp.ndarray\n",
        "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "        x = jnp.concatenate([observation, action], axis=-1)\n",
        "        q1 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        q2 = MLP((*self.hidden_dims, 1), layer_norm=True)(x)\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "class TD3Actor(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    max_action: float = 1.0  # In D4RL, action is scaled to [-1, 1]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observation: jnp.ndarray) -> jnp.ndarray:\n",
        "        action = MLP((*self.hidden_dims, self.action_dim))(observation)\n",
        "        action = self.max_action * jnp.tanh(\n",
        "            action\n",
        "        )  # scale to [-max_action, max_action]\n",
        "        return action\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "\n",
        "class TD3BCTrainState(NamedTuple):\n",
        "    actor: TrainState\n",
        "    critic: TrainState\n",
        "    target_actor: TrainState\n",
        "    target_critic: TrainState\n",
        "    max_action: float = 1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "Wyj1sj8G1O0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD3BC Object"
      ],
      "metadata": {
        "id": "eJ50iJGFuprx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3BC(object):\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            predicted_action = train_state.actor.apply_fn(\n",
        "                actor_params, batch.observations\n",
        "            )\n",
        "            critic_params = jax.lax.stop_gradient(train_state.critic.params)\n",
        "            q_value, _ = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, predicted_action\n",
        "            )\n",
        "\n",
        "            mean_abs_q = jax.lax.stop_gradient(jnp.abs(q_value).mean())\n",
        "            loss_lambda = config.alpha / mean_abs_q\n",
        "\n",
        "            bc_loss = jnp.square(predicted_action - batch.actions).mean()\n",
        "            loss_actor = -1.0 * q_value.mean() * loss_lambda + bc_loss\n",
        "            return loss_actor\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        batch: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", jnp.ndarray]:\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q_pred_1, q_pred_2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            target_next_action = train_state.target_actor.apply_fn(\n",
        "                train_state.target_actor.params, batch.next_observations\n",
        "            )\n",
        "            policy_noise = (\n",
        "                config.policy_noise_std\n",
        "                * train_state.max_action\n",
        "                * jax.random.normal(rng, batch.actions.shape)\n",
        "            )\n",
        "            target_next_action = target_next_action + policy_noise.clip(\n",
        "                -config.policy_noise_clip, config.policy_noise_clip\n",
        "            )\n",
        "            target_next_action = target_next_action.clip(\n",
        "                -train_state.max_action, train_state.max_action\n",
        "            )\n",
        "            q_next_1, q_next_2 = train_state.target_critic.apply_fn(\n",
        "                train_state.target_critic.params,\n",
        "                batch.next_observations,\n",
        "                target_next_action,\n",
        "            )\n",
        "            target = batch.rewards[..., None] + config.discount * jnp.minimum(\n",
        "                q_next_1, q_next_2\n",
        "            ) * (1 - batch.dones[..., None])\n",
        "            target = jax.lax.stop_gradient(target)  # stop gradient for target\n",
        "            value_loss_1 = jnp.square(q_pred_1 - target)\n",
        "            value_loss_2 = jnp.square(q_pred_2 - target)\n",
        "            value_loss = (value_loss_1 + value_loss_2).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        data: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: TD3BCConfig,\n",
        "    ) -> Tuple[\"TD3BCTrainState\", Dict]:\n",
        "        for _ in range(\n",
        "            config.n_jitted_updates\n",
        "        ):  # we can jit for roop for static unroll\n",
        "            rng, batch_rng = jax.random.split(rng, 2)\n",
        "            batch_idx = jax.random.randint(\n",
        "                batch_rng, (config.batch_size,), 0, len(data.observations)\n",
        "            )\n",
        "            batch: Transition = jax.tree_util.tree_map(lambda x: x[batch_idx], data)\n",
        "            rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "            train_state, critic_loss = self.update_critic(\n",
        "                train_state, batch, critic_rng, config\n",
        "            )\n",
        "            if _ % config.policy_freq == 0:\n",
        "                train_state, actor_loss = self.update_actor(\n",
        "                    train_state, batch, actor_rng, config\n",
        "                )\n",
        "                new_target_critic = target_update(\n",
        "                    train_state.critic, train_state.target_critic, config.tau\n",
        "                )\n",
        "                new_target_actor = target_update(\n",
        "                    train_state.actor, train_state.target_actor, config.tau\n",
        "                )\n",
        "                train_state = train_state._replace(\n",
        "                    target_critic=new_target_critic,\n",
        "                    target_actor=new_target_actor,\n",
        "                )\n",
        "        return train_state, {\n",
        "            \"critic_loss\": critic_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: TD3BCTrainState,\n",
        "        obs: jnp.ndarray,\n",
        "        max_action: float = 1.0,  # In D4RL, action is scaled to [-1, 1]\n",
        "    ) -> jnp.ndarray:\n",
        "        action = train_state.actor.apply_fn(train_state.actor.params, obs)\n",
        "        action = action.clip(-max_action, max_action)\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "aAkZZy3ouKpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TrainState"
      ],
      "metadata": {
        "id": "MBarr3TLvEuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_td3bc_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: TD3BCConfig,\n",
        ") -> TD3BCTrainState:\n",
        "    critic_model = DoubleCritic(\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    action_dim = actions.shape[-1]\n",
        "    actor_model = TD3Actor(\n",
        "        action_dim=action_dim,\n",
        "        hidden_dims=config.hidden_dims,\n",
        "    )\n",
        "    rng, critic_rng, actor_rng = jax.random.split(rng, 3)\n",
        "    # initialize critic\n",
        "    critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    target_critic_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(config.critic_lr),\n",
        "    )\n",
        "    # initialize actor\n",
        "    actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    target_actor_train_state: TrainState = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=optax.adam(config.actor_lr),\n",
        "    )\n",
        "    return TD3BCTrainState(\n",
        "        actor=actor_train_state,\n",
        "        critic=critic_train_state,\n",
        "        target_actor=target_actor_train_state,\n",
        "        target_critic=target_critic_train_state,\n",
        "    )"
      ],
      "metadata": {
        "id": "VllbSO2Bu7oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "b64_CT78vHk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    policy_fn: Callable[[jnp.ndarray], jnp.ndarray],\n",
        "    env_name: str,\n",
        "    num_episodes: int,\n",
        "    obs_mean,\n",
        "    obs_std,\n",
        "    max_steps_per_episode: int = 100,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    评估策略\n",
        "\n",
        "    Args:\n",
        "        policy_fn: 策略函数\n",
        "        env_name: 环境名称\n",
        "        num_episodes: episode数量\n",
        "        obs_mean: observation均值\n",
        "        obs_std: observation标准差\n",
        "        max_steps_per_episode: 每个episode的最大步数\n",
        "\n",
        "    Returns:\n",
        "        平均episode回报\n",
        "    \"\"\"\n",
        "    # 创建环境\n",
        "    env, env_params = xminigrid.make(env_name)\n",
        "\n",
        "    episode_returns = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        episode_return = 0\n",
        "        timestep = env.reset(env_params, jax.random.PRNGKey(episode))\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            # 处理observation - xminigrid的observation是直接的JAX数组\n",
        "            obs_array = timestep.observation\n",
        "            obs_numpy = np.array(obs_array)\n",
        "\n",
        "            # xminigrid的observation形状是(7, 7, 2)\n",
        "            if obs_numpy.shape == (7, 7, 2):\n",
        "                # 将(7, 7, 2)转换为(7, 7, 3)的RGB图像\n",
        "                object_types = obs_numpy[:, :, 0]\n",
        "                colors = obs_numpy[:, :, 1]\n",
        "\n",
        "                rgb_image = np.zeros((7, 7, 3), dtype=np.uint8)\n",
        "                rgb_image[:, :, 0] = colors\n",
        "                rgb_image[:, :, 1] = object_types\n",
        "                rgb_image[:, :, 2] = 0\n",
        "\n",
        "                # 上采样到22x22\n",
        "                from scipy.ndimage import zoom\n",
        "                try:\n",
        "                    rgb_image = zoom(rgb_image, (22/7, 22/7, 1), order=0)\n",
        "                except ImportError:\n",
        "                    rgb_image = np.repeat(np.repeat(rgb_image, 3, axis=0), 3, axis=1)\n",
        "                    rgb_image = rgb_image[:22, :22, :]\n",
        "\n",
        "                direction = np.array([0.0])\n",
        "            else:\n",
        "                rgb_image = obs_numpy\n",
        "                direction = np.array([0.0])\n",
        "\n",
        "            obs_dict = {\n",
        "                'image': rgb_image,\n",
        "                'direction': direction\n",
        "            }\n",
        "            processed_obs = processor.process_observation(obs_dict)\n",
        "\n",
        "            # 归一化observation\n",
        "            if obs_mean is not None and obs_std is not None:\n",
        "                processed_obs = (processed_obs - obs_mean) / obs_std\n",
        "\n",
        "            # 获取动作\n",
        "            action = policy_fn(obs=processed_obs)\n",
        "\n",
        "            # 执行动作\n",
        "            timestep = env.step(env_params, timestep, action)\n",
        "            episode_return += timestep.reward\n",
        "\n",
        "            if timestep.is_done():\n",
        "                break\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "    return np.mean(episode_returns)\n"
      ],
      "metadata": {
        "id": "ISV0wZG6vBMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # wandb.init(project=config.project, config=config)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    # dataset, obs_mean, obs_std = get_dataset(config)\n",
        "\n",
        "    # create train_state\n",
        "    rng, subkey = jax.random.split(rng)\n",
        "    # example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state = create_td3bc_train_state(\n",
        "        subkey, example_batch.observations, example_batch.actions, config\n",
        "    )\n",
        "    algo = TD3BC()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, update_rng = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(\n",
        "            train_state,\n",
        "            dataset,\n",
        "            update_rng,\n",
        "            config,\n",
        "        )  # update parameters\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            # wandb.log(train_metrics, step=i)\n",
        "\n",
        "        if i % eval_interval == 0:\n",
        "            policy_fn = partial(act_fn, train_state=train_state)\n",
        "            normalized_score = evaluate(\n",
        "                policy_fn,\n",
        "                config.env_name,\n",
        "                num_episodes=config.eval_episodes,\n",
        "                obs_mean=obs_mean,\n",
        "                obs_std=obs_std,\n",
        "            )\n",
        "            print(i, normalized_score)\n",
        "            eval_metrics = {f\"{config.env_name}/episode_return\": normalized_score}\n",
        "            # wandb.log(eval_metrics, step=i)\n",
        "\n",
        "    # # final evaluation\n",
        "    # policy_fn = partial(act_fn, train_state=train_state)\n",
        "    # normalized_score = evaluate(\n",
        "    #     policy_fn,\n",
        "    #     config.env_name,\n",
        "    #     num_episodes=config.eval_episodes,\n",
        "    #     obs_mean=obs_mean,\n",
        "    #     obs_std=obs_std,\n",
        "    # )\n",
        "    # print(\"Final Evaluation Score:\", normalized_score)\n",
        "    # wandb.log({f\"{config.env_name}/final_episode_return\": normalized_score})\n",
        "    # wandb.finish()"
      ],
      "metadata": {
        "id": "5eHE93XFvDk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5ea458-9592-42f1-dedc-dd0a82c43e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_td3bc_train_state' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-66-3683242256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train_state = create_td3bc_train_state(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0msubkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_td3bc_train_state' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j06BRkx7luFa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}